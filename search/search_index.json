{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Opsmate, The AI SRE teammate to free you from the toils of production engineering.","text":"<p>Opsmate is an LLM-powered SRE copilot for understanding and solving production problems. By encoding expert troubleshooting patterns and operational knowledge, Opsmate lets users describe problem statements and intentions in natural language, eliminating the need to memorise complex command line or domain-specific tool syntax.</p> <p>Opsmate can not only perform problem solving autonomously, but also allow human operators to provide feedback and take over the control when needed. It accelerates incident response, reduces mean time to repair (MTTR), and empowers teams to focus on solving problems rather than wrestling with tooling.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>You can start using Opsmate by running it locally on your workstation. There are several ways to install Opsmate on your workstation:</p> pippipxuvxDockerSource <pre><code>pip install -U opsmate\n</code></pre> <pre><code>pipx install opsmate\n# or\npipx upgrade opsmate\n</code></pre> <pre><code>uvx opsmate [OPTIONS] COMMAND [ARGS]...\n</code></pre> <pre><code># Note this is less useful as you cannot access the host from the container\n# But still useful to interact with cloud API in an isolated containerised environment\ndocker pull ghcr.io/opsmate-ai/opsmate:latest # or the specific version if you prefer not living on the edge\nalias opsmate=\"docker run -it --rm --env OPENAI_API_KEY=$OPENAI_API_KEY -v $HOME/.opsmate:/root/.opsmate ghcr.io/opsmate-ai/opsmate:latest\"\n</code></pre> <pre><code>git clone git@github.com:opsmate-ai/opsmate.git\ncd opsmate\n\nuv build\n\npipx install ./dist/opsmate-*.whl\n</code></pre> <p>Note that the Opsmate is powered by large language models. At the moment it supports</p> <ul> <li>OpenAI</li> <li>Anthropic</li> <li>xAI</li> </ul> <p>To use Opsmate, you need to set any one of the <code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code> or <code>XAI_API_KEY</code> environment variables.</p> <pre><code>export OPENAI_API_KEY=\"sk-proj...\"\nexport ANTHROPIC_API_KEY=\"sk-ant-api03-...\"\nexport XAI_API_KEY=\"xai-...\"\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Run <code>opsmate run \"what's the distro of the os\"</code> to get the OS distribution of the host</p> <p>Run <code>opsmate solve \"resolve the high cpu usage on the server\" --review</code> to solve the problem step by step and review the solution with human in the loop.</p> <p>Run <code>opsmate chat --review</code> to chat with Opsmate.</p> <p>Run <code>opsmate serve</code> to launch a notebook interface for Opsmate.</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>CLI Reference for simple command usage.</li> <li>LLM Providers for LLM provider configuration.</li> <li>Tools for tool usage.</li> <li>Integrations and Cookbooks for advanced usages.</li> <li>Production for how to production-grade Opsmate deployment behind local workstation usage.</li> </ul>"},{"location":"development/","title":"Development","text":""},{"location":"development/#install-dependencies","title":"Install dependencies","text":"<pre><code>uv sync\n</code></pre>"},{"location":"development/#running-tests","title":"Running tests","text":"<pre><code>uv run pytest ./opsmate/tests -n auto\n</code></pre>"},{"location":"how-to-o11y-with-tempo/","title":"How to setup observability with Tempo","text":"<p>To run Grafana and Tempo locally:</p> <pre><code>(\n  cd tempo\n  docker compose up -d\n)\n</code></pre> <p>To run Opsmate with tracing enabled:</p> <pre><code>opsmate chat\n</code></pre>"},{"location":"macros/","title":"Macros","text":"In\u00a0[\u00a0]: Copied! <pre>import uuid\n</pre> import uuid In\u00a0[\u00a0]: Copied! <pre>def value_str(value):\n    if isinstance(value, str):\n        return f'\"{value}\"'\n    s = str(value)\n    if isinstance(value, bool):\n        s = s.lower()\n    return s\n</pre> def value_str(value):     if isinstance(value, str):         return f'\"{value}\"'     s = str(value)     if isinstance(value, bool):         s = s.lower()     return s In\u00a0[\u00a0]: Copied! <pre>def define_env(env):\n    \"\"\"\n    This is the hook for defining variables, macros and filters\n    \"\"\"\n\n    @env.macro\n    def asciinema(file, **kwargs):\n        html = \"\"\n        opts = {\n            \"autoPlay\": True,\n            \"controls\": True,\n            \"loop\": True,\n            \"speed\": 1.5,\n            \"theme\": \"asciinema\",\n            \"rows\": 24,\n        }\n\n        # Overwrite defaults with kwargs\n        for key, value in kwargs.items():\n            opts[key] = value\n\n        # Create an empty div that we will use for the player\n        div_id = \"asciinema-\" + str(uuid.uuid4())\n        div_style = \"z-index: 1; position: relative;\"\n        html += '&lt;div id=\"' + div_id + '\" style=\"' + div_style + '\"&gt;&lt;/div&gt;'\n\n        # Define JS representing creating the player\n        create_player_js = \"\"\n        create_player_js += (\n            \"AsciinemaPlayer.create('\"\n            + file\n            + \"', document.getElementById('\"\n            + div_id\n            + \"'), {\"\n        )\n        for key, value in opts.items():\n            # create_player_js += '\"' + key + '\": ' + value_str(value) + \",\"\n            create_player_js += f'\"{key}\": {value_str(value)},'\n        create_player_js += \"});\"\n\n        # Create script tag that will perform cast by either registering for the DOM to\n        # load or firing immediately if already loaded\n        html += \"&lt;script&gt;\"\n        html += \"if (document.readyState === 'loading') {\"\n        html += \"document.addEventListener('DOMContentLoaded', function() {\"\n        html += create_player_js\n        html += \"});\"\n        html += \"} else {\"\n        html += create_player_js\n        html += \"}\"\n        html += \"&lt;/script&gt;\"\n\n        return html\n</pre> def define_env(env):     \"\"\"     This is the hook for defining variables, macros and filters     \"\"\"      @env.macro     def asciinema(file, **kwargs):         html = \"\"         opts = {             \"autoPlay\": True,             \"controls\": True,             \"loop\": True,             \"speed\": 1.5,             \"theme\": \"asciinema\",             \"rows\": 24,         }          # Overwrite defaults with kwargs         for key, value in kwargs.items():             opts[key] = value          # Create an empty div that we will use for the player         div_id = \"asciinema-\" + str(uuid.uuid4())         div_style = \"z-index: 1; position: relative;\"         html += ''          # Define JS representing creating the player         create_player_js = \"\"         create_player_js += (             \"AsciinemaPlayer.create('\"             + file             + \"', document.getElementById('\"             + div_id             + \"'), {\"         )         for key, value in opts.items():             # create_player_js += '\"' + key + '\": ' + value_str(value) + \",\"             create_player_js += f'\"{key}\": {value_str(value)},'         create_player_js += \"});\"          # Create script tag that will perform cast by either registering for the DOM to         # load or firing immediately if already loaded         html += \"\"          return html"},{"location":"production/","title":"Production","text":"<p>This documentation highlights how to run Opsmate in production environment.</p>"},{"location":"production/#why-bother","title":"Why bother?","text":"<p><code>Opsmate</code> can be used as a command line tool standalone however it comes with a few limitations:</p> <ul> <li>Every local workstation is a snowflake in its own way, thus it's hard to have a consistent experience across different machines.</li> <li>Some of the production environments access are simply not available from the local workstation.</li> <li>People cannot collaborate on a local workstation.</li> </ul> <p>To address these issues, we also provide a <code>opsmate-operator</code> that can run <code>Opsmate</code> on demand in a Kubernetes cluster.</p>"},{"location":"production/#key-features","title":"Key features","text":"<p>Here are some of the key features of <code>opsmate-operator</code>:</p> <ul> <li>Manage the Opsmate environment via a <code>EnvironmentBuild</code> CRD.</li> <li><code>Opsmate</code> can be scheduled on demand via a <code>Task</code> CRD.</li> <li>Each of the <code>Opsmate</code> task comes with a dedicated secured HTTPS endpoint and web UI, run inside a dedicated pod.</li> <li><code>Opsmate</code> environment builds and tasks are scoped by the namespace thus support multi-tenancy.</li> <li>The task comes with a <code>TTL</code> (time to live) thus it will be automatically garbage collected after the TTL expires. By doing so we avoid resource waste.</li> <li>An API-server to allow you to manage the <code>Opsmate</code> environment and tasks.</li> </ul>"},{"location":"production/#how-to-install-the-operator","title":"How to install the operator","text":"<p>Here is an example of how to install the operator using Terraform and Helm. <pre><code># Where you install the operator\nresource \"kubernetes_namespace\" \"opsmate_operator\" {\n  metadata {\n    name = \"opsmate-operator\"\n  }\n}\n\nresource \"helm_release\" \"opsmate_operator\" {\n  name             = \"opsmate-operator\"\n  repository       = \"oci://ghcr.io/opsmate-ai/opsmate-operator\"\n  chart            = \"opsmate-operator\"\n  version          = \"0.2.0\"\n  namespace        = kubernetes_namespace.opsmate_operator.metadata[0].name\n  create_namespace = false\n  max_history      = 3\n\n  set {\n    name  = \"installCRDs\"\n    value = \"true\"\n  }\n\n  values = [\n    yamlencode({\n      controllerManager = {\n        fullnameOverride = \"opsmate-operator\"\n      }\n    }),\n  ]\n}\n</code></pre></p>"},{"location":"production/#environment-build","title":"Environment Build","text":"<p>Opsmate Environment Build is a CRD (Custom Resource Definition) that defines the environment that will be used to run the <code>Opsmate</code> task.</p> <p>The following example we:</p> <ul> <li>Create a new namespace <code>opsmate-workspace</code></li> <li>Create a new cluster role <code>opsmate-cluster-reader</code> which is bound to the <code>opsmate-cluster-reader</code> service account.</li> <li>Create a new <code>environmentBuild</code> called <code>cluster-reader</code> which will be used as a template for running the <code>Opsmate</code> task.</li> </ul> <p>The <code>environmentBuild</code> is composed of:</p> <ul> <li>A <code>opsmate</code> container that runs as a Web UI and API server.</li> <li>A <code>worker</code> container that is responsible for running background heavy-lifting tasks such as ingesting the knowledge base and embedding into the vector database.</li> <li>The <code>opsmate</code> and the <code>worker</code> containers shared the same volume for storing the sqlite database and the vector database.</li> </ul> Click to show opsmate-cluster-reader ClusterRole <pre><code>---\n# cluster reader role\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: opsmate-cluster-reader\nrules:\n- apiGroups: [\"\"]  # Core API group\n  resources:\n  - nodes\n  - namespaces\n  - pods\n  - services\n  - configmaps\n  - secrets\n  - persistentvolumes\n  - persistentvolumeclaims\n  - events\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups: [\"apps\"]  # Apps API group\n  resources:\n  - deployments\n  - daemonsets\n  - statefulsets\n  - replicasets\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups: [\"batch\"]  # Batch API group\n  resources:\n  - jobs\n  - cronjobs\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups: [\"networking.k8s.io\"]  # Networking API group\n  resources:\n  - ingresses\n  - networkpolicies\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups: [\"storage.k8s.io\"]  # Storage API group\n  resources:\n  - storageclasses\n  verbs:\n  - get\n  - list\n  - watch\n</code></pre> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: opsmate-workspace\n---\n# service account for cluster reader\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: opsmate-cluster-reader\n  namespace: opsmate-workspace\n---\n# role binding for cluster reader\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: opsmate-cluster-reader\nsubjects:\n- kind: ServiceAccount\n  name: opsmate-cluster-reader\n  namespace: opsmate-workspace\nroleRef:\n  kind: ClusterRole\n  name: opsmate-cluster-reader\n---\n# configmap for opsmate task\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: opsmate-config\n  namespace: opsmate-workspace\ndata:\n  OPSMATE_DB_URL: sqlite:////var/opsmate/opsmate.sqlite\n  EMBEDDINGS_DB_PATH: /var/opsmate/embedding\n  GITHUB_EMBEDDINGS_CONFIG: |\n    {\n      \"opsmate-ai/opsmate\": \"**/*.md\"\n    }\n---\n# cluster reader environment build\napiVersion: sre.opsmate.io/v1alpha1\nkind: EnvironmentBuild\nmetadata:\n  name: cluster-reader\n  namespace: opsmate-workspace\nspec:\n  podTemplate:\n    spec:\n      serviceAccountName: opsmate-cluster-reader\n      initContainers:\n        - name: opsmate-db-migrate\n          image: ghcr.io/opsmate-ai/opsmate:0.2.0a0\n          args:\n            - db-migrate\n          envFrom:\n            - configMapRef:\n                name: opsmate-config\n          volumeMounts:\n            - name: opsmate-vol\n              mountPath: /var/opsmate\n      containers:\n        - name: opsmate\n          image: ghcr.io/opsmate-ai/opsmate:0.2.0a0\n          ports:\n            - containerPort: 8000\n          envFrom:\n            - secretRef:\n                name: opsmate-secret\n            - configMapRef:\n                name: opsmate-config\n          volumeMounts:\n            - name: opsmate-vol\n              mountPath: /var/opsmate\n          args:\n            - serve\n            - --auto-migrate=false\n        - name: worker\n          image: ghcr.io/opsmate-ai/opsmate:0.1.45a0\n          envFrom:\n            - secretRef:\n                name: opsmate-secret\n            - configMapRef:\n                name: opsmate-config\n          args:\n            - worker\n            - --auto-migrate=false\n          volumeMounts:\n            - name: opsmate-vol\n              mountPath: /var/opsmate\n      imagePullSecrets:\n        - name: opsmate-workspace-image-pull-secret\n      volumes:\n      - name: opsmate-vol\n        emptyDir:\n          sizeLimit: 500Mi\n  service:\n    type: ClusterIP\n    ports:\n      - port: 80\n        targetPort: 8000\n  ingressTLS: true\n  ingressTargetPort: 80\n</code></pre> <p>There are a few secrets that you will need to create:</p> <ul> <li><code>OPENAI_API_KEY</code> - If you are using OpenAI as your LLM provider. Currently it is mandatory as we are using OpenAI's embedding API for embedding the knowledge base.</li> <li><code>ANTHROPIC_API_KEY</code> - If you are using Anthropic as your LLM provider.</li> <li><code>XAI_API_KEY</code> - If you are using xAI as your LLM provider.</li> <li><code>GITHUB_TOKEN</code> - This is used for</li> <li>Accessing the GitHub repository for loading knowledge base.</li> <li>Used by Opsmate to to clone the repo, commit changes and raise PRs.</li> </ul> <p>Here are the examples of how to create the secrets:</p> SecretExternal Secret Manager <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: opsmate-secret\n  namespace: opsmate-workspace\ntype: Opaque\ndata:\n  OPENAI_API_KEY: &lt;your-openai-api-key-base64-encoded&gt;\n  ANTHROPIC_API_KEY: &lt;your-anthropic-api-key-base64-encoded&gt;\n  GITHUB_TOKEN: &lt;your-github-token-base64-encoded&gt;\n</code></pre> <pre><code>---\napiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: gcp-secret-store\n  namespace: opsmate-workspace\nspec:\n  provider:\n    gcpsm:\n      projectID: $YOUR_GCP_PROJECT_ID\n---\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: opsmate-secret\n  namespace: opsmate-workspace\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    kind: SecretStore\n    name: gcp-secret-store\n  target:\n    name: opsmate-secret\n    creationPolicy: Owner\n  data:\n    - secretKey: OPENAI_API_KEY\n      remoteRef:\n        key: opsmate-workspace-openai-key\n    - secretKey: ANTHROPIC_API_KEY\n      remoteRef:\n        key: opsmate-workspace-anthropic-key\n    - secretKey: GITHUB_TOKEN\n      remoteRef:\n        key: opsmate-workspace-github-token-ro\n</code></pre>"},{"location":"production/#task","title":"Task","text":"<p>The task is a CRD that defines a workspace that will be used for tackling production problem.</p> <p>Here is an example of a task:</p> <pre><code>---\napiVersion: sre.opsmate.io/v1alpha1\nkind: Task\nmetadata:\n  name: investigator\n  namespace: opsmate-workspace\nspec:\n  userID: anonymous\n  environmentBuildName: cluster-reader\n  description: \"a opsmate task for investigating the cluster\"\n  context: \"you are on a kubernetes cluster\"\n  domainName: \"investigator.opsmate.your-corp.com\"\n  ingressAnnotations:\n    external-dns.alpha.kubernetes.io/hostname: investigator.opsmate.your-corp.com\n  ingressSecretName: opsmate-cert\n</code></pre> <p>In the example above we assume that you:</p> <ul> <li>Own the domain name <code>opsmate.your-corp.com</code></li> <li>Can use external-dns to manage the ingress for the domain name.</li> <li>Have a wildcard <code>*.opsmate.your-corp.com</code> certificate in the <code>opsmate-workspace</code> namespace managed by cert-manager. Notes the wildcard certificate can now be provisioned by LetsEncrypt.</li> </ul> <p>After you create the task you can access the task via the following URL:</p> <pre><code>https://investigator.opsmate.your-corp.com?token=$(kubectl -n opsmate-workspace get task investigator -o jsonpath='{.status.token}')\n</code></pre>"},{"location":"CLI/","title":"CLI","text":"<p>This documentation highlights some of the most common use cases of Opsmate CLI tools.</p>"},{"location":"CLI/#natural-language-cli-run","title":"Natural Language CLI run","text":"<p>One of the most simple use case of Opsmate is to run commands using natural language. This comes handy when you need to run a command that you don't know/remember the exact instruction.</p> <pre><code>$ opsmate run \"what's the gpu of the vm\"\n                                                        Command\n# Check the GPU installed on the VM using lspci command and filter for VGA or compatible graphics device.\nlspci | grep -i 'vga\\|3d\\|2d'\n\n\n                                                        Output\n04:00.0 VGA compatible controller: Red Hat, Inc. Virtio 1.0 GPU (rev 01)\n\nThe VM is using a VGA compatible controller with a Red Hat, Inc. Virtio 1.0 GPU (rev 01).\n</code></pre>"},{"location":"CLI/#advanced-reasoning","title":"Advanced reasoning","text":"<p>A more advanced use case is to leverage Opsmate to perform reasoning and problem solving of production issues via using the <code>solve</code> command as you can see in the following example. Like a human SRE, Opsmate can make mistakes but with the advanced reasoning ability it can reflect on its mistakes and correct itself.</p> <pre><code>opsmate solve \"what's the k8s distro of the current context\"\n\nThought process\nThought: To determine the Kubernetes distribution of the current context, I need to access the Kubernetes configuration and context details.\nAction: Run the command kubectl version --short or check the Kubernetes configuration using kubectl config current-context to get information about the server and its version.\n\n...\n\n\nOutput\n error: unknown flag: --short\n See 'kubectl version --help' for usage.\n\n...\n\nThought: I need to run a valid command to get cluster details without the --short option.\nAction: Run kubectl version to get the full version details which might give us clues about the distribution in use.\n...\n\nAnswer: The Kubernetes distribution of the current context is K3s, as indicated by the +k3s1 suffix in the server version output from kubectl version.\n</code></pre>"},{"location":"CLI/#chat-with-opsmate","title":"Chat with Opsmate","text":"<p>To have the human-in-the-loop experience you can run</p> <pre><code>opsmate chat\n</code></pre>"},{"location":"CLI/#api-and-web-ui","title":"API and Web UI","text":"<p>To serve the Opsmate with a web interface and API you can run the following command:</p> <pre><code>opsmate serve\n</code></pre> <p>You can access the web interface at http://localhost:8080.</p> <p>API documentation is available at http://localhost:8080/api/docs.</p>"},{"location":"CLI/chat/","title":"opsmate chat","text":"<p><code>opsmate chat</code> allows you to use the Opsmate in an interactive chat interface.</p>"},{"location":"CLI/chat/#options","title":"OPTIONS","text":"<pre><code>Usage: opsmate chat [OPTIONS]\n\n  Chat with the Opsmate.\n\nOptions:\n  -i, --max-iter INTEGER          Max number of iterations the AI assistant\n                                  can reason about  [default: 10]\n  --tool-calls-per-action INTEGER\n                                  Number of tool calls per action  [default:\n                                  1]\n  --tools TEXT                    The tools to use for the session. Run\n                                  `opsmate list-tools` to see the available\n                                  tools. By default the tools from the context\n                                  are used. (env: OPSMATE_TOOLS)  [default:\n                                  \"\"]\n  --loglevel TEXT                 Set loglevel (env: OPSMATE_LOGLEVEL)\n                                  [default: INFO]\n  --categorise BOOLEAN            Whether to categorise the embeddings (env:\n                                  OPSMATE_CATEGORISE)  [default: True]\n  --reranker-name TEXT            The name of the reranker model (env:\n                                  OPSMATE_RERANKER_NAME)  [default: \"\"]\n  --embedding-model-name TEXT     The name of the embedding model (env:\n                                  OPSMATE_EMBEDDING_MODEL_NAME)  [default:\n                                  text-embedding-ada-002]\n  --embedding-registry-name TEXT  The name of the embedding registry (env:\n                                  OPSMATE_EMBEDDING_REGISTRY_NAME)  [default:\n                                  openai]\n  --embeddings-db-path TEXT       The path to the lance db. When s3:// is used\n                                  for AWS S3, az:// is used for Azure Blob\n                                  Storage, and gs:// is used for Google Cloud\n                                  Storage (env: OPSMATE_EMBEDDINGS_DB_PATH)\n                                  [default: /root/.opsmate/embeddings]\n  -c, --context TEXT              The context to use for the session. Run\n                                  `opsmate list-contexts` to see the available\n                                  contexts. (env: OPSMATE_CONTEXT)  [default:\n                                  cli]\n  --contexts-dir TEXT             Set contexts_dir (env: OPSMATE_CONTEXTS_DIR)\n                                  [default: /root/.opsmate/contexts]\n  --plugins-dir TEXT              Set plugins_dir (env: OPSMATE_PLUGINS_DIR)\n                                  [default: /root/.opsmate/plugins]\n  -m, --model TEXT                The model to use for the session. Run\n                                  `opsmate list-models` to see the available\n                                  models. (env: OPSMATE_MODEL)  [default:\n                                  gpt-4o]\n  --db-url TEXT                   Set db_url (env: OPSMATE_DB_URL)  [default:\n                                  sqlite:////root/.opsmate/opsmate.db]\n  --shell-command-runtime TEXT    The runtime to use for the tool call (env:\n                                  SHELL_COMMAND_RUNTIME)  [default: local]\n  --runtime-k8s-shell TEXT        Set shell_cmd (env: RUNTIME_K8S_SHELL)\n                                  [default: /bin/sh]\n  --runtime-k8s-container TEXT    Name of the container of the pod, if not\n                                  specified, the first container will be used\n                                  (env: RUNTIME_K8S_CONTAINER)\n  --runtime-k8s-pod TEXT          Set pod_name (env: RUNTIME_K8S_POD)\n                                  [default: \"\"]\n  --runtime-k8s-namespace TEXT    Set namespace (env: RUNTIME_K8S_NAMESPACE)\n                                  [default: default]\n  --runtime-ssh-connect-retries INTEGER\n                                  Set connect_retries (env:\n                                  RUNTIME_SSH_CONNECT_RETRIES)  [default: 3]\n  --runtime-ssh-timeout INTEGER   Set timeout (env: RUNTIME_SSH_TIMEOUT)\n                                  [default: 10]\n  --runtime-ssh-shell TEXT        Set shell_cmd (env: RUNTIME_SSH_SHELL)\n                                  [default: /bin/bash]\n  --runtime-ssh-key-file TEXT     Set key_file (env: RUNTIME_SSH_KEY_FILE)\n  --runtime-ssh-password TEXT     Set password (env: RUNTIME_SSH_PASSWORD)\n  --runtime-ssh-username TEXT     Set username (env: RUNTIME_SSH_USERNAME)\n                                  [default: \"\"]\n  --runtime-ssh-port INTEGER      Set port (env: RUNTIME_SSH_PORT)  [default:\n                                  22]\n  --runtime-ssh-host TEXT         Set host (env: RUNTIME_SSH_HOST)  [default:\n                                  \"\"]\n  --runtime-docker-service-name TEXT\n                                  Name of the service to run (env:\n                                  RUNTIME_DOCKER_SERVICE_NAME)  [default:\n                                  default]\n  --runtime-docker-compose-file TEXT\n                                  Path to the docker compose file (env:\n                                  RUNTIME_DOCKER_COMPOSE_FILE)  [default:\n                                  docker-compose.yml]\n  --runtime-docker-shell TEXT     Set shell_cmd (env: RUNTIME_DOCKER_SHELL)\n                                  [default: /bin/bash]\n  --runtime-docker-container-name TEXT\n                                  Set container_name (env:\n                                  RUNTIME_DOCKER_CONTAINER_NAME)  [default:\n                                  \"\"]\n  --runtime-local-shell TEXT      Set shell_cmd (env: RUNTIME_LOCAL_SHELL)\n                                  [default: /bin/bash]\n  -r, --review                    Review and edit commands before execution\n  -s, --system-prompt TEXT        System prompt to use\n  -l, --max-output-length INTEGER\n                                  Max length of the output, if the output is\n                                  truncated, the tmp file will be printed in\n                                  the output  [default: 10000]\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"CLI/chat/#usage","title":"USAGE","text":""},{"location":"CLI/chat/#basic","title":"Basic","text":"<p>Herer is the most basic usage of the <code>opsmate chat</code> command:</p> <pre><code>Opsmate&gt; Howdy! How can I help you?\n\nCommands:\n\n!clear - Clear the chat history\n!exit - Exit the chat\n!help - Show this message\n</code></pre>"},{"location":"CLI/chat/#with-a-system-prompt","title":"With a system prompt","text":"<p>You can use a system prompt with the <code>opsmate chat</code> command by using the <code>-s</code> or <code>--system-prompt</code> flag.</p> <pre><code>opsmate chat -s \"you are a rabbit\"\n2025-02-26 18:10:12 [info     ] adding the plugin directory to the sys path plugin_dir=/home/jingkaihe/.opsmate/plugins\nOpsmate&gt; Howdy! How can I help you?\n\nCommands:\n\n!clear - Clear the chat history\n!exit - Exit the chat\n!help - Show this message\n\nYou&gt; who are you\n\nAnswer\n\nI am a rabbit, here to assist you with your queries and tasks.\nYou&gt;\n</code></pre>"},{"location":"CLI/db-migrate/","title":"opsmate db-migrate","text":"<p><code>db-migrate</code> is used to update the sqlite database that powers the opsmate to the latest version. By default it migrates the database to the latest version.</p>"},{"location":"CLI/db-migrate/#usage","title":"USAGE","text":"<pre><code>Usage: opsmate db-migrate [OPTIONS]\n\n  Apply migrations.\n\nOptions:\n  -r, --revision TEXT  Revision to upgrade to  [default: head]\n  --help               Show this message and exit.\n</code></pre>"},{"location":"CLI/db-migrate/#examples","title":"EXAMPLES","text":""},{"location":"CLI/db-migrate/#migrating-to-the-latest-version","title":"Migrating to the latest version","text":"<pre><code>opsmate db-migrate\n</code></pre>"},{"location":"CLI/db-migrate/#migrating-to-a-specific-version","title":"Migrating to a specific version","text":"<pre><code>opsmate db-migrate --revision &lt;revision&gt;\n</code></pre> <p>To list all the versions available, run <code>opsmate db-revisions</code>.</p>"},{"location":"CLI/db-migrate/#see-also","title":"SEE ALSO","text":"<ul> <li>opsmate db-revisions</li> <li>opsmate db-rollback</li> </ul>"},{"location":"CLI/db-migrate/#options","title":"OPTIONS","text":"<pre><code>Usage: opsmate db-migrate [OPTIONS]\n\n  Apply migrations.\n\nOptions:\n  -r, --revision TEXT  Revision to upgrade to  [default: head]\n  --help               Show this message and exit.\n</code></pre>"},{"location":"CLI/db-revisions/","title":"opsmate db-revisions","text":"<p><code>db-revisions</code> shows all the revisions of the opsmate database.</p>"},{"location":"CLI/db-revisions/#options","title":"OPTIONS","text":"<pre><code>Usage: opsmate db-revisions [OPTIONS]\n\n  List all the revisions available.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre>"},{"location":"CLI/db-revisions/#see-also","title":"SEE ALSO","text":"<ul> <li>opsmate db-migrate</li> <li>opsmate db-rollback</li> </ul>"},{"location":"CLI/db-rollback/","title":"opsmate db-rollback","text":"<p><code>db-rollback</code> is used to rollback the opsmate database to the previous version.</p>"},{"location":"CLI/db-rollback/#options","title":"OPTIONS","text":"<pre><code>Usage: opsmate db-rollback [OPTIONS]\n\n  Rollback migrations.\n\nOptions:\n  -r, --revision TEXT  Revision to downgrade to  [default: -1]\n  --help               Show this message and exit.\n</code></pre>"},{"location":"CLI/db-rollback/#examples","title":"EXAMPLES","text":"<pre><code>opsmate db-rollback\n</code></pre> <p>To rollback to a specific version, run:</p> <pre><code>opsmate db-rollback --revision &lt;revision&gt;\n</code></pre> <p>To list all the versions available, run <code>opsmate db-revisions</code>.</p>"},{"location":"CLI/db-rollback/#see-also","title":"SEE ALSO","text":"<ul> <li>opsmate db-revisions</li> <li>opsmate db-migrate</li> </ul>"},{"location":"CLI/ingest-prometheus-metrics-metadata/","title":"opsmate ingest-prometheus-metrics-metadata","text":"<p><code>opsmate ingest-prometheus-metrics-metadata</code> ingests metrics metadata into the knowledge base.</p>"},{"location":"CLI/ingest-prometheus-metrics-metadata/#options","title":"OPTIONS","text":"<pre><code>Usage: opsmate ingest-prometheus-metrics-metadata [OPTIONS]\n\n  Ingest prometheus metrics metadata into the knowledge base. The ingestion is\n  done via fetching the metrics metadata from the prometheus server, and then\n  storing it into the knowledge base. The ingested metrics metadata will be\n  used for providing context to the LLM when querying prometheus based metrics\n  Note this only enqueues the tasks to ingest metrics. To execute the actual\n  ingestion in the background, run `opsmate worker`. Please run: `opsmate\n  worker -w 1 -q lancedb-batch-ingest`\n\nOptions:\n  --prometheus-endpoint TEXT      Prometheus endpoint. If not provided it uses\n                                  $PROMETHEUS_ENDPOINT environment variable,\n                                  or defaults to http://localhost:9090\n                                  [default: (dynamic)]\n  --prometheus-user-id TEXT       Prometheus user id. If not provided it uses\n                                  $PROMETHEUS_USER_ID environment variable, or\n                                  defaults to empty string  [default:\n                                  (dynamic)]\n  --prometheus-api-key TEXT       Prometheus api key. If not provided it uses\n                                  $PROMETHEUS_API_KEY environment variable, or\n                                  defaults to empty string  [default:\n                                  (dynamic)]\n  --tools TEXT                    The tools to use for the session. Run\n                                  `opsmate list-tools` to see the available\n                                  tools. By default the tools from the context\n                                  are used. (env: OPSMATE_TOOLS)  [default:\n                                  \"\"]\n  --loglevel TEXT                 Set loglevel (env: OPSMATE_LOGLEVEL)\n                                  [default: INFO]\n  --categorise BOOLEAN            Whether to categorise the embeddings (env:\n                                  OPSMATE_CATEGORISE)  [default: True]\n  --reranker-name TEXT            The name of the reranker model (env:\n                                  OPSMATE_RERANKER_NAME)  [default: \"\"]\n  --embedding-model-name TEXT     The name of the embedding model (env:\n                                  OPSMATE_EMBEDDING_MODEL_NAME)  [default:\n                                  text-embedding-ada-002]\n  --embedding-registry-name TEXT  The name of the embedding registry (env:\n                                  OPSMATE_EMBEDDING_REGISTRY_NAME)  [default:\n                                  openai]\n  --embeddings-db-path TEXT       The path to the lance db. When s3:// is used\n                                  for AWS S3, az:// is used for Azure Blob\n                                  Storage, and gs:// is used for Google Cloud\n                                  Storage (env: OPSMATE_EMBEDDINGS_DB_PATH)\n                                  [default: /root/.opsmate/embeddings]\n  -c, --context TEXT              The context to use for the session. Run\n                                  `opsmate list-contexts` to see the available\n                                  contexts. (env: OPSMATE_CONTEXT)  [default:\n                                  cli]\n  --contexts-dir TEXT             Set contexts_dir (env: OPSMATE_CONTEXTS_DIR)\n                                  [default: /root/.opsmate/contexts]\n  --plugins-dir TEXT              Set plugins_dir (env: OPSMATE_PLUGINS_DIR)\n                                  [default: /root/.opsmate/plugins]\n  -m, --model TEXT                The model to use for the session. Run\n                                  `opsmate list-models` to see the available\n                                  models. (env: OPSMATE_MODEL)  [default:\n                                  gpt-4o]\n  --db-url TEXT                   Set db_url (env: OPSMATE_DB_URL)  [default:\n                                  sqlite:////root/.opsmate/opsmate.db]\n  --auto-migrate BOOLEAN          Automatically migrate the database to the\n                                  latest version  [default: True]\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"CLI/ingest-prometheus-metrics-metadata/#description","title":"DESCRIPTION","text":"<p>The <code>ingest-prometheus-metrics-metadata</code> command enqueues tasks to ingest prometheus-based metrics metadata into the Opsmate knowledge base. The ingested metrics metadata will be used for improve the retrieval and precision of the Prometheus query results generated by the LLM.</p> <p>Note this only enqueues the tasks to ingest metrics. To execute the actual ingestion in the background, run <code>opsmate worker</code>. Please run: <code>opsmate worker -w 1 -q lancedb-batch-ingest</code></p>"},{"location":"CLI/ingest-prometheus-metrics-metadata/#examples","title":"EXAMPLES","text":""},{"location":"CLI/ingest-prometheus-metrics-metadata/#ingest-metrics-metadata-from-prometheus","title":"Ingest metrics metadata from Prometheus","text":"<p>The following command enqueues tasks to ingest metrics metadata from a Prometheus endpoint:</p> <pre><code>opsmate ingest-prometheus-metrics-metadata\n</code></pre> <p>By default, the command uses dynamic values for the Prometheus endpoint, user ID, and API key. You can specify these values explicitly as below.</p> <pre><code>opsmate ingest-prometheus-metrics-metadata \\\n  --prometheus-endpoint=\"https://prometheus-prod-01-eu-west-0.grafana.net/api/prom\" \\\n  --prometheus-user-id=\"xxxxx\" \\\n  --prometheus-api-key=\"glc_xxxx\"\n</code></pre> <p>If the endpoints are not provided, the command will use the values from the environment variables:</p> <pre><code>export PROMETHEUS_ENDPOINT=\"https://prometheus-prod-01-eu-west-0.grafana.net/api/prom\"\nexport PROMETHEUS_USER_ID=\"xxxxx\"\nexport PROMETHEUS_API_KEY=\"glc_xxxx\"\nopsmate ingest-prometheus-metrics-metadata\n</code></pre>"},{"location":"CLI/ingest-prometheus-metrics-metadata/#running-the-worker-to-process-the-ingestion-tasks","title":"Running the worker to process the ingestion tasks","text":"<p>After enqueuing the tasks, you need to run a worker to process them:</p> <pre><code>opsmate worker -w 1 -q lancedb-batch-ingest\n</code></pre>"},{"location":"CLI/ingest-prometheus-metrics-metadata/#see-also","title":"SEE ALSO","text":"<ul> <li>opsmate worker</li> <li>opsmate serve</li> <li>opsmate chat</li> </ul>"},{"location":"CLI/ingest/","title":"opsmate ingest","text":"<p><code>opsmate ingest</code> initiate the knowledge ingestion process.</p> <p>NOTE: The <code>ingest</code> command only initiates the ingestion process. As the process can be long running, the actual heavy lifting is handled by a <code>opsmate worker</code> process.</p>"},{"location":"CLI/ingest/#options","title":"OPTIONS","text":"<pre><code>Usage: opsmate ingest [OPTIONS]\n\n  Ingest a knowledge base. Notes the ingestion worker needs to be started\n  separately with `opsmate worker`.\n\nOptions:\n  --source TEXT                   Source of the knowledge base\n                                  fs:////path/to/kb or\n                                  github:///owner/repo[:branch]\n  --path TEXT                     Path to the knowledge base  [default: \"\"]\n  --glob TEXT                     Glob to use to find the knowledge base\n                                  [default: **/*.md]\n  --tools TEXT                    The tools to use for the session. Run\n                                  `opsmate list-tools` to see the available\n                                  tools. By default the tools from the context\n                                  are used. (env: OPSMATE_TOOLS)  [default:\n                                  \"\"]\n  --loglevel TEXT                 Set loglevel (env: OPSMATE_LOGLEVEL)\n                                  [default: INFO]\n  --categorise BOOLEAN            Whether to categorise the embeddings (env:\n                                  OPSMATE_CATEGORISE)  [default: True]\n  --reranker-name TEXT            The name of the reranker model (env:\n                                  OPSMATE_RERANKER_NAME)  [default: \"\"]\n  --embedding-model-name TEXT     The name of the embedding model (env:\n                                  OPSMATE_EMBEDDING_MODEL_NAME)  [default:\n                                  text-embedding-ada-002]\n  --embedding-registry-name TEXT  The name of the embedding registry (env:\n                                  OPSMATE_EMBEDDING_REGISTRY_NAME)  [default:\n                                  openai]\n  --embeddings-db-path TEXT       The path to the lance db. When s3:// is used\n                                  for AWS S3, az:// is used for Azure Blob\n                                  Storage, and gs:// is used for Google Cloud\n                                  Storage (env: OPSMATE_EMBEDDINGS_DB_PATH)\n                                  [default: /root/.opsmate/embeddings]\n  -c, --context TEXT              The context to use for the session. Run\n                                  `opsmate list-contexts` to see the available\n                                  contexts. (env: OPSMATE_CONTEXT)  [default:\n                                  cli]\n  --contexts-dir TEXT             Set contexts_dir (env: OPSMATE_CONTEXTS_DIR)\n                                  [default: /root/.opsmate/contexts]\n  --plugins-dir TEXT              Set plugins_dir (env: OPSMATE_PLUGINS_DIR)\n                                  [default: /root/.opsmate/plugins]\n  -m, --model TEXT                The model to use for the session. Run\n                                  `opsmate list-models` to see the available\n                                  models. (env: OPSMATE_MODEL)  [default:\n                                  gpt-4o]\n  --db-url TEXT                   Set db_url (env: OPSMATE_DB_URL)  [default:\n                                  sqlite:////root/.opsmate/opsmate.db]\n  --auto-migrate BOOLEAN          Automatically migrate the database to the\n                                  latest version  [default: True]\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"CLI/ingest/#examples","title":"EXAMPLES","text":""},{"location":"CLI/ingest/#ingest-a-knowledge-base-from-github","title":"Ingest a knowledge base from github","text":"<pre><code>opsmate ingest \\\n    --source github:///kubernetes-sigs/kubebuilder:master \\\n    --path docs/book/src/reference\n</code></pre> <p>Once you start running <code>opsmate worker</code> the ingestion process will start.</p>"},{"location":"CLI/ingest/#see-also","title":"SEE ALSO","text":"<ul> <li>opsmate worker</li> <li>opsmate serve</li> </ul>"},{"location":"CLI/install/","title":"opsmate install","text":"<p><code>opsmate install</code> installs Opsmate plugins as python packages.</p> <p>Currently there are two types python package based plugins that are supported:</p> <ul> <li><code>opsmate-provider-*</code>: These are the language model providers.</li> <li><code>opsmate-runtime-*</code>: These are the runtime environments that can be used to run the Opsmate.</li> </ul>"},{"location":"CLI/install/#options","title":"OPTIONS","text":"<pre><code>Usage: opsmate install [OPTIONS] [PACKAGES]...\n\n  Install the opsmate plugins.\n\nOptions:\n  -U, --upgrade        Upgrade the given packages to the latest version\n  --force-reinstall    Reinstall all packages even if they are already up-to-\n                       date\n  -e, --editable TEXT  Install a project in editable mode (i.e. setuptools\n                       \"develop mode\") from a local project path or a VCS url\n  --no-cache-dir       Disable the cache\n  --help               Show this message and exit.\n</code></pre>"},{"location":"CLI/install/#see-also","title":"SEE ALSO","text":"<ul> <li>Add New LLM Providers</li> <li>Integrate with New Runtime</li> <li>Uninstall</li> </ul>"},{"location":"CLI/list-contexts/","title":"opsmate list-contexts","text":"<p><code>opsmate list-contexts</code> lists all the contexts available.</p>"},{"location":"CLI/list-contexts/#options","title":"OPTIONS","text":"<pre><code>Usage: opsmate list-contexts [OPTIONS]\n\n  List all the contexts available.\n\nOptions:\n  --tools TEXT                    The tools to use for the session. Run\n                                  `opsmate list-tools` to see the available\n                                  tools. By default the tools from the context\n                                  are used. (env: OPSMATE_TOOLS)  [default:\n                                  \"\"]\n  --loglevel TEXT                 Set loglevel (env: OPSMATE_LOGLEVEL)\n                                  [default: INFO]\n  --categorise BOOLEAN            Whether to categorise the embeddings (env:\n                                  OPSMATE_CATEGORISE)  [default: True]\n  --reranker-name TEXT            The name of the reranker model (env:\n                                  OPSMATE_RERANKER_NAME)  [default: \"\"]\n  --embedding-model-name TEXT     The name of the embedding model (env:\n                                  OPSMATE_EMBEDDING_MODEL_NAME)  [default:\n                                  text-embedding-ada-002]\n  --embedding-registry-name TEXT  The name of the embedding registry (env:\n                                  OPSMATE_EMBEDDING_REGISTRY_NAME)  [default:\n                                  openai]\n  --embeddings-db-path TEXT       The path to the lance db. When s3:// is used\n                                  for AWS S3, az:// is used for Azure Blob\n                                  Storage, and gs:// is used for Google Cloud\n                                  Storage (env: OPSMATE_EMBEDDINGS_DB_PATH)\n                                  [default: /root/.opsmate/embeddings]\n  -c, --context TEXT              The context to use for the session. Run\n                                  `opsmate list-contexts` to see the available\n                                  contexts. (env: OPSMATE_CONTEXT)  [default:\n                                  cli]\n  --contexts-dir TEXT             Set contexts_dir (env: OPSMATE_CONTEXTS_DIR)\n                                  [default: /root/.opsmate/contexts]\n  --plugins-dir TEXT              Set plugins_dir (env: OPSMATE_PLUGINS_DIR)\n                                  [default: /root/.opsmate/plugins]\n  -m, --model TEXT                The model to use for the session. Run\n                                  `opsmate list-models` to see the available\n                                  models. (env: OPSMATE_MODEL)  [default:\n                                  gpt-4o]\n  --db-url TEXT                   Set db_url (env: OPSMATE_DB_URL)  [default:\n                                  sqlite:////root/.opsmate/opsmate.db]\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"CLI/list-contexts/#usage","title":"USAGE","text":""},{"location":"CLI/list-contexts/#basic","title":"Basic","text":"<pre><code>opsmate list-contexts\n\n                                    Contexts\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Context   \u2503 Description                                                       \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 cli       \u2502 General purpose context for solving problems on the command line. \u2502\n\u2502 k8s       \u2502 Kubernetes context for solving problems on Kubernetes.            \u2502\n\u2502 terraform \u2502 Terraform context for running Terraform based IaC commands.       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"CLI/list-contexts/#adding-custom-contexts","title":"Adding custom contexts","text":"<p>You can also add custom contexts with the help from the opsmate plugin system.</p> <p>The default location is <code>$HOME/.opsmate/contexts</code>. You can change this by setting the <code>OPSMATE_CONTEXTS_DIR</code> environment variable.</p> <p>In the example below we added a <code>gcloud</code> context by <code>$HOME/.opsmate/contexts/gcloud.py</code> directory:</p> <pre><code>from opsmate.dino.context import context\nfrom opsmate.plugins import PluginRegistry\nimport asyncio\n\n\n@context(\n    name=\"gcloud\",\n    tools=[\n        PluginRegistry.get_tool(\"ShellCommand\"),\n        PluginRegistry.get_tool(\"ACITool\"),\n        PluginRegistry.get_tool(\"KnowledgeRetrieval\"),\n        PluginRegistry.get_tool(\"HtmlToText\"),\n    ],\n)\nasync def gcloud_ctx() -&gt; str:\n    \"\"\"GCP SME\"\"\"\n    tasks = {\n        \"gcloud_project\": __gcloud_project(),\n        \"gcloud_region\": __gcloud_region(),\n        \"gcloud_projects\": __list_gcloud_projects(),\n    }\n    results = await asyncio.gather(*tasks.values())\n    results = dict(zip(tasks.keys(), results))\n    \"\"\"gcloud sme\"\"\"\n\n    return f\"\"\"\n&lt;assistant&gt;\nYou are a world class SRE who is an expert in gcloud. You are tasked to help with gcloud related problem solving\n&lt;/assistant&gt;\n\n&lt;useful_info&gt;\ngcloud_project: {results[\"gcloud_project\"]}\ngcloud_region: {results[\"gcloud_region\"]}\ngcloud_projects:\n    &lt;gcloud_projects&gt;\n    {results[\"gcloud_projects\"]}\n    &lt;/gcloud_projects&gt;\n&lt;/useful_info&gt;\n\n&lt;important&gt;\n- When you believe the output of `gcloud` command is big, please feel free to use the `tail` command to limit the number of lines.\n- If the output contains `&lt;truncated&gt;...&lt;/truncated&gt;` it indicates that the output is truncated. If you believe some of the important context are missing, view the tmp file specified using the ACITool to see the missing lines.\n- You should also use `format` to limit the number of lines of the output.\n&lt;/important&gt;\n\"\"\"\n\n\nasync def __gcloud_project():\n    return await __run_cmd(\"gcloud config get-value project\")\n\n\nasync def __gcloud_region():\n    return await __run_cmd(\"gcloud config get-value compute/region\")\n\n\nasync def __list_gcloud_projects():\n    return await __run_cmd(\n        \"gcloud projects list --format='value(projectId)' | tail -n 100\"\n    )\n\n\nasync def __run_cmd(cmd: str):\n    process = await asyncio.subprocess.create_subprocess_shell(\n        cmd,\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE,\n    )\n    stdout, _ = await asyncio.wait_for(process.communicate(), timeout=10)\n\n    return stdout.decode().strip()\n</code></pre> <p>Now if you run <code>opsmate list-contexts</code> you will see the <code>gcloud</code> context:</p> <pre><code>$ opsmate list-contexts\n2025-02-28 13:52:15 [info     ] adding the plugin directory to the sys path plugin_dir=/home/jingkaihe/.opsmate/plugins\n2025-02-28 13:52:15 [info     ] adding the context directory to the sys path context_dir=/home/jingkaihe/.opsmate/contexts\n2025-02-28 13:52:15 [info     ] loading context file           context_path=/home/jingkaihe/.opsmate/contexts/gcloud.py\n2025-02-28 13:52:15 [info     ] loaded context file            context_path=/home/jingkaihe/.opsmate/contexts/gcloud.py\n               Contexts\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Context   \u2503 Description            \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 cli       \u2502 System Admin Assistant \u2502\n\u2502 k8s       \u2502 Kubernetes SME         \u2502\n\u2502 terraform \u2502 Terraform SME          \u2502\n\u2502 gcloud    \u2502 GCP SME                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After adding the plugin, you can use the <code>gcloud</code> context via using the <code>-c</code> flag:</p> <pre><code>$ opsmate solve -c gcloud \"what is the current gcp project\"\n2025-02-28 13:49:00 [info     ] adding the plugin directory to the sys path plugin_dir=/home/jingkaihe/.opsmate/plugins\n2025-02-28 13:49:00 [info     ] adding the context directory to the sys path context_dir=/home/jingkaihe/.opsmate/contexts\n2025-02-28 13:49:00 [info     ] loading context file           context_path=/home/jingkaihe/.opsmate/contexts/gcloud.py\n2025-02-28 13:49:00 [info     ] loaded context file            context_path=/home/jingkaihe/.opsmate/contexts/gcloud.py\n\n                                                               Answer\n\nThe current GCP project is \"THE-CURRENT-GCP-PROJECT\".\n</code></pre> <p>Note the <code>--context</code> flag is available to run, solve and chat commands.</p>"},{"location":"CLI/list-contexts/#see-also","title":"SEE ALSO","text":"<ul> <li>opsmate run</li> <li>opsmate solve</li> <li>opsmate chat</li> </ul>"},{"location":"CLI/list-models/","title":"opsmate list-models","text":"<p><code>opsmate list-models</code> lists all the models available.</p> <p>Currently on Opsmate we cherry-pick the models that are suitable for performing SRE/DevOps oriented tasks that being said in the future we will look into supporting extra models through the plugin system.</p>"},{"location":"CLI/list-models/#options","title":"OPTIONS","text":"<pre><code>Usage: opsmate list-models [OPTIONS]\n\n  List all the models available.\n\nOptions:\n  --provider TEXT  Provider to list the models for\n  --help           Show this message and exit.\n</code></pre>"},{"location":"CLI/list-models/#usage","title":"USAGE","text":"<pre><code>                  Models\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Provider  \u2503 Model                      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 openai    \u2502 gpt-4o                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 openai    \u2502 gpt-4o-mini                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 openai    \u2502 o1                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 openai    \u2502 o3-mini                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 anthropic \u2502 claude-3-5-sonnet-20241022 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 anthropic \u2502 claude-3-7-sonnet-20250219 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 xai       \u2502 grok-2-1212                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 xai       \u2502 grok-2-vision-1212         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 xai       \u2502 grok-3-mini-fast-beta      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 xai       \u2502 grok-3-mini-beta           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 xai       \u2502 grok-3-fast-beta           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 xai       \u2502 grok-3-beta                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"CLI/list-models/#see-also","title":"SEE ALSO","text":"<ul> <li>Add new LLM providers</li> </ul>"},{"location":"CLI/list-runtimes/","title":"List runtimes","text":"<p><code>opsmate list-runtimes</code> lists all the runtimes available.</p>"},{"location":"CLI/list-runtimes/#options","title":"OPTIONS","text":"<pre><code>Usage: opsmate list-runtimes [OPTIONS]\n\n  List all the runtimes available.\n\nOptions:\n  --help  Show this message and exit.\n</code></pre>"},{"location":"CLI/list-runtimes/#usage","title":"USAGE","text":"<p>The command below will list all the runtimes available to Opsmate.</p> <pre><code>opsmate list-runtimes\n                                                   Runtimes\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name   \u2503 Description                                                                                        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 local  \u2502 Local runtime allows model to execute tool calls within the same namespace as the opsmate process. \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 docker \u2502 Docker runtime allows model to execute tool calls within a docker container.                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ssh    \u2502 SSH runtime allows model to execute tool calls on a remote server via SSH.                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"CLI/list-tools/","title":"opsmate list-tools","text":"<p><code>opsmate list-tools</code> lists all the tools available.</p>"},{"location":"CLI/list-tools/#options","title":"OPTIONS","text":"<pre><code>Usage: opsmate list-tools [OPTIONS]\n\n  List all the tools available.\n\nOptions:\n  --tools TEXT                    The tools to use for the session. Run\n                                  `opsmate list-tools` to see the available\n                                  tools. By default the tools from the context\n                                  are used. (env: OPSMATE_TOOLS)  [default:\n                                  \"\"]\n  --loglevel TEXT                 Set loglevel (env: OPSMATE_LOGLEVEL)\n                                  [default: INFO]\n  --categorise BOOLEAN            Whether to categorise the embeddings (env:\n                                  OPSMATE_CATEGORISE)  [default: True]\n  --reranker-name TEXT            The name of the reranker model (env:\n                                  OPSMATE_RERANKER_NAME)  [default: \"\"]\n  --embedding-model-name TEXT     The name of the embedding model (env:\n                                  OPSMATE_EMBEDDING_MODEL_NAME)  [default:\n                                  text-embedding-ada-002]\n  --embedding-registry-name TEXT  The name of the embedding registry (env:\n                                  OPSMATE_EMBEDDING_REGISTRY_NAME)  [default:\n                                  openai]\n  --embeddings-db-path TEXT       The path to the lance db. When s3:// is used\n                                  for AWS S3, az:// is used for Azure Blob\n                                  Storage, and gs:// is used for Google Cloud\n                                  Storage (env: OPSMATE_EMBEDDINGS_DB_PATH)\n                                  [default: /root/.opsmate/embeddings]\n  -c, --context TEXT              The context to use for the session. Run\n                                  `opsmate list-contexts` to see the available\n                                  contexts. (env: OPSMATE_CONTEXT)  [default:\n                                  cli]\n  --contexts-dir TEXT             Set contexts_dir (env: OPSMATE_CONTEXTS_DIR)\n                                  [default: /root/.opsmate/contexts]\n  --plugins-dir TEXT              Set plugins_dir (env: OPSMATE_PLUGINS_DIR)\n                                  [default: /root/.opsmate/plugins]\n  -m, --model TEXT                The model to use for the session. Run\n                                  `opsmate list-models` to see the available\n                                  models. (env: OPSMATE_MODEL)  [default:\n                                  gpt-4o]\n  --db-url TEXT                   Set db_url (env: OPSMATE_DB_URL)  [default:\n                                  sqlite:////root/.opsmate/opsmate.db]\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"CLI/list-tools/#usage","title":"USAGE","text":"<p>The command below will list all the tools available to Opsmate. Notes the plugins can be installed in the <code>~/.opsmate/plugins</code> directory. The example you see below only lists all the built-in tools.</p> <pre><code>opsmate list-tools\n2025-02-26 12:03:25 [info     ] adding the plugin directory to the sys path plugin_dir=/home/your-username/.opsmate/plugins\n                                                   Tools\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Tool                \u2503 Description                                                                        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 ACITool             \u2502                                                                                    \u2502\n\u2502                     \u2502     # ACITool                                                                      \u2502\n\u2502                     \u2502                                                                                    \u2502\n\u2502                     \u2502     File system utility with the following commands:                               \u2502\n\u2502                     \u2502                                                                                    \u2502\n\u2502                     \u2502     search &lt;file|dir&gt; &lt;content&gt;           # Search in file/directory               \u2502\n\u2502                     \u2502     view &lt;file|dir&gt;          # View file (optional 0-indexed line range) or        \u2502\n\u2502                     \u2502 directory                                                                          \u2502\n\u2502                     \u2502     create &lt;file&gt; &lt;content&gt;          # Create new file                             \u2502\n\u2502                     \u2502     update &lt;file&gt; &lt;old&gt; &lt;new&gt;         # Replace content (old must be unique), with \u2502\n\u2502                     \u2502 optional 0-indexed line range                                                      \u2502\n\u2502                     \u2502     append &lt;file&gt; &lt;line&gt; &lt;content&gt;   # Insert at line number                       \u2502\n\u2502                     \u2502     undo &lt;file&gt;                      # Undo last file change                       \u2502\n\u2502                     \u2502                                                                                    \u2502\n\u2502                     \u2502     Notes:                                                                         \u2502\n\u2502                     \u2502     - Line numbers are 0-indexed                                                   \u2502\n\u2502                     \u2502     - Directory view: 2-depth, ignores dotfiles                                    \u2502\n\u2502                     \u2502     - Empty new content in update deletes old content                              \u2502\n\u2502                     \u2502                                                                                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 FileAppend          \u2502 FileAppend tool allows you to append to a file                                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 FileDelete          \u2502 FileDelete tool allows you to delete a file                                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 FileRead            \u2502 FileRead tool allows you to read a file                                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 FileWrite           \u2502 FileWrite tool allows you to write to a file                                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 FilesFind           \u2502 FilesFind tool allows you to find files in a directory                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 FilesList           \u2502 FilesList tool allows you to list files in a directory recursively                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 GithubCloneAndCD    \u2502                                                                                    \u2502\n\u2502                     \u2502     Clone a github repository and cd into the directory                            \u2502\n\u2502                     \u2502                                                                                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 GithubRaisePR       \u2502                                                                                    \u2502\n\u2502                     \u2502     Raise a PR for a given github repository                                       \u2502\n\u2502                     \u2502                                                                                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 HtmlToText          \u2502 HtmlToText tool allows you to convert an HTTP response to text                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 HttpCall            \u2502                                                                                    \u2502\n\u2502                     \u2502     HttpCall tool allows you to call a URL                                         \u2502\n\u2502                     \u2502     Supports POST, PUT, DELETE, PATCH                                              \u2502\n\u2502                     \u2502                                                                                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 HttpGet             \u2502 HttpGet tool allows you to get the content of a URL                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 KnowledgeRetrieval  \u2502                                                                                    \u2502\n\u2502                     \u2502     Knowledge retrieval tool allows you to search for relevant knowledge from the  \u2502\n\u2502                     \u2502 knowledge base.                                                                    \u2502\n\u2502                     \u2502                                                                                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ShellCommand        \u2502                                                                                    \u2502\n\u2502                     \u2502     ShellCommand tool allows you to run shell commands and get the output.         \u2502\n\u2502                     \u2502                                                                                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 SysEnv              \u2502 SysEnv tool allows you to get the environment variables                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 SysStats            \u2502 SysStats tool allows you to get the stats of a file                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 current_time        \u2502                                                                                    \u2502\n\u2502                     \u2502     Get the current time in %Y-%m-%dT%H:%M:%SZ format                              \u2502\n\u2502                     \u2502                                                                                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 datetime_extraction \u2502                                                                                    \u2502\n\u2502                     \u2502     You are tasked to extract the datetime range from the text                     \u2502\n\u2502                     \u2502                                                                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"CLI/reset/","title":"opsmate reset","text":"<p><code>opsmate reset</code> deletes all the data used by Opsmate. Note that it DOES NOT delete the plugins.</p>"},{"location":"CLI/reset/#options","title":"OPTIONS","text":"<pre><code>Usage: opsmate reset [OPTIONS]\n\n  Reset the Opsmate database and embeddings db. Note that if the database is\n  using litestream it will not be reset. Same applies to the embeddings db, if\n  the embedding db is using GCS, S3 or Azure Blob Storage, it will not be\n  reset.\n\nOptions:\n  --tools TEXT                    The tools to use for the session. Run\n                                  `opsmate list-tools` to see the available\n                                  tools. By default the tools from the context\n                                  are used. (env: OPSMATE_TOOLS)  [default:\n                                  \"\"]\n  --loglevel TEXT                 Set loglevel (env: OPSMATE_LOGLEVEL)\n                                  [default: INFO]\n  --categorise BOOLEAN            Whether to categorise the embeddings (env:\n                                  OPSMATE_CATEGORISE)  [default: True]\n  --reranker-name TEXT            The name of the reranker model (env:\n                                  OPSMATE_RERANKER_NAME)  [default: \"\"]\n  --embedding-model-name TEXT     The name of the embedding model (env:\n                                  OPSMATE_EMBEDDING_MODEL_NAME)  [default:\n                                  text-embedding-ada-002]\n  --embedding-registry-name TEXT  The name of the embedding registry (env:\n                                  OPSMATE_EMBEDDING_REGISTRY_NAME)  [default:\n                                  openai]\n  --embeddings-db-path TEXT       The path to the lance db. When s3:// is used\n                                  for AWS S3, az:// is used for Azure Blob\n                                  Storage, and gs:// is used for Google Cloud\n                                  Storage (env: OPSMATE_EMBEDDINGS_DB_PATH)\n                                  [default: /root/.opsmate/embeddings]\n  -c, --context TEXT              The context to use for the session. Run\n                                  `opsmate list-contexts` to see the available\n                                  contexts. (env: OPSMATE_CONTEXT)  [default:\n                                  cli]\n  --contexts-dir TEXT             Set contexts_dir (env: OPSMATE_CONTEXTS_DIR)\n                                  [default: /root/.opsmate/contexts]\n  --plugins-dir TEXT              Set plugins_dir (env: OPSMATE_PLUGINS_DIR)\n                                  [default: /root/.opsmate/plugins]\n  -m, --model TEXT                The model to use for the session. Run\n                                  `opsmate list-models` to see the available\n                                  models. (env: OPSMATE_MODEL)  [default:\n                                  gpt-4o]\n  --db-url TEXT                   Set db_url (env: OPSMATE_DB_URL)  [default:\n                                  sqlite:////root/.opsmate/opsmate.db]\n  --skip-confirm                  Skip confirmation\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"CLI/reset/#examples","title":"EXAMPLES","text":""},{"location":"CLI/reset/#reset-the-opsmate","title":"Reset the Opsmate","text":"<p>This will reset the database and the vector store. You will be prompted to confirm the reset.</p> <pre><code>opsmate reset\n</code></pre>"},{"location":"CLI/reset/#reset-the-opsmate-without-confirmation","title":"Reset the Opsmate without confirmation","text":"<p>This will reset the databases without confirmation.</p> <pre><code>opsmate reset --skip-confirm\n</code></pre>"},{"location":"CLI/run/","title":"opsmate run","text":"<p><code>opsmate run</code> executes a command and returns the output.</p>"},{"location":"CLI/run/#options","title":"OPTIONS","text":"<pre><code>Usage: opsmate run [OPTIONS] INSTRUCTION\n\n  Run a task with the Opsmate.\n\nOptions:\n  -nt, --no-tool-output           Do not print tool outputs\n  -no, --no-observation           Do not print observation\n  --tools TEXT                    The tools to use for the session. Run\n                                  `opsmate list-tools` to see the available\n                                  tools. By default the tools from the context\n                                  are used. (env: OPSMATE_TOOLS)  [default:\n                                  \"\"]\n  --loglevel TEXT                 Set loglevel (env: OPSMATE_LOGLEVEL)\n                                  [default: INFO]\n  --categorise BOOLEAN            Whether to categorise the embeddings (env:\n                                  OPSMATE_CATEGORISE)  [default: True]\n  --reranker-name TEXT            The name of the reranker model (env:\n                                  OPSMATE_RERANKER_NAME)  [default: \"\"]\n  --embedding-model-name TEXT     The name of the embedding model (env:\n                                  OPSMATE_EMBEDDING_MODEL_NAME)  [default:\n                                  text-embedding-ada-002]\n  --embedding-registry-name TEXT  The name of the embedding registry (env:\n                                  OPSMATE_EMBEDDING_REGISTRY_NAME)  [default:\n                                  openai]\n  --embeddings-db-path TEXT       The path to the lance db. When s3:// is used\n                                  for AWS S3, az:// is used for Azure Blob\n                                  Storage, and gs:// is used for Google Cloud\n                                  Storage (env: OPSMATE_EMBEDDINGS_DB_PATH)\n                                  [default: /root/.opsmate/embeddings]\n  -c, --context TEXT              The context to use for the session. Run\n                                  `opsmate list-contexts` to see the available\n                                  contexts. (env: OPSMATE_CONTEXT)  [default:\n                                  cli]\n  --contexts-dir TEXT             Set contexts_dir (env: OPSMATE_CONTEXTS_DIR)\n                                  [default: /root/.opsmate/contexts]\n  --plugins-dir TEXT              Set plugins_dir (env: OPSMATE_PLUGINS_DIR)\n                                  [default: /root/.opsmate/plugins]\n  -m, --model TEXT                The model to use for the session. Run\n                                  `opsmate list-models` to see the available\n                                  models. (env: OPSMATE_MODEL)  [default:\n                                  gpt-4o]\n  --db-url TEXT                   Set db_url (env: OPSMATE_DB_URL)  [default:\n                                  sqlite:////root/.opsmate/opsmate.db]\n  --shell-command-runtime TEXT    The runtime to use for the tool call (env:\n                                  SHELL_COMMAND_RUNTIME)  [default: local]\n  --runtime-k8s-shell TEXT        Set shell_cmd (env: RUNTIME_K8S_SHELL)\n                                  [default: /bin/sh]\n  --runtime-k8s-container TEXT    Name of the container of the pod, if not\n                                  specified, the first container will be used\n                                  (env: RUNTIME_K8S_CONTAINER)\n  --runtime-k8s-pod TEXT          Set pod_name (env: RUNTIME_K8S_POD)\n                                  [default: \"\"]\n  --runtime-k8s-namespace TEXT    Set namespace (env: RUNTIME_K8S_NAMESPACE)\n                                  [default: default]\n  --runtime-ssh-connect-retries INTEGER\n                                  Set connect_retries (env:\n                                  RUNTIME_SSH_CONNECT_RETRIES)  [default: 3]\n  --runtime-ssh-timeout INTEGER   Set timeout (env: RUNTIME_SSH_TIMEOUT)\n                                  [default: 10]\n  --runtime-ssh-shell TEXT        Set shell_cmd (env: RUNTIME_SSH_SHELL)\n                                  [default: /bin/bash]\n  --runtime-ssh-key-file TEXT     Set key_file (env: RUNTIME_SSH_KEY_FILE)\n  --runtime-ssh-password TEXT     Set password (env: RUNTIME_SSH_PASSWORD)\n  --runtime-ssh-username TEXT     Set username (env: RUNTIME_SSH_USERNAME)\n                                  [default: \"\"]\n  --runtime-ssh-port INTEGER      Set port (env: RUNTIME_SSH_PORT)  [default:\n                                  22]\n  --runtime-ssh-host TEXT         Set host (env: RUNTIME_SSH_HOST)  [default:\n                                  \"\"]\n  --runtime-docker-service-name TEXT\n                                  Name of the service to run (env:\n                                  RUNTIME_DOCKER_SERVICE_NAME)  [default:\n                                  default]\n  --runtime-docker-compose-file TEXT\n                                  Path to the docker compose file (env:\n                                  RUNTIME_DOCKER_COMPOSE_FILE)  [default:\n                                  docker-compose.yml]\n  --runtime-docker-shell TEXT     Set shell_cmd (env: RUNTIME_DOCKER_SHELL)\n                                  [default: /bin/bash]\n  --runtime-docker-container-name TEXT\n                                  Set container_name (env:\n                                  RUNTIME_DOCKER_CONTAINER_NAME)  [default:\n                                  \"\"]\n  --runtime-local-shell TEXT      Set shell_cmd (env: RUNTIME_LOCAL_SHELL)\n                                  [default: /bin/bash]\n  -r, --review                    Review and edit commands before execution\n  -s, --system-prompt TEXT        System prompt to use\n  -l, --max-output-length INTEGER\n                                  Max length of the output, if the output is\n                                  truncated, the tmp file will be printed in\n                                  the output  [default: 10000]\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"CLI/run/#usage","title":"USAGE","text":""},{"location":"CLI/run/#simple-command","title":"Simple command","text":"<p>This is the most basic usage of <code>opsmate run</code>, it will execute the command based on the natural language instruction.</p> <pre><code>opsmate run \"what's the linux distribution?\"\n</code></pre>"},{"location":"CLI/run/#execute-command-with-review","title":"Execute command with review","text":"<p>By default the command will be executed immediately without any review. You can use the <code>--review</code> flag to review the command before execution. Instead of a \"yes\" or \"no\" confirmation, you will be able to edit the command before execution.</p> <pre><code>opsmate run \"what's the linux distribution?\" --review\n...\nEdit the command if needed, then press Enter to execute: !cancel - Cancel the command\nPress Enter or edit the command (cat /etc/os-release): cat /etc/os-release | grep '^PRETTY_NAME'\n...\n</code></pre>"},{"location":"CLI/run/#execute-command-with-different-model","title":"Execute command with different model","text":"<p>By default, the model is <code>gpt-4o</code>, but you can use a different model for command execution.</p> <pre><code>opsmate run \"what's the linux distribution?\" -m gpt-4o-mini\n</code></pre>"},{"location":"CLI/run/#execute-command-with-different-context","title":"Execute command with different context","text":"<p>Context is represents a collection of tools and prompts. By default, the context is <code>cli</code>, but you can create your own context or use the predefined contexts as shown below.</p> <pre><code>opsmate run \"how many pods are running in the cluster?\" -c k8s\n</code></pre>"},{"location":"CLI/run/#execute-command-with-different-system-prompt","title":"Execute command with different system prompt","text":"<p>You can use the <code>--system-prompt</code> or <code>-s</code> flag to use a different system prompt.</p> <pre><code>opsmate run -s \"You are a kubernetes SME\" \"how many pods are running in the cluster?\"\n</code></pre>"},{"location":"CLI/run/#execute-command-with-different-tools","title":"Execute command with different tools","text":"<p>You can also use the <code>--tools</code> or <code>-t</code> flag to use a different tools. The tools are comma separated values. The example below shows how to use the <code>HtmlToText</code> tool to get top 10 news on the hacker news.</p> <pre><code>$ opsmate run -n \"find me top 10 news on the hacker news, title only\" --tools HtmlToText\n2025-02-26 15:14:44 [info     ] adding the plugin directory to the sys path plugin_dir=/home/jingkaihe/.opsmate/plugins\n2025-02-26 15:14:44 [info     ] Running on                     instruction=find me top 10 news on the hacker news, title only model=gpt-4o\n1. The FFT Strikes Back: An Efficient Alternative to Self-Attention\n2. Telescope \u2013 an open-source web-based log viewer for logs stored in ClickHouse\n3. I Went to SQL Injection Court\n4. DeepGEMM: clean and efficient FP8 GEMM kernels with fine-grained scaling\n5. The Miserable State of Modems and Mobile Network Operators\n6. Hyperspace\n7. Material Theme has been pulled from VS Code's marketplace\n8. State of emergency declared after blackout plunges most of Chile into darkness\n9. Part two of Grant Sanderson's video with Terry Tao on the cosmic distance ladder\n10. Launch HN: Browser Use (YC W25) \u2013 open-source web agents\n</code></pre> <p>In the example above we also use the <code>-n</code> flag to suppress the tool outputs.</p>"},{"location":"CLI/run/#pipeline","title":"Pipeline","text":"<p>When the <code>INSTRUCTION</code> is <code>-</code>, the CLI will read the instruction from the standard input. With this you can chain the commands together.</p> <p>For example</p> <pre><code>cat instructions.txt | opsmate run -\n</code></pre> <p>Or chaining the <code>opsmate run</code> commands together.</p> <pre><code>opsmate run -n \"how many cores on the machine\" | opsmate run - -n -s \"print the number * 2 from the text you are given\"\n2025-02-26 15:23:07 [info     ] adding the plugin directory to the sys path plugin_dir=/home/jingkaihe/.opsmate/plugins\n2025-02-26 15:23:10 [info     ] Running on                     instruction=2025-02-26 15:23:07 [info     ] adding the plugin directory to the sys path plugin_dir=/home/jingkaihe/.opsmate/plugins\n2025-02-26 15:23:07 [info     ] Running on                     instruction=how many cores on the machine model=gpt-4o\n2025-02-26 15:23:08 [info     ] running shell command          command=nproc\n8 model=gpt-4o\n2025-02-26 15:23:11 [info     ] running shell command          command=echo $((8 * 2))\n16\n</code></pre>"},{"location":"CLI/run/#see-also","title":"SEE ALSO","text":"<ul> <li>opsmate solve</li> <li>opsmate chat</li> <li>opsmate list-contexts</li> <li>opsmate list-tools</li> <li>opsmate list-models</li> </ul>"},{"location":"CLI/schedule-embeddings-reindex/","title":"opsmate schedule-embeddings-reindex","text":"<p><code>opsmate schedule-embeddings-reindex</code> schedules a task to reindex the embeddings. Note that this command only schedules the task.To reindex the embeddings, the <code>opsmate worker</code> process needs to be running.</p> <p>Opsmate uses LanceDB to store the embedding vectors for semantic search and full text search. By default LanceDB does not support incremental indexing. This <code>schedule-embeddings-reindex</code> command schedules a task to reindex the embeddings. Once the reindex task is scheduled, the task will be run periodically by default every 30 seconds.</p>"},{"location":"CLI/schedule-embeddings-reindex/#options","title":"OPTIONS","text":"<pre><code>Usage: opsmate schedule-embeddings-reindex [OPTIONS]\n\n  Schedule the reindex embeddings table task. It will purge all the reindex\n  tasks before scheduling the new one. After schedule the reindex task will be\n  run periodically every 30 seconds.\n\nOptions:\n  -i, --interval-seconds INTEGER  Interval seconds to run the reindex task\n                                  [default: 30]\n  -nw, --no-wait-for-completion   Do not wait for the reindex task to complete\n                                  before scheduling the next one\n  --tools TEXT                    The tools to use for the session. Run\n                                  `opsmate list-tools` to see the available\n                                  tools. By default the tools from the context\n                                  are used. (env: OPSMATE_TOOLS)  [default:\n                                  \"\"]\n  --loglevel TEXT                 Set loglevel (env: OPSMATE_LOGLEVEL)\n                                  [default: INFO]\n  --categorise BOOLEAN            Whether to categorise the embeddings (env:\n                                  OPSMATE_CATEGORISE)  [default: True]\n  --reranker-name TEXT            The name of the reranker model (env:\n                                  OPSMATE_RERANKER_NAME)  [default: \"\"]\n  --embedding-model-name TEXT     The name of the embedding model (env:\n                                  OPSMATE_EMBEDDING_MODEL_NAME)  [default:\n                                  text-embedding-ada-002]\n  --embedding-registry-name TEXT  The name of the embedding registry (env:\n                                  OPSMATE_EMBEDDING_REGISTRY_NAME)  [default:\n                                  openai]\n  --embeddings-db-path TEXT       The path to the lance db. When s3:// is used\n                                  for AWS S3, az:// is used for Azure Blob\n                                  Storage, and gs:// is used for Google Cloud\n                                  Storage (env: OPSMATE_EMBEDDINGS_DB_PATH)\n                                  [default: /root/.opsmate/embeddings]\n  -c, --context TEXT              The context to use for the session. Run\n                                  `opsmate list-contexts` to see the available\n                                  contexts. (env: OPSMATE_CONTEXT)  [default:\n                                  cli]\n  --contexts-dir TEXT             Set contexts_dir (env: OPSMATE_CONTEXTS_DIR)\n                                  [default: /root/.opsmate/contexts]\n  --plugins-dir TEXT              Set plugins_dir (env: OPSMATE_PLUGINS_DIR)\n                                  [default: /root/.opsmate/plugins]\n  -m, --model TEXT                The model to use for the session. Run\n                                  `opsmate list-models` to see the available\n                                  models. (env: OPSMATE_MODEL)  [default:\n                                  gpt-4o]\n  --db-url TEXT                   Set db_url (env: OPSMATE_DB_URL)  [default:\n                                  sqlite:////root/.opsmate/opsmate.db]\n  --auto-migrate BOOLEAN          Automatically migrate the database to the\n                                  latest version  [default: True]\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"CLI/schedule-embeddings-reindex/#usage","title":"USAGE","text":""},{"location":"CLI/schedule-embeddings-reindex/#basic","title":"Basic","text":"<p>Here is the most basic usage:</p> <pre><code>opsmate schedule-embeddings-reindex\n</code></pre> <p>It will wait for the reindex task to complete before scheduling the next one.</p>"},{"location":"CLI/schedule-embeddings-reindex/#interval","title":"Interval","text":"<p>Note that the default interval between reindex tasks is 30 seconds. You can change it by using the <code>--interval-seconds</code> option.</p> <pre><code>opsmate schedule-embeddings-reindex -i 60\n</code></pre> <p>This will schedule a reindex task to run every 60 seconds.</p>"},{"location":"CLI/schedule-embeddings-reindex/#no-wait-for-completion","title":"No wait for completion","text":"<p>You can do not wait for the reindex task to complete before scheduling the next one by using the <code>--no-wait-for-completion</code> option.</p> <pre><code>opsmate schedule-embeddings-reindex -nw\n</code></pre> <p>This is useful when your existing reindex task is stalled but you want to schedule a new one without tidying up the existing one. In most cases you should not use this option.</p>"},{"location":"CLI/schedule-embeddings-reindex/#see-also","title":"SEE ALSO","text":"<ul> <li>opsmate worker</li> <li>opsmate ingest</li> </ul>"},{"location":"CLI/serve/","title":"opsmate serve","text":"<p><code>opsmate serve</code> starts the Opsmate server.</p> <p>The server has two major functionalities:</p> <ol> <li>It offers a web interface for interacting with Opsmate.</li> <li>It includes an experimental REST API server for interacting with Opsmate.</li> </ol>"},{"location":"CLI/serve/#options","title":"OPTIONS","text":"<pre><code>Usage: opsmate serve [OPTIONS]\n\n  Start the Opsmate server.\n\nOptions:\n  -h, --host TEXT                 Host to serve on  [default: 0.0.0.0]\n  -p, --port INTEGER              Port to serve on  [default: 8080]\n  -w, --workers INTEGER           Number of uvicorn workers to serve on\n                                  [default: 2]\n  --dev                           Run in development mode\n  --system-prompt TEXT            Set system_prompt (env:\n                                  OPSMATE_SYSTEM_PROMPT)  [default: \"\"]\n  --token TEXT                    Set token (env: OPSMATE_TOKEN)  [default:\n                                  \"\"]\n  --session-name TEXT             Set session_name (env: OPSMATE_SESSION_NAME)\n                                  [default: session]\n  --tools TEXT                    The tools to use for the session. Run\n                                  `opsmate list-tools` to see the available\n                                  tools. By default the tools from the context\n                                  are used. (env: OPSMATE_TOOLS)  [default:\n                                  \"\"]\n  --loglevel TEXT                 Set loglevel (env: OPSMATE_LOGLEVEL)\n                                  [default: INFO]\n  --categorise BOOLEAN            Whether to categorise the embeddings (env:\n                                  OPSMATE_CATEGORISE)  [default: True]\n  --reranker-name TEXT            The name of the reranker model (env:\n                                  OPSMATE_RERANKER_NAME)  [default: \"\"]\n  --embedding-model-name TEXT     The name of the embedding model (env:\n                                  OPSMATE_EMBEDDING_MODEL_NAME)  [default:\n                                  text-embedding-ada-002]\n  --embedding-registry-name TEXT  The name of the embedding registry (env:\n                                  OPSMATE_EMBEDDING_REGISTRY_NAME)  [default:\n                                  openai]\n  --embeddings-db-path TEXT       The path to the lance db. When s3:// is used\n                                  for AWS S3, az:// is used for Azure Blob\n                                  Storage, and gs:// is used for Google Cloud\n                                  Storage (env: OPSMATE_EMBEDDINGS_DB_PATH)\n                                  [default: /root/.opsmate/embeddings]\n  -c, --context TEXT              The context to use for the session. Run\n                                  `opsmate list-contexts` to see the available\n                                  contexts. (env: OPSMATE_CONTEXT)  [default:\n                                  cli]\n  --contexts-dir TEXT             Set contexts_dir (env: OPSMATE_CONTEXTS_DIR)\n                                  [default: /root/.opsmate/contexts]\n  --plugins-dir TEXT              Set plugins_dir (env: OPSMATE_PLUGINS_DIR)\n                                  [default: /root/.opsmate/plugins]\n  -m, --model TEXT                The model to use for the session. Run\n                                  `opsmate list-models` to see the available\n                                  models. (env: OPSMATE_MODEL)  [default:\n                                  gpt-4o]\n  --db-url TEXT                   Set db_url (env: OPSMATE_DB_URL)  [default:\n                                  sqlite:////root/.opsmate/opsmate.db]\n  --shell-command-runtime TEXT    The runtime to use for the tool call (env:\n                                  SHELL_COMMAND_RUNTIME)  [default: local]\n  --runtime-k8s-shell TEXT        Set shell_cmd (env: RUNTIME_K8S_SHELL)\n                                  [default: /bin/sh]\n  --runtime-k8s-container TEXT    Name of the container of the pod, if not\n                                  specified, the first container will be used\n                                  (env: RUNTIME_K8S_CONTAINER)\n  --runtime-k8s-pod TEXT          Set pod_name (env: RUNTIME_K8S_POD)\n                                  [default: \"\"]\n  --runtime-k8s-namespace TEXT    Set namespace (env: RUNTIME_K8S_NAMESPACE)\n                                  [default: default]\n  --runtime-ssh-connect-retries INTEGER\n                                  Set connect_retries (env:\n                                  RUNTIME_SSH_CONNECT_RETRIES)  [default: 3]\n  --runtime-ssh-timeout INTEGER   Set timeout (env: RUNTIME_SSH_TIMEOUT)\n                                  [default: 10]\n  --runtime-ssh-shell TEXT        Set shell_cmd (env: RUNTIME_SSH_SHELL)\n                                  [default: /bin/bash]\n  --runtime-ssh-key-file TEXT     Set key_file (env: RUNTIME_SSH_KEY_FILE)\n  --runtime-ssh-password TEXT     Set password (env: RUNTIME_SSH_PASSWORD)\n  --runtime-ssh-username TEXT     Set username (env: RUNTIME_SSH_USERNAME)\n                                  [default: \"\"]\n  --runtime-ssh-port INTEGER      Set port (env: RUNTIME_SSH_PORT)  [default:\n                                  22]\n  --runtime-ssh-host TEXT         Set host (env: RUNTIME_SSH_HOST)  [default:\n                                  \"\"]\n  --runtime-docker-service-name TEXT\n                                  Name of the service to run (env:\n                                  RUNTIME_DOCKER_SERVICE_NAME)  [default:\n                                  default]\n  --runtime-docker-compose-file TEXT\n                                  Path to the docker compose file (env:\n                                  RUNTIME_DOCKER_COMPOSE_FILE)  [default:\n                                  docker-compose.yml]\n  --runtime-docker-shell TEXT     Set shell_cmd (env: RUNTIME_DOCKER_SHELL)\n                                  [default: /bin/bash]\n  --runtime-docker-container-name TEXT\n                                  Set container_name (env:\n                                  RUNTIME_DOCKER_CONTAINER_NAME)  [default:\n                                  \"\"]\n  --runtime-local-shell TEXT      Set shell_cmd (env: RUNTIME_LOCAL_SHELL)\n                                  [default: /bin/bash]\n  --auto-migrate BOOLEAN          Automatically migrate the database to the\n                                  latest version  [default: True]\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"CLI/serve/#examples","title":"EXAMPLES","text":""},{"location":"CLI/serve/#start-the-opsmate-server","title":"Start the Opsmate server","text":"<p>The command below starts the Opsmate server on the default host and port.</p> <pre><code>opsmate serve\n</code></pre> <p>You can scale up the number of uvicorn workers to handle more requests.</p> <pre><code>opsmate serve -w 4\n</code></pre> <p>In the example above, the server will start 4 uvicorn workers.</p>"},{"location":"CLI/serve/#run-in-development-mode","title":"Run in development mode","text":"<p>You can start the server in development mode, which is useful for development purposes.</p> <pre><code>opsmate serve --dev\n</code></pre>"},{"location":"CLI/serve/#disable-automatic-database-migration","title":"Disable automatic database migration","text":"<p>By default the <code>serve</code> command automatically migrates the sqlite database to the latest version. You can disable this behavior by passing <code>--auto-migrate=[0|False]</code>.</p> <pre><code>opsmate serve --auto-migrate=0\n</code></pre>"},{"location":"CLI/serve/#environment-variables","title":"Environment variables","text":""},{"location":"CLI/serve/#opsmate_session_name","title":"OPSMATE_SESSION_NAME","text":"<p>The name of the title shown in the web UI, defaults to <code>session</code>.</p>"},{"location":"CLI/serve/#opsmate_token","title":"OPSMATE_TOKEN","text":"<p>This enables token based authentication.</p> <pre><code>OPSMATE_TOKEN=&lt;token&gt; opsmate serve\n</code></pre> <p>Once set you can visit the server via <code>http://&lt;host&gt;:&lt;port&gt;?token=&lt;token&gt;</code>. This is NOT a production-grade authn solution and should only be used for development purposes.</p> <p>For proper authn, authz and TLS termination you should use a production-grade ingress or API Gateway solution.</p>"},{"location":"CLI/serve/#opsmate_tools","title":"OPSMATE_TOOLS","text":"<p>A comma separated list of tools to use, defaults to <code>ShellCommand,KnowledgeRetrieval</code>.</p>"},{"location":"CLI/serve/#opsmate_model","title":"OPSMATE_MODEL","text":"<p>The model used by the AI assistant, defaults to <code>gpt-4o</code>.</p>"},{"location":"CLI/serve/#opsmate_system_prompt","title":"OPSMATE_SYSTEM_PROMPT","text":"<p>The system prompt used by the AI assistant, defaults to the <code>k8s</code> context.</p>"},{"location":"CLI/serve/#see-also","title":"SEE ALSO","text":"<ul> <li>opsmate worker</li> <li>opsmate chat</li> </ul>"},{"location":"CLI/solve/","title":"opsmate solve","text":"<p><code>opsmate solve</code> solves a SRE/DevOps oriented task via reasoning.</p> <p>Unlike most of the state-of-the-art LLMs models (e.g. o1-pro, deepseek R1) that scheming in the background and come back to you 1 minute later, Opsmate reasoning via actively interactive with the environment to gather information and trial and error to find the best solution. We believe short feedback loop is key to solve SRE/DevOps oriented tasks.</p>"},{"location":"CLI/solve/#options","title":"OPTIONS","text":"<pre><code>Usage: opsmate solve [OPTIONS] INSTRUCTION\n\n  Solve a problem with the Opsmate.\n\nOptions:\n  -i, --max-iter INTEGER          Max number of iterations the AI assistant\n                                  can reason about  [default: 10]\n  -nt, --no-tool-output           Do not print tool outputs\n  -a, --answer-only               Print only the answer\n  --tool-calls-per-action INTEGER\n                                  Number of tool calls per action  [default:\n                                  1]\n  --tools TEXT                    The tools to use for the session. Run\n                                  `opsmate list-tools` to see the available\n                                  tools. By default the tools from the context\n                                  are used. (env: OPSMATE_TOOLS)  [default:\n                                  \"\"]\n  --loglevel TEXT                 Set loglevel (env: OPSMATE_LOGLEVEL)\n                                  [default: INFO]\n  --categorise BOOLEAN            Whether to categorise the embeddings (env:\n                                  OPSMATE_CATEGORISE)  [default: True]\n  --reranker-name TEXT            The name of the reranker model (env:\n                                  OPSMATE_RERANKER_NAME)  [default: \"\"]\n  --embedding-model-name TEXT     The name of the embedding model (env:\n                                  OPSMATE_EMBEDDING_MODEL_NAME)  [default:\n                                  text-embedding-ada-002]\n  --embedding-registry-name TEXT  The name of the embedding registry (env:\n                                  OPSMATE_EMBEDDING_REGISTRY_NAME)  [default:\n                                  openai]\n  --embeddings-db-path TEXT       The path to the lance db. When s3:// is used\n                                  for AWS S3, az:// is used for Azure Blob\n                                  Storage, and gs:// is used for Google Cloud\n                                  Storage (env: OPSMATE_EMBEDDINGS_DB_PATH)\n                                  [default: /root/.opsmate/embeddings]\n  -c, --context TEXT              The context to use for the session. Run\n                                  `opsmate list-contexts` to see the available\n                                  contexts. (env: OPSMATE_CONTEXT)  [default:\n                                  cli]\n  --contexts-dir TEXT             Set contexts_dir (env: OPSMATE_CONTEXTS_DIR)\n                                  [default: /root/.opsmate/contexts]\n  --plugins-dir TEXT              Set plugins_dir (env: OPSMATE_PLUGINS_DIR)\n                                  [default: /root/.opsmate/plugins]\n  -m, --model TEXT                The model to use for the session. Run\n                                  `opsmate list-models` to see the available\n                                  models. (env: OPSMATE_MODEL)  [default:\n                                  gpt-4o]\n  --db-url TEXT                   Set db_url (env: OPSMATE_DB_URL)  [default:\n                                  sqlite:////root/.opsmate/opsmate.db]\n  --shell-command-runtime TEXT    The runtime to use for the tool call (env:\n                                  SHELL_COMMAND_RUNTIME)  [default: local]\n  --runtime-k8s-shell TEXT        Set shell_cmd (env: RUNTIME_K8S_SHELL)\n                                  [default: /bin/sh]\n  --runtime-k8s-container TEXT    Name of the container of the pod, if not\n                                  specified, the first container will be used\n                                  (env: RUNTIME_K8S_CONTAINER)\n  --runtime-k8s-pod TEXT          Set pod_name (env: RUNTIME_K8S_POD)\n                                  [default: \"\"]\n  --runtime-k8s-namespace TEXT    Set namespace (env: RUNTIME_K8S_NAMESPACE)\n                                  [default: default]\n  --runtime-ssh-connect-retries INTEGER\n                                  Set connect_retries (env:\n                                  RUNTIME_SSH_CONNECT_RETRIES)  [default: 3]\n  --runtime-ssh-timeout INTEGER   Set timeout (env: RUNTIME_SSH_TIMEOUT)\n                                  [default: 10]\n  --runtime-ssh-shell TEXT        Set shell_cmd (env: RUNTIME_SSH_SHELL)\n                                  [default: /bin/bash]\n  --runtime-ssh-key-file TEXT     Set key_file (env: RUNTIME_SSH_KEY_FILE)\n  --runtime-ssh-password TEXT     Set password (env: RUNTIME_SSH_PASSWORD)\n  --runtime-ssh-username TEXT     Set username (env: RUNTIME_SSH_USERNAME)\n                                  [default: \"\"]\n  --runtime-ssh-port INTEGER      Set port (env: RUNTIME_SSH_PORT)  [default:\n                                  22]\n  --runtime-ssh-host TEXT         Set host (env: RUNTIME_SSH_HOST)  [default:\n                                  \"\"]\n  --runtime-docker-service-name TEXT\n                                  Name of the service to run (env:\n                                  RUNTIME_DOCKER_SERVICE_NAME)  [default:\n                                  default]\n  --runtime-docker-compose-file TEXT\n                                  Path to the docker compose file (env:\n                                  RUNTIME_DOCKER_COMPOSE_FILE)  [default:\n                                  docker-compose.yml]\n  --runtime-docker-shell TEXT     Set shell_cmd (env: RUNTIME_DOCKER_SHELL)\n                                  [default: /bin/bash]\n  --runtime-docker-container-name TEXT\n                                  Set container_name (env:\n                                  RUNTIME_DOCKER_CONTAINER_NAME)  [default:\n                                  \"\"]\n  --runtime-local-shell TEXT      Set shell_cmd (env: RUNTIME_LOCAL_SHELL)\n                                  [default: /bin/bash]\n  -r, --review                    Review and edit commands before execution\n  -s, --system-prompt TEXT        System prompt to use\n  -l, --max-output-length INTEGER\n                                  Max length of the output, if the output is\n                                  truncated, the tmp file will be printed in\n                                  the output  [default: 10000]\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"CLI/solve/#usage","title":"USAGE","text":""},{"location":"CLI/solve/#the-most-basic-usage","title":"The most basic usage","text":"<p>In the example below, the Opsmate will reason about the problem and come up with a solution, going through the \"thought-action-observation\" loop.</p> <pre><code>opsmate solve \"how many cores on the server?\"\n</code></pre>"},{"location":"CLI/solve/#using-a-different-model","title":"Using a different model","text":"<p>Like the <code>run</code> command, you can use the <code>--model</code> option to use a different model. <pre><code>opsmate solve \"how many cores on the server?\" -m grok-2-1212\n</code></pre></p>"},{"location":"CLI/solve/#increase-the-number-of-iterations","title":"Increase the number of iterations","text":"<p>You can increase the number of iterations the Opsmate can reason about by using the <code>--max-iter</code> option for anything that requires long reasoning. There are a few things to bare in mind though:</p> <ul> <li>More iterations means more LLM tokens used. As the context window gets progressively larger over iterations, the cost will increase.</li> <li>In real-world use cases more iterations doesn't necessarily translate to better results. The common pattern we have observed is that with the current frontier LLMs, 10-15 iterations is the sweet spot. The longer the task, the more \"confused\" LLM becomes.</li> </ul> <pre><code>opsmate solve \"how many cores on the server?\" --max-iter 20\n</code></pre>"},{"location":"CLI/solve/#use-various-tools","title":"Use various tools","text":"<p>The Opsmate can use various tools to solve the problem. You can see the list of available tools by running the <code>list-tools</code> command. To use these tools, you can pass the <code>--tools</code> option.</p> <p>Here is an example of gathering top 10 news from hacker news and write it to a file:</p> <pre><code>opsmate solve -na \\\n  \"find me top 10 news on the hacker news with bullet points and write to hn-top-10.md\" \\\n  --tools HtmlToText,FileWrite\n...\n\ncat hn-top-10.md\n- [Do You Not Like Money?](https://news.ycombinator.com/item?id=43183568) by rbanffy\n- [Chile blackout affects 14 regions](https://news.ycombinator.com/item?id=43182892) by impish9208\n- [The miserable state of modems and mobile network operators](https://news.ycombinator.com/item?id=43182854) by hasheddan\n- [Automattic hit with class action over WP Engine dispute](https://news.ycombinator.com/item?id=43182576) by rpgbr\n- [A Radical Proposal for How Mind Emerges from Matter](https://news.ycombinator.com/item?id=43181520) by Hooke\n- [Iterlog Coding](https://news.ycombinator.com/item?id=43181610) by snarkconjecture\n- [VSC Material Theme](https://news.ycombinator.com/item?id=43178831) by Inityx\n- [Fixing Illinois FOIA](https://news.ycombinator.com/item?id=43175628) by mrkurt\n- [The XB 70](https://news.ycombinator.com/item?id=43175315) by rbanffy\n- [Document Ranking for Complex Problems](https://news.ycombinator.com/item?id=43174910) by noperator\n</code></pre>"},{"location":"CLI/solve/#review-and-edit-commands","title":"Review and edit commands","text":"<p>Just like the <code>run</code> command, you can use the <code>--review</code> option to review and edit the commands before execution.</p> <pre><code>opsmate solve \"how many cores on the server?\" -r\n</code></pre>"},{"location":"CLI/solve/#use-a-different-system-prompt","title":"Use a different system prompt","text":"<p>You can use the <code>--system-prompt</code> or <code>-s</code> flag to use a different system prompt.</p> <pre><code>opsmate solve \"how many cores on the server?\" -s \"You are a kubernetes SME\"\n</code></pre>"},{"location":"CLI/solve/#see-also","title":"SEE ALSO","text":"<ul> <li>opsmate run</li> <li>opsmate list-tools</li> <li>opsmate list-models</li> </ul>"},{"location":"CLI/uninstall/","title":"opsmate uninstall","text":"<p><code>opsmate uninstall</code> uninstalls Opsmate plugins.</p>"},{"location":"CLI/uninstall/#options","title":"OPTIONS","text":"<pre><code>Usage: opsmate uninstall [OPTIONS] PACKAGES...\n\n  Uninstall the given packages.\n\nOptions:\n  -y, --yes  Do not prompt for confirmation\n  --help     Show this message and exit.\n</code></pre>"},{"location":"CLI/uninstall/#see-also","title":"SEE ALSO","text":"<ul> <li>Install</li> </ul>"},{"location":"CLI/worker/","title":"opsmate worker","text":"<p><code>opsmate worker</code> starts a background worker that handles background tasks, such as chunking knowledge base documents and storing them in the vector database.</p> <p>This is required for any knowledge ingestion, as the process can be long running and we don't want to run it in the foreground.</p>"},{"location":"CLI/worker/#options","title":"OPTIONS","text":"<pre><code>Usage: opsmate worker [OPTIONS]\n\n  Start the Opsmate worker.\n\nOptions:\n  -w, --workers INTEGER           Number of concurrent background workers\n                                  [default: 10]\n  -q, --queue TEXT                Queue to use for the worker  [default:\n                                  default]\n  --tools TEXT                    The tools to use for the session. Run\n                                  `opsmate list-tools` to see the available\n                                  tools. By default the tools from the context\n                                  are used. (env: OPSMATE_TOOLS)  [default:\n                                  \"\"]\n  --loglevel TEXT                 Set loglevel (env: OPSMATE_LOGLEVEL)\n                                  [default: INFO]\n  --categorise BOOLEAN            Whether to categorise the embeddings (env:\n                                  OPSMATE_CATEGORISE)  [default: True]\n  --reranker-name TEXT            The name of the reranker model (env:\n                                  OPSMATE_RERANKER_NAME)  [default: \"\"]\n  --embedding-model-name TEXT     The name of the embedding model (env:\n                                  OPSMATE_EMBEDDING_MODEL_NAME)  [default:\n                                  text-embedding-ada-002]\n  --embedding-registry-name TEXT  The name of the embedding registry (env:\n                                  OPSMATE_EMBEDDING_REGISTRY_NAME)  [default:\n                                  openai]\n  --embeddings-db-path TEXT       The path to the lance db. When s3:// is used\n                                  for AWS S3, az:// is used for Azure Blob\n                                  Storage, and gs:// is used for Google Cloud\n                                  Storage (env: OPSMATE_EMBEDDINGS_DB_PATH)\n                                  [default: /root/.opsmate/embeddings]\n  -c, --context TEXT              The context to use for the session. Run\n                                  `opsmate list-contexts` to see the available\n                                  contexts. (env: OPSMATE_CONTEXT)  [default:\n                                  cli]\n  --contexts-dir TEXT             Set contexts_dir (env: OPSMATE_CONTEXTS_DIR)\n                                  [default: /root/.opsmate/contexts]\n  --plugins-dir TEXT              Set plugins_dir (env: OPSMATE_PLUGINS_DIR)\n                                  [default: /root/.opsmate/plugins]\n  -m, --model TEXT                The model to use for the session. Run\n                                  `opsmate list-models` to see the available\n                                  models. (env: OPSMATE_MODEL)  [default:\n                                  gpt-4o]\n  --db-url TEXT                   Set db_url (env: OPSMATE_DB_URL)  [default:\n                                  sqlite:////root/.opsmate/opsmate.db]\n  --auto-migrate BOOLEAN          Automatically migrate the database to the\n                                  latest version  [default: True]\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"CLI/worker/#examples","title":"EXAMPLES","text":""},{"location":"CLI/worker/#start-the-worker","title":"Start the worker","text":"<pre><code>opsmate worker\n</code></pre> <p>The command above starts the worker with the default number of workers, which is 10.</p>"},{"location":"CLI/worker/#use-custom-number-of-workers","title":"Use custom number of workers","text":"<pre><code>opsmate worker -w 5\n</code></pre> <p>The concurrent workers are coroutines which are suitable for IO and network bound tasks. For any CPU bound tasks you can scale up the number of <code>opsmate worker</code> processes via using supervisor program such as <code>systemd</code> or honcho.</p>"},{"location":"CLI/worker/#see-also","title":"SEE ALSO","text":"<ul> <li>opsmate serve</li> <li>opsmate ingest</li> </ul>"},{"location":"configurations/OTel/","title":"OTel Integration","text":"<p>Opsmate provides built-in integration with OpenTelemetry for distributed tracing. This allows you to monitor and troubleshoot your application's performance and behavior.</p>"},{"location":"configurations/OTel/#setup","title":"Setup","text":"<p>To enable OTel tracing, set the following environment variables:</p> <pre><code># Required: OTLP endpoint\nexport OTEL_EXPORTER_OTLP_ENDPOINT=http://your-collector:4317\n\n# Optional: Protocol - defaults to HTTP if not specified\nexport OTEL_EXPORTER_OTLP_PROTOCOL=grpc  # or \"http\"\n\n# Optional: Service name - defaults to \"opsmate\"\nexport SERVICE_NAME=&lt;your-service-name&gt;\n\n# Optional: OTel header - typically for the purpose of breaer or basic auth\nexport OTEL_EXPORTER_OTEL_HEADER=\n</code></pre> <p>Here is the official documentation for the OTel configuration:</p> <ul> <li>OTLP Exporter</li> </ul> <p>After setting up, the following commands are OTel traced:</p> <ul> <li>opsmate run</li> <li>opsmate solve</li> <li>opsmate chat</li> <li>opsmate serve</li> <li>opsmate worker</li> </ul>"},{"location":"configurations/OTel/#automatic-instrumentation","title":"Automatic Instrumentation","text":"<p>Out of the box, the following integrations are automatically instrumented:</p> <ul> <li>OpenAI API and OpenAI compatible providers API calls</li> <li>Anthropic API calls</li> <li>SQLAlchemy database calls (when the database operations are performed)</li> <li>Starlette HTTP requests (when running in server mode)</li> </ul>"},{"location":"configurations/OTel/#disable-tracing","title":"Disable Tracing","text":"<p>To disable tracing, set the following environment variable:</p> <pre><code>export OPSMATE_DISABLE_OTEL=true\n\n# or\n\nunset OTEL_EXPORTER_OTLP_ENDPOINT\n</code></pre>"},{"location":"configurations/add-new-llm-providers/","title":"Add New LLM Providers","text":"<p>Opsmate out-of-box supports OpenAI, Anthropic and xAI as the LLM providers. We plan to add more providers in the future. In the meantime you can also add your own LLM provider that are currently not supported by Opsmate.</p> <p>In this article we will demonstrate how to add a new LLM provider by using Groq as an example.</p> <p>The full code for this example can be found here.</p>"},{"location":"configurations/add-new-llm-providers/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12+</li> <li>Groq API key</li> <li>You already have Opsmate installed - If not, you can install it by following the installation guide</li> <li>Python virtual environment - It's not mandatory but highly recommended to use a virtual environment to install the dependencies for testing purposes.</li> </ul>"},{"location":"configurations/add-new-llm-providers/#steps","title":"Steps","text":""},{"location":"configurations/add-new-llm-providers/#step-1-create-a-new-directory-for-the-provider","title":"Step 1: Create a new directory for the provider","text":"<pre><code>mkdir -p groq\ncd groq\ntouch provider_groq.py # This is the file that will contain the provider code\ntouch pyproject.toml # This is the file that will contain plugin metadata\n</code></pre>"},{"location":"configurations/add-new-llm-providers/#step-2-implement-the-provider-code","title":"Step 2: Implement the provider code","text":"<pre><code>from opsmate.dino.provider import Provider, register_provider\nfrom opsmate.dino.types import Message\nfrom typing import Any, Awaitable, List\nfrom instructor.client import T\nfrom instructor import AsyncInstructor\nfrom tenacity import AsyncRetrying\nfrom functools import cache\nfrom groq import AsyncGroq\nimport instructor\nimport os\n\n\n@register_provider(\"groq\")\nclass GroqProvider(Provider):\n    DEFAULT_BASE_URL = \"https://api.groq.com\"\n\n    # Here is the full list of models that support tool use https://console.groq.com/docs/tool-use\n    models = [\n        \"qwen-2.5-32b\",\n        \"deepseek-r1-distill-qwen-32b\",\n        \"deepseek-r1-distill-llama-70b\",\n        \"llama-3.3-70b-versatile\",\n        # commented out as it cannot reliably use tools\n        # \"llama-3.1-8b-instant\",\n        # \"mixtral-8x7b-32768\",\n        # \"gemma2-9b-it\",\n    ]\n\n    @classmethod\n    async def chat_completion(\n        cls,\n        response_model: type[T],\n        messages: List[Message],\n        max_retries: int | AsyncRetrying = 3,\n        validation_context: dict[str, Any] | None = None,\n        context: dict[str, Any] | None = None,\n        strict: bool = True,\n        client: AsyncInstructor | None = None,\n        **kwargs: Any,\n    ) -&gt; Awaitable[T]:\n        client = client or cls.default_client()\n        kwargs.pop(\"client\", None)\n\n        messages = [{\"role\": m.role, \"content\": m.content} for m in messages]\n\n        filtered_kwargs = cls._filter_kwargs(kwargs)\n        return await client.chat.completions.create(\n            response_model=response_model,\n            messages=messages,\n            max_retries=max_retries,\n            validation_context=validation_context,\n            context=context,\n            strict=strict,\n            **filtered_kwargs,\n        )\n\n    @classmethod\n    @cache\n    def _default_client(cls) -&gt; AsyncInstructor:\n        return instructor.from_groq(\n            AsyncGroq(\n                base_url=os.getenv(\"GROQ_BASE_URL\", cls.DEFAULT_BASE_URL),\n                api_key=os.getenv(\"GROQ_API_KEY\"),\n            ),\n            mode=instructor.Mode.JSON,\n        )\n</code></pre> <p>The minimum requirements for the new provider code includes:</p> <ul> <li>The <code>@register_provider</code> function that decorate the class that implements the <code>dino.provider.Provider</code>. Note that not only the provider class must implement the <code>dino.provider.Provider</code> interface and being decorated with <code>@register_provider</code> but also it needs to be a subclass of <code>Provider</code>.</li> <li>The <code>models</code> class variable that lists the models that the provider supports.</li> <li>The <code>chat_completion</code> method that implements the <code>Provider</code> interface.</li> <li>The <code>default_client</code> method that returns a <code>AsyncInstructor</code> client.</li> </ul> <p>Note that the <code>chat_completion</code> method must be an async method, and the <code>default_client</code> must returns an <code>AsyncInstructor</code> client.</p>"},{"location":"configurations/add-new-llm-providers/#step-3-implement-plugin-in-pyprojecttoml","title":"Step 3: Implement plugin in <code>pyproject.toml</code>","text":"<pre><code>[project]\nname = \"opsmate-provider-groq\"\nversion = \"0.1.0\"\ndescription = \"Groq provider for opsmate\"\ndependencies = [\n    \"opsmate\",\n    \"groq\",\n]\n\n[project.entry-points.\"opsmate.dino.providers\"]\ngroq = \"provider_groq:GroqProvider\"\n</code></pre> <p>Note that in the <code>pyproject.toml</code> file:</p> <ul> <li>We use <code>opsmate-provider-groq</code> as the name of the provider. It is not mandatory but <code>opsmate-provider-&lt;provider-name&gt;</code> is the convention we follow.</li> <li>We also use <code>groq</code> and <code>opsmate</code> as the dependencies.</li> <li>The <code>[project.entry-points.\"opsmate.dino.providers\"]</code> section is used to register the provider. The name of the entry point group MUST be <code>opsmate.dino.providers</code>.</li> <li>We use <code>groq</code> as the name of the entry point, which points to the <code>provider_groq:GroqProvider</code> class.</li> </ul> <p>If you are not familar with Python entry point based plugin system, you can refer to this document.</p>"},{"location":"configurations/add-new-llm-providers/#step-4-install-the-dependencies","title":"Step 4: Install the dependencies","text":"<pre><code>opsmate install -e .\n</code></pre>"},{"location":"configurations/add-new-llm-providers/#step-5-test-the-provider","title":"Step 5: Test the provider","text":"<p>After installation you can list all the models via</p> <pre><code>$ opsmate list-models\n                   Models\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Provider  \u2503 Model                         \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 openai    \u2502 gpt-4o                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 openai    \u2502 gpt-4o-mini                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 openai    \u2502 o1-preview                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 anthropic \u2502 claude-3-5-sonnet-20241022    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 anthropic \u2502 claude-3-7-sonnet-20250219    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 xai       \u2502 grok-2-1212                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 groq      \u2502 qwen-2.5-32b                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 groq      \u2502 deepseek-r1-distill-qwen-32b  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 groq      \u2502 deepseek-r1-distill-llama-70b \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 groq      \u2502 llama-3.3-70b-versatile       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You will notice that the models from Groq are automatically added to the list of models.</p> <p>You can use the <code>-m</code> flag to specify the model to use. For example:</p> <pre><code>export OPSMATE_LOGLEVEL=ERROR\n$ opsmate run -n --tools HtmlToText -m llama-3.3-70b-versatile \"find me top 10 news on the hacker news, titl\ne only in bullet points\"\nThe top 10 news on Hacker News are:\n* The most unhinged video wall, made out of Chromebooks\n* Show HN: Berlin Swapfest \u2013 Electronics flea market\n* GLP-1 drugs \u2013 the biggest economic disruptor since the internet? (2024)\n* Efabless \u2013 Shutdown Notice\n* Video encoding requires using your eyes\n* Making o1, o3, and Sonnet 3.7 hallucinate for everyone\n* How to gain code execution on hundreds of millions of people and popular apps\n* Show HN: I made a website where you can create your own \"Life in Weeks\" timeline\n* Drone captures narwhals using their tusks to explore, forage and play\n* Maestro \u2013 Next generation mobile UI automation\n</code></pre>"},{"location":"configurations/add-new-llm-providers/#cleanup","title":"Cleanup","text":"<pre><code>opsmate uninstall -y opsmate-provider-groq\n</code></pre>"},{"location":"configurations/advanced-knowledge-retrieval/","title":"Advanced Knowledge Retrieval","text":"<p>Opsmate out of box uses `openai/text-embedding-ada-002\" for text embeddings, and no rerankers are being used during the retrieval.</p> <p>That being said alternative embeddings and rerankers are available. This document outlines how to setup and use them.</p>"},{"location":"configurations/advanced-knowledge-retrieval/#embeddings","title":"Embeddings","text":"<p>Opsmate supports two types of embeddings:</p> <ol> <li>OpenAI embeddings</li> <li>Sentence Transformers embeddings</li> </ol>"},{"location":"configurations/advanced-knowledge-retrieval/#openai-embeddings","title":"OpenAI embeddings","text":"<p>To explicitlyuse OpenAI embeddings, you need to set the following environment variables:</p> <ul> <li><code>OPSMATE_EMBEDDING_REGISTRY_NAME=openai</code></li> <li><code>OPSMATE_EMBEDDING_MODEL_NAME=text-embedding-ada-002</code></li> </ul>"},{"location":"configurations/advanced-knowledge-retrieval/#sentence-transformers-embeddings","title":"Sentence Transformers embeddings","text":"<p>By default the sentence transformers is not installed. To install it, run:</p> pippipx <pre><code>pip install -U opsmate[sentence-transformers]\n</code></pre> <pre><code>pipx install opsmate[sentence-transformers] --force\n</code></pre> <p>Once installed, Opmsmate will automatically use the Sentence Transformers for embeddings.</p> <p>You can explicitly specify the Sentence Transformers embeddings by setting the following environment variables:</p> <ul> <li><code>OPSMATE_EMBEDDING_REGISTRY_NAME=sentence-transformers</code></li> <li><code>OPSMATE_EMBEDDING_MODEL_NAME=BAAI/bge-small-en-v1.5</code></li> </ul> <p> At the moment we do not officially support embedding models switch once the knowledge base is created. </p> <p>To switch between embedding models, you need to delete the existing knowledge base and re-ingest.</p>"},{"location":"configurations/advanced-knowledge-retrieval/#rerankers","title":"Rerankers","text":"<p>Opsmate supports the following rerankers:</p> <ol> <li>RRF reranker</li> <li>AnswerDotAI reranker</li> <li>Cohere reranker</li> <li>OpenAI reranker</li> </ol>"},{"location":"configurations/advanced-knowledge-retrieval/#rrf-reranker","title":"RRF reranker","text":"<p>To use the RRF reranker, you need to set the following environment variables:</p> <ul> <li><code>OPSMATE_RERANKER_NAME=rrf</code></li> </ul>"},{"location":"configurations/advanced-knowledge-retrieval/#answerdotai-reranker","title":"AnswerDotAI reranker","text":"<p>Out of box, the AnswerDotAI reranker is not installed. To install it, run:</p> pippipx <pre><code>pip install -U opsmate[reranker-answerdotai]\n</code></pre> <pre><code>pipx install opsmate[reranker-answerdotai] --force\n</code></pre> <p>To use the AnswerDotAI reranker, you need to set the following environment variables:</p> <ul> <li><code>OPSMATE_RERANKER_NAME=answerdotai</code></li> </ul>"},{"location":"configurations/advanced-knowledge-retrieval/#cohere-reranker","title":"Cohere reranker","text":"<p>Out of box, the Cohere reranker is not installed. To install it, run:</p> pippipx <pre><code>pip install -U opsmate[reranker-cohere]\n</code></pre> <pre><code>pipx install opsmate[reranker-cohere] --force\n</code></pre> <p>To use the Cohere reranker, you need to set the following environment variables:</p> <ul> <li><code>OPSMATE_RERANKER_NAME=cohere</code></li> <li><code>COHERE_API_KEY=&lt;your-cohere-api-key&gt;</code></li> </ul>"},{"location":"configurations/advanced-knowledge-retrieval/#openai-reranker","title":"OpenAI reranker","text":"<p>To use the OpenAI reranker, you need to set the following environment variables:</p> <ul> <li><code>OPSMATE_RERANKER_NAME=openai</code></li> <li><code>OPENAI_API_KEY=&lt;your-openai-api-key&gt;</code></li> </ul>"},{"location":"configurations/integrate-with-new-runtime/","title":"Add New Runtimes","text":"<p>By default Opsmate executes commands in the same environment as the <code>opsmate</code> process, however in some cases you might want to execute the code in a remote runtime. It can be the case of:</p> <ul> <li>You want to remote-ssh into a machine and perform troubleshooting and service maintenance.</li> <li>The target environment is rather legacy that doesn't support Python 3.10+, which is the minimum version required by Opsmate.</li> <li>You are not allowed to install software on the target remote runtime, thus there is no way to install Opsmate.</li> <li>The target runtime doesn't have internet access, thus it's not possible to access the large language models.</li> </ul> <p>Out of box Opsmate supports the following runtimes:</p> <ul> <li>Local</li> <li>SSH</li> <li>Docker and Docker Compose</li> </ul> <p>That being said nothing prevents you from integrating with a new runtime. In this article we will use Google Compute Engine as an example and create <code>opsmate-gce-runtime</code> plugin.</p> <p>The full code for this example can be found here.</p>"},{"location":"configurations/integrate-with-new-runtime/#prerequisites","title":"Prerequisites","text":"<ul> <li>You already have Opsmate installed - If not, you can install it by following the installation guide</li> <li>You have <code>gcloud</code> CLI installed</li> <li>You have set up the <code>gcloud</code> CLI and are authenticated to your Google Cloud account</li> </ul>"},{"location":"configurations/integrate-with-new-runtime/#steps","title":"Steps","text":""},{"location":"configurations/integrate-with-new-runtime/#step-0-create-a-gce-instance","title":"Step 0: Create a GCE instance","text":"<pre><code>export NETWORK=&lt;your-network&gt;\nexport SUBNET=&lt;your-subnet&gt;\nexport ZONE=europe-west1-c # Replace with your zone\n\n# Create a firewall rule to allow SSH access\ngcloud compute firewall-rules create allow-ssh \\\n  --network=$NETWORK \\\n  --direction=INGRESS \\\n  --priority=1000 \\\n  --source-ranges=0.0.0.0/0 \\\n  --action=ALLOW \\\n  --rules=tcp:22 \\\n  --target-tags=allow-ssh\n\ngcloud compute instances create my-vm \\\n  --tags=allow-ssh \\\n  --zone=$ZONE \\\n  --network=$NETWORK \\\n  --subnet=$SUBNET \\\n  --no-address \\\n  --machine-type=n1-standard-1 \\\n  --image-project=ubuntu-os-cloud \\\n  --image-family=ubuntu-2404-lts-amd64\n</code></pre>"},{"location":"configurations/integrate-with-new-runtime/#step-1-create-a-new-directory-for-the-runtime","title":"Step 1: Create a new directory for the runtime","text":"<pre><code>mkdir -p gce\ncd gce\ntouch gce.py # This is the file that will contain the runtime code\ntouch pyproject.toml # This is the file that will contain runtime plugin metadata\n</code></pre>"},{"location":"configurations/integrate-with-new-runtime/#step-2-implement-the-runtime-code","title":"Step 2: Implement the runtime code","text":"<p>The runtime code can be found here. In the code below we highlight the parts that are essential for the runtime integration, namely:</p> <ul> <li>The <code>GCERuntimeConfig</code> class that defines the configuration for the runtime.</li> <li>The <code>GCERuntime</code> class that implements the runtime.</li> </ul> <p>The mandatory methods to be implemented for any runtimes are:</p> <ul> <li><code>connect</code> - Connect to the runtime.</li> <li><code>disconnect</code> - Disconnect from the runtime.</li> <li><code>run</code> in the case below it inherits from <code>LocalRuntime</code>.</li> <li><code>os_info</code> - Get OS information from the remote runtime.</li> <li><code>whoami</code> - Get current user information from the remote runtime.</li> <li><code>has_systemd</code> - Check if the remote runtime uses systemd.</li> <li><code>runtime_info</code> - Return information about the runtime.</li> </ul> <p>For the <code>GCERuntime</code> it inherits from <code>LocalRuntime</code> and thus it can use the <code>co</code> function to execute commands.</p> <pre><code>import os\nimport asyncio\nfrom opsmate.runtime.runtime import (\n    register_runtime,\n    RuntimeConfig,\n    RuntimeError,\n    co,\n)\nfrom opsmate.runtime.local import LocalRuntime\nfrom pydantic import Field, ConfigDict\nfrom typing import List\nimport structlog\n\n\nlogger = structlog.get_logger(__name__)\n\n\nclass GCERuntimeConfig(RuntimeConfig):\n    instance_name: str = Field(alias=\"RUNTIME_GCE_INSTANCE\", default=\"\")\n    # ...\n\n\n@register_runtime(\"gce\", GCERuntimeConfig)\nclass GCERuntime(LocalRuntime):\n    \"\"\"GCE runtime allows model to execute tool calls on a GCE instance using gcloud compute ssh.\"\"\"\n\n    def __init__(self, config: GCERuntimeConfig):\n        ...\n\n    async def connect(self):\n        \"\"\"Connect to the GCE instance with retry logic.\"\"\"\n        ...\n\n    async def disconnect(self):\n        \"\"\"Disconnect from the GCE instance and clean up resources.\"\"\"\n        ...\n\n    async def os_info(self):\n        \"\"\"Get OS information from the remote GCE instance.\"\"\"\n        ...\n\n    async def whoami(self):\n        \"\"\"Get current user information from the remote GCE instance.\"\"\"\n        ...\n\n    async def has_systemd(self):\n        \"\"\"Check if the remote GCE instance uses systemd.\"\"\"\n        ...\n\n    async def runtime_info(self):\n        \"\"\"Return information about the GCE runtime.\"\"\"\n        ...\n</code></pre>"},{"location":"configurations/integrate-with-new-runtime/#step-3-declare-the-runtime-metadata-in-pyprojecttoml","title":"Step 3: Declare the runtime metadata in <code>pyproject.toml</code>","text":"<p>Opmsate has a <code>pip</code> based plugin system that allows you to load the runtime plugins as a python package. To do so you need to declare the runtime metadata in the <code>pyproject.toml</code>, as shown below:</p> <pre><code>[project]\nname = \"opsmate-runtime-gce\"\nversion = \"0.1.0\"\ndescription = \"GCE runtime for opsmate\"\ndependencies = [\n    \"opsmate\",\n]\n\n[project.entry-points.\"opsmate.runtime.runtimes\"]\ngce = \"gce:GCERuntime\"\n</code></pre> <p>Note that in the <code>pyproject.toml</code> file:</p> <ul> <li>We use <code>opsmate-runtime-gce</code> as the name of the runtime. It is not mandatory but <code>opsmate-runtime-&lt;runtime-name&gt;</code> is the convention we follow.</li> <li>We declare <code>opsmate</code> as the dependency.</li> <li>The <code>[project.entry-points.\"opsmate.runtime.runtimes\"]</code> section is used to register the runtime. The name of the entry point group MUST be <code>opsmate.runtime.runtimes</code>.</li> <li>We use <code>gce</code> as the name of the entry point, which points to the <code>gce:GCERuntime</code> class.</li> </ul>"},{"location":"configurations/integrate-with-new-runtime/#step-4-install-the-runtime-plugin","title":"Step 4: Install the runtime plugin","text":"<pre><code>opsmate install -e .\n</code></pre>"},{"location":"configurations/integrate-with-new-runtime/#step-5-validate-the-runtime-has-been-installed","title":"Step 5: Validate the runtime has been installed","text":"<pre><code>$ opsmate list-runtimes\n                                                   Runtimes\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name   \u2503 Description                                                                                        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 local  \u2502 Local runtime allows model to execute tool calls within the same namespace as the opsmate process. \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 docker \u2502 Docker runtime allows model to execute tool calls within a docker container.                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ssh    \u2502 SSH runtime allows model to execute tool calls on a remote server via SSH.                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 gce    \u2502 GCE runtime allows model to execute tool calls on a GCE instance using gcloud compute ssh.         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We can also run <code>opsmate chat --help | grep -i gce</code> to validate that the runtime is available:</p> <pre><code>opsmate chat --help | grep -i gce\n  --runtime-gce-extra-flags TEXT  Set extra_flags (env:\n                                  RUNTIME_GCE_EXTRA_FLAGS)  [default: \"\"]\n  --runtime-gce-gcloud-binary TEXT\n                                  RUNTIME_GCE_GCLOUD_BINARY)  [default:\n  --runtime-gce-connect-retries INTEGER\n                                  RUNTIME_GCE_CONNECT_RETRIES)  [default: 3]\n  --runtime-gce-timeout INTEGER   Set timeout (env: RUNTIME_GCE_TIMEOUT)\n  --runtime-gce-shell TEXT        Set shell_cmd (env: RUNTIME_GCE_SHELL)\n  --runtime-gce-iap-options TEXT  Set iap_tunnel_options (env:\n                                  RUNTIME_GCE_IAP_OPTIONS)  [default: \"\"]\n  --runtime-gce-use-iap BOOLEAN   Set use_iap (env: RUNTIME_GCE_USE_IAP)\n  --runtime-gce-username TEXT     Set username (env: RUNTIME_GCE_USERNAME)\n  --runtime-gce-project TEXT      Set project (env: RUNTIME_GCE_PROJECT)\n  --runtime-gce-zone TEXT         Set zone (env: RUNTIME_GCE_ZONE)  [default:\n  --runtime-gce-instance TEXT     Set instance_name (env:\n                                  RUNTIME_GCE_INSTANCE)  [default: \"\"]\n</code></pre>"},{"location":"configurations/integrate-with-new-runtime/#step-6-test-the-runtime","title":"Step 6: Test the runtime","text":"<p>Here is the minimal command to interact with the GCE runtime: <pre><code>opsmate chat --shell-command-runtime gce \\\n  --runtime-gce-instance &lt;instance-name&gt; \\\n  --runtime-gce-zone &lt;zone&gt; \\\n</code></pre></p> <p>Here is an e2e interaction with the GCE runtime:</p> <p>In the example I am interacting with a GCE instance called <code>my-vm</code> in the <code>europe-west1-c</code> zone running inside a VPC without any public IP address. Opsmate uses the <code>gcloud compute ssh</code> command to connect to the instance with the help from IAP tunneling.</p>"},{"location":"configurations/integrate-with-new-runtime/#step-7-cleanup","title":"Step 7: Cleanup","text":"<pre><code>opsmate uninstall -y opsmate-runtime-gce\n\n# Delete the GCE instance\ngcloud compute instances delete my-vm --zone=$ZONE\n\n# Delete the firewall rule\ngcloud compute firewall-rules delete allow-ssh\n</code></pre>"},{"location":"configurations/integrate-with-new-runtime/#see-also","title":"See also","text":"<ul> <li>Cookbook - Manage VMs</li> </ul>"},{"location":"configurations/llm-configurations/","title":"LLM Configurations","text":""},{"location":"configurations/llm-configurations/#default-llm-model","title":"Default LLM Model","text":"<p>The default LLM model can be specified via <code>--model</code> or <code>-m</code> flag through the command line on the program startup.</p> <p>It can also be specified as <code>OPSMATE_MODEL</code> environment variable, e.g.</p> <pre><code>export OPSMATE_MODEL=\"gpt-4.1\"\n</code></pre> <p>Alternatively, you can also save the model configuration in the <code>~/.opsmate/config.yaml</code> file.</p> <pre><code>---\n# ...\nOPSMATE_MODEL: claude-3-7-sonnet-20250219\n# ...\n</code></pre>"},{"location":"configurations/llm-configurations/#llm-configuration","title":"LLM Configuration","text":"<p>There are more nuanced configurations for the LLM, such as temperatures, top_p, thinking budget, etc.</p> <p>Here is the out of box configurations:</p> <pre><code>OPSMATE_MODELS_CONFIG:\n  claude-3-7-sonnet-20250219:\n    thinking:\n      budget_tokens: 1024\n      type: enabled\n  grok-3-mini-beta:\n    reasoning_effort: medium\n    tool_call_model: grok-3-beta\n  grok-3-mini-fast-beta:\n    reasoning_effort: medium\n    tool_call_model: grok-3-beta\n  o1:\n    reasoning_effort: medium\n    tool_call_model: gpt-4.1\n  o3:\n    reasoning_effort: medium\n    tool_call_model: gpt-4.1\n  o3-mini:\n    reasoning_effort: medium\n    tool_call_model: gpt-4.1\n  o4-mini:\n    reasoning_effort: medium\n    tool_call_model: gpt-4.1\n</code></pre> <p>Note that in the configuration above, we use tool call models as the supplemental model for the reasoning models, this is because while reasoning models are capable of reasoning, in many use cases they are not very effective at tool calling.</p> <p>To override the default configurations, you can copy and paste the above configurations to your <code>~/.opsmate/config.yaml</code> file, and override the specific configurations you want to change.</p>"},{"location":"configurations/llm-configurations/#claude-37-sonnet-for-thinking","title":"Claude 3.7 Sonnet for Thinking","text":"<p>The <code>claude-3-7-sonnet-20250219</code> model is a powerful reasoning model with <code>thinking</code> enabled. To enable you can add the following configuration to your <code>~/.opsmate/config.yaml</code> file:</p> <pre><code>OPSMATE_MODELS_CONFIG:\n  claude-3-7-sonnet-20250219:\n    thinking:\n      budget_tokens: 1024\n      type: enabled\n  # ...\n</code></pre>"},{"location":"configurations/use-cloud-storage-for-embeddings-storage/","title":"Use Cloud Storage for Embeddings Storage","text":"<p>Opsmate uses LanceDB to store knowledge bases. By default we store the knowledge base in the local filesystem, default at <code>~/.opsmate/embeddings</code>, and configures as <code>OPSMATE_EMBEDDINGS_DB_PATH</code>.</p> <p>The full pros and cons of storage considerations are covered in the LanceDB Storage `documentation.</p> <p>Currently In addition to local filesystem, Opsmate officially supports AWS S3 and Azure Blob Storage based cloud storage. That being said we expect other approaches suggested by LanceDB to work as well.</p>"},{"location":"configurations/use-cloud-storage-for-embeddings-storage/#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have already provisioned the cloud storage bucket.</li> <li>You must have read-only+ access to the cloud storage bucket.</li> </ul>"},{"location":"configurations/use-cloud-storage-for-embeddings-storage/#how-to-use-cloud-storage-for-embeddings-storage","title":"How to use cloud storage for embeddings storage","text":"Environment VariableCLIConfig File <p>Simply set the <code>OPSMATE_EMBEDDINGS_DB_PATH</code> environment variable to the cloud storage path. <pre><code># AWS S3\nOPSMATE_EMBEDDINGS_DB_PATH=s3://bucket/path\n# Azure Blob Storage\nOPSMATE_EMBEDDINGS_DB_PATH=az://bucket/path\n# Google Cloud Storage\nOPSMATE_EMBEDDINGS_DB_PATH=gs://bucket/path\n</code></pre></p> <p>Use the <code>--embeddings-db-path</code> flag to specify the cloud storage path. <pre><code># AWS S3\nopsmate ingest --embeddings-db-path=s3://bucket/path\n# Azure Blob Storage\nopsmate ingest --embeddings-db-path=az://bucket/path\n# Google Cloud Storage\nopsmate ingest --embeddings-db-path=gs://bucket/path\n</code></pre></p> <p>Alternatively you can also set the config in <code>~/.opsmate/config.yaml</code>: <pre><code># AWS S3\nOPSMATE_EMBEDDINGS_DB_PATH: s3://bucket/path\n# Azure Blob Storage\nOPSMATE_EMBEDDINGS_DB_PATH: az://bucket/path\n# Google Cloud Storage\nOPSMATE_EMBEDDINGS_DB_PATH: gs://bucket/path\n</code></pre></p> <p>Please refer to the LanceDB Configure Cloud Storage for more details.</p>"},{"location":"cookbooks/","title":"Comprehensive Opsmate Cookbook Collection","text":""},{"location":"cookbooks/#cookbooks","title":"Cookbooks","text":"<p>Welcome to the Opsmate Cookbook Collection. Here you will find a variety of examples and recipes to help you to use Opsmate effectively.</p>"},{"location":"cookbooks/#links","title":"Links","text":"<ul> <li>Use Opsmate for Automation</li> <li>Use Opsmate to Manage VMs</li> <li>Plugin System</li> <li>5 Levels of Workflow Orchestration</li> <li>Knowledge Management</li> <li>Docker Runtime</li> <li>Kubernetes Runtime</li> <li>Interacting with MySQL using Opsmate</li> </ul>"},{"location":"cookbooks/5-levels-of-workflow-orchestration/","title":"5 Levels of Workflow Orchestration","text":"<p>Ever find the code you wrote too spaghetti? Or you want to parallelise some of the steps in the workflow for the sake of better performance and user experience? Or simply retry some of the steps to get different results?</p> <p>Workflow IMO is particularly relevant in the context of LLM because despite the fact that LLM poses a lot of knowledge and capability of reasoning, without knowing the context of the institutional context and SOP (standard operating procedure), it is likely to produce results that are suboptimal. Workflow (or SOP if you want to think of it this way) is a way to provide a narrow pathway for LLM to follow. It comes with a few benefits:</p> <ol> <li>As an operator or engineer who develop LLM-based solutions, a rigid workflow is a way of managing the expectation of the user.</li> <li>With a workflow your SOP is broken down into a series of steps. It's easier for the developer to systematically improving each steps capability, thus improving the overall performance of the workflow.</li> <li>A narrow pathway that reflects the institutional SOP, meaning it's more capable of producing results at scale.</li> <li>It's simpler to manage the human-in-the-loop with workflow as you have clear steps where human operator can intervene.</li> </ol> <p>In this cookbook, we will be looking at 5 levels of workflow orchestration, for the purpose of fun and learning.</p> <p>Here are the 5 levels:</p> <ul> <li>Level 1: Vanilla Async Calls</li> <li>Level 2: DSL-based DAG with parallel execution</li> <li>Level 3: DAG with human-in-the-loop</li> <li>Level 4: DAG with reasoning</li> <li>Level 5: Workflow that support loop back</li> </ul> In\u00a0[1]: Copied! <pre>from opsmate.dino import dino\nfrom pydantic import BaseModel\nimport asyncio\n\nclass HomeTownInfo(BaseModel):\n  person_name: str\n  home_town: str\n\n@dino(\n  model=\"gpt-4o-mini\",\n  response_model=HomeTownInfo,\n)\nasync def home_town(person_name: str) -&gt; str:\n  \"\"\"\n  You are given a person's name, return the home town of the person.\n  \"\"\"\n  return person_name\n\n\n@dino(\n  model=\"gpt-4o-mini\",\n  response_model=str,\n)\nasync def distance(h1: HomeTownInfo, h2: HomeTownInfo) -&gt; str:\n  \"\"\"\n  You are given two places, return the distance between the two places in km.\n  \"\"\"\n  return f\"The distance between {h1.home_town} and {h2.home_town}\"\n\n\nresults = await asyncio.gather(*[home_town(\"Tony Blair\"), home_town(\"Elon Musk\")])\n\nprint(results)\nprint(await distance(results[0], results[1]))\n</pre> from opsmate.dino import dino from pydantic import BaseModel import asyncio  class HomeTownInfo(BaseModel):   person_name: str   home_town: str  @dino(   model=\"gpt-4o-mini\",   response_model=HomeTownInfo, ) async def home_town(person_name: str) -&gt; str:   \"\"\"   You are given a person's name, return the home town of the person.   \"\"\"   return person_name   @dino(   model=\"gpt-4o-mini\",   response_model=str, ) async def distance(h1: HomeTownInfo, h2: HomeTownInfo) -&gt; str:   \"\"\"   You are given two places, return the distance between the two places in km.   \"\"\"   return f\"The distance between {h1.home_town} and {h2.home_town}\"   results = await asyncio.gather(*[home_town(\"Tony Blair\"), home_town(\"Elon Musk\")])  print(results) print(await distance(results[0], results[1]))  <pre>[HomeTownInfo(person_name='Tony Blair', home_town='Edinburgh'), HomeTownInfo(person_name='Elon Musk', home_town='Pretoria, South Africa')]\nThe distance between Edinburgh, Scotland, and Pretoria, South Africa, is approximately 8,600 kilometers (5,343 miles).\n</pre> <p>In this relatively simple example, the code is reasonably manageable. That being said there are a few issues:</p> <ol> <li>We need to topologically sort the procedures, which is not always easy.</li> <li>We have to manually manage the concurrency of the steps.</li> </ol> <p>This is where we can use the DSL-based DAG to help us.</p> In\u00a0[2]: Copied! <pre>from opsmate.workflow.workflow import (\n    step,\n    step_factory,\n    StatelessWorkflowExecutor,\n    WorkflowContext,\n)\nfrom pydantic import BaseModel\nfrom opsmate.dino import dino\n\n\nclass HomeTownInfo(BaseModel):\n    person_name: str\n    home_town: str\n\n\ndef home_town(person_name: str):\n    @step_factory\n    @step\n    @dino(\n        model=\"gpt-4o-mini\",\n        response_model=HomeTownInfo,\n    )\n    async def _home_town(ctx) -&gt; str:\n        \"\"\"\n        You are given a person's name, return the home town of the person.\n        \"\"\"\n        return ctx.metadata[\"person_name\"]\n\n    return _home_town(metadata={\"person_name\": person_name})\n\n\n@step\n@dino(\n    model=\"gpt-4o-mini\",\n    response_model=str,\n)\nasync def distance(ctx) -&gt; str:\n    \"\"\"\n    You are given two places, return the distance between the two places in km.\n    \"\"\"\n    return f\"The distance between {ctx.step_results[0].home_town} and {ctx.step_results[1].home_town}\"\n\n\nworkflow = (home_town(\"Tony Blair\") | home_town(\"Elon Musk\")) &gt;&gt; distance\n\nexecutor = StatelessWorkflowExecutor(workflow)\n\nctx = WorkflowContext()\nawait executor.run(ctx)\n\nprint(ctx.results['distance'])\n</pre> from opsmate.workflow.workflow import (     step,     step_factory,     StatelessWorkflowExecutor,     WorkflowContext, ) from pydantic import BaseModel from opsmate.dino import dino   class HomeTownInfo(BaseModel):     person_name: str     home_town: str   def home_town(person_name: str):     @step_factory     @step     @dino(         model=\"gpt-4o-mini\",         response_model=HomeTownInfo,     )     async def _home_town(ctx) -&gt; str:         \"\"\"         You are given a person's name, return the home town of the person.         \"\"\"         return ctx.metadata[\"person_name\"]      return _home_town(metadata={\"person_name\": person_name})   @step @dino(     model=\"gpt-4o-mini\",     response_model=str, ) async def distance(ctx) -&gt; str:     \"\"\"     You are given two places, return the distance between the two places in km.     \"\"\"     return f\"The distance between {ctx.step_results[0].home_town} and {ctx.step_results[1].home_town}\"   workflow = (home_town(\"Tony Blair\") | home_town(\"Elon Musk\")) &gt;&gt; distance  executor = StatelessWorkflowExecutor(workflow)  ctx = WorkflowContext() await executor.run(ctx)  print(ctx.results['distance']) <pre>2025-01-15 20:03:36 [info     ] rshift                         left=Step(WorkflowType.PARALLEL-725095f7) right=Step(distance-05e141bd)\n2025-01-15 20:03:36 [info     ] Running round                  round=1 steps_left=5\n2025-01-15 20:03:36 [info     ] Running step                   prev_size=0 step=Step(_home_town-7b526f4a) step_op=&lt;WorkflowType.NONE: 'none'&gt;\n2025-01-15 20:03:36 [info     ] Running step                   prev_size=0 step=Step(_home_town-b2cc3e1f) step_op=&lt;WorkflowType.NONE: 'none'&gt;\n2025-01-15 20:03:37 [info     ] Running round                  round=2 steps_left=3\n2025-01-15 20:03:37 [info     ] Running step                   prev_size=0 step=Step(WorkflowType.PARALLEL-725095f7) step_op=&lt;WorkflowType.PARALLEL: 'parallel'&gt;\n2025-01-15 20:03:37 [info     ] Running round                  round=3 steps_left=2\n2025-01-15 20:03:37 [info     ] Running step                   prev_size=0 step=Step(WorkflowType.SEQUENTIAL-5c6494e1) step_op=&lt;WorkflowType.SEQUENTIAL: 'sequential'&gt;\n2025-01-15 20:03:37 [info     ] Running round                  round=4 steps_left=1\n2025-01-15 20:03:37 [info     ] Running step                   prev_size=0 step=Step(distance-05e141bd) step_op=&lt;WorkflowType.NONE: 'none'&gt;\nThe distance between Edburton, West Sussex, England and Pretoria, South Africa is approximately 8,034 kilometers.\n</pre> <p>The code itself is slightly more wordly, however it allows:</p> <ol> <li>Parallel execution of steps (as you can see in the logs, workflow steps can be batched up in a single \"round\").</li> <li>Provide traceability of the steps</li> <li>It's easier to visualise the workflow, which is typically required as part of the LLM application requirement.</li> </ol> <p>In fact we can visualise it with the <code>draw_dot</code> function from the workflow module as shown below:</p> In\u00a0[4]: Copied! <pre>from opsmate.workflow.workflow import draw_dot\n\ndot = draw_dot(workflow, rankdir=\"TB\")\ndot\n</pre> from opsmate.workflow.workflow import draw_dot  dot = draw_dot(workflow, rankdir=\"TB\") dot  Out[4]: In\u00a0[5]: Copied! <pre>from opsmate.workflow.workflow import WorkflowExecutor, build_workflow\nfrom opsmate.workflow.models import SQLModel\nfrom sqlmodel import create_engine, Session\n\nengine = create_engine(\"sqlite:///:memory:\")\nSQLModel.metadata.create_all(engine)\n</pre> from opsmate.workflow.workflow import WorkflowExecutor, build_workflow from opsmate.workflow.models import SQLModel from sqlmodel import create_engine, Session  engine = create_engine(\"sqlite:///:memory:\") SQLModel.metadata.create_all(engine)  In\u00a0[5]: Copied! <pre># let's re-use the workflow from the previous level\nblueprint = (home_town(\"Tony Blair\") | home_town(\"Elon Musk\")) &gt;&gt; distance\n\nsession = Session(engine)\n\nworkflow = build_workflow(\"workflow\", \"Workflow\", blueprint, session)\n\nexecutor = WorkflowExecutor(workflow, session)\nctx = WorkflowContext()\nawait executor.run(ctx)\n\nprint(workflow.find_step(\"distance\", session).result)\n\n# find Elon Musk's result\nelon_step = workflow.find_step(\"_home_town\", session, metadata={\"person_name\": \"Elon Musk\"})\nelon_step_result = elon_step.result\nelon_step_result.home_town = \"New York\"\nelon_step.result = elon_step_result\nsession.add(elon_step)\nsession.commit()\n\nawait executor.mark_rerun(elon_step, self_rerun=False)\nawait executor.run(ctx)\n\nprint(workflow.find_step(\"distance\", session).result)\n</pre> # let's re-use the workflow from the previous level blueprint = (home_town(\"Tony Blair\") | home_town(\"Elon Musk\")) &gt;&gt; distance  session = Session(engine)  workflow = build_workflow(\"workflow\", \"Workflow\", blueprint, session)  executor = WorkflowExecutor(workflow, session) ctx = WorkflowContext() await executor.run(ctx)  print(workflow.find_step(\"distance\", session).result)  # find Elon Musk's result elon_step = workflow.find_step(\"_home_town\", session, metadata={\"person_name\": \"Elon Musk\"}) elon_step_result = elon_step.result elon_step_result.home_town = \"New York\" elon_step.result = elon_step_result session.add(elon_step) session.commit()  await executor.mark_rerun(elon_step, self_rerun=False) await executor.run(ctx)  print(workflow.find_step(\"distance\", session).result)  <pre>2025-01-15 18:20:34 [info     ] rshift                         left=Step(WorkflowType.PARALLEL-6627e483) right=Step(distance-e45c74ff)\n2025-01-15 18:20:34 [info     ] Running round                  round=1\n2025-01-15 18:20:34 [info     ] Running step                   step_id=1 step_name=_home_town\n2025-01-15 18:20:34 [info     ] Running step                   step_id=1 step_name=_home_town step_type=&lt;WorkflowType.NONE: 'none'&gt;\n2025-01-15 18:20:34 [info     ] Running step                   step_fn=home_town.&lt;locals&gt;._home_town step_id=1 step_name=_home_town\n2025-01-15 18:20:34 [info     ] Running step                   step_id=2 step_name=_home_town\n2025-01-15 18:20:34 [info     ] Running step                   step_id=2 step_name=_home_town step_type=&lt;WorkflowType.NONE: 'none'&gt;\n2025-01-15 18:20:34 [info     ] Running step                   step_fn=home_town.&lt;locals&gt;._home_town step_id=2 step_name=_home_town\n2025-01-15 18:20:34 [info     ] Running round                  round=2\n2025-01-15 18:20:34 [info     ] Running step                   step_id=3 step_name=\n2025-01-15 18:20:34 [info     ] Running step                   step_id=3 step_name= step_type=&lt;WorkflowType.PARALLEL: 'parallel'&gt;\n2025-01-15 18:20:34 [info     ] Running round                  round=3\n2025-01-15 18:20:34 [info     ] Running step                   step_id=4 step_name=\n2025-01-15 18:20:34 [info     ] Running step                   step_id=4 step_name= step_type=&lt;WorkflowType.SEQUENTIAL: 'sequential'&gt;\n2025-01-15 18:20:34 [info     ] Running round                  round=4\n2025-01-15 18:20:34 [info     ] Running step                   step_id=5 step_name=distance\n2025-01-15 18:20:34 [info     ] Running step                   step_id=5 step_name=distance step_type=&lt;WorkflowType.NONE: 'none'&gt;\n2025-01-15 18:20:34 [info     ] Running step                   step_fn=distance step_id=5 step_name=distance\nThe distance between Edinburgh, Scotland and Pretoria, South Africa is approximately 8,400 kilometers.\n2025-01-15 18:20:36 [info     ] Running round                  round=1\n2025-01-15 18:20:36 [info     ] Running step                   step_id=3 step_name=\n2025-01-15 18:20:36 [info     ] Running step                   step_id=3 step_name= step_type=&lt;WorkflowType.PARALLEL: 'parallel'&gt;\n2025-01-15 18:20:36 [info     ] Running round                  round=2\n2025-01-15 18:20:36 [info     ] Running step                   step_id=4 step_name=\n2025-01-15 18:20:36 [info     ] Running step                   step_id=4 step_name= step_type=&lt;WorkflowType.SEQUENTIAL: 'sequential'&gt;\n2025-01-15 18:20:36 [info     ] Running round                  round=3\n2025-01-15 18:20:36 [info     ] Running step                   step_id=5 step_name=distance\n2025-01-15 18:20:36 [info     ] Running step                   step_id=5 step_name=distance step_type=&lt;WorkflowType.NONE: 'none'&gt;\n2025-01-15 18:20:36 [info     ] Running step                   step_fn=distance step_id=5 step_name=distance\nThe approximate distance between Edinburgh, Scotland, and New York City, USA, is about 5,400 kilometers (approximately 3,355 miles). This distance represents a direct flight or great-circle distance.\n</pre> <p>In the example above. Instead of running a stateless workflow, we ran a workflow backed by a sqlite database so that it supports durable execution.</p> <p>We run the workflow, and it produces</p> <pre><code>The distance between Edinburgh, Scotland, and Pretoria, South Africa is approximately 8,300 kilometers.\n</code></pre> <p>Which is identical to the output from the stateless workflow execution.</p> <p>To demonstrate the human-in-the-loop, we corrected the output of Elon Musk's home town to \"New York\".</p> <p>We then re-run the workflow, and it produced</p> <pre><code>The distance between Edinburgh, Scotland, and New York, USA is approximately 5,900 kilometers.\n</code></pre> <p>The steps of the second persisted workflow execution is also shorter, which is expected because only the descendant steps of the \"Elon Musk\" step is re-run.</p> In\u00a0[11]: Copied! <pre>from opsmate.tools import FilesFind\nfrom opsmate.dino.react import react\nfrom opsmate.dino.tools import dtool\nfrom opsmate.workflow.workflow import step\nfrom typing import Annotated\nfrom pydantic import BaseModel, Field\nimport asyncio\nimport structlog\n\nlogger = structlog.get_logger()\n\n\n@dtool\nasync def count_loc(filepath: Annotated[str, \"The path of the exact file\"]) -&gt; str:\n    \"\"\"\n    Count the lines of code in the given file.\n    \"\"\"\n\n    output = await asyncio.create_subprocess_shell(\n        f\"wc -l {filepath}\",\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.STDOUT,\n    )\n    stdout, _ = await output.communicate()\n    return stdout.decode().strip()\n\n\n@dtool\nasync def add(numbers: Annotated[list[int], \"The numbers to add\"]) -&gt; str:\n    \"\"\"\n    Add the given numbers.\n    \"\"\"\n    return str(sum(numbers))\n\n\nclass Step(BaseModel):\n    description: str = Field(description=\"The description of the step\")\n\nclass Plan(BaseModel):\n    goal: str = Field(description=\"The goal of the task\")\n    steps: list[Step] = Field(description=\"Steps to solve the problem the steps must be in sequential order\")\n\n@step\n@dino(\n    model=\"gpt-4o\",\n    response_model=Plan,\n)\nasync def planning(ctx: WorkflowContext):\n    \"\"\"\n    You are a world-class planning algorithm.\n    You are given a problem, try to break it down into a series of steps in sequential order.\n    Think step by step to have a good understanding of the problem\n    \"\"\"\n    return ctx.input[\"problem\"]\n\n@react(\n    model=\"gpt-4o\",\n    tools=[FilesFind, count_loc, add],\n    iterable=True,\n)\nasync def _execute_plan(plan: Plan):\n    \"\"\"\n    You are given a plan, execute the plan by using the tools provided.\n    Please do it step by step.\n    \"\"\"\n    return f\"\"\"\n&lt;plan&gt;\n## Goal\n{plan.goal}\n\n## Steps\n{\"---\\n\".join([step.description for step in plan.steps])}\n&lt;/plan&gt;\n\"\"\"\n\n@step\nasync def execute_plan(ctx: WorkflowContext):\n    plan = ctx.step_results\n    async for result in await _execute_plan(plan):\n        _result = result\n        logger.info(\"thought process\", result=result)\n\n    return _result\n\nblueprint = planning &gt;&gt; execute_plan\n\nworkflow = build_workflow(\"plan-then-execute\", \"Plan then execute\", blueprint, session)\nexecutor = WorkflowExecutor(workflow, session)\n\nctx = WorkflowContext(input={\"problem\": \"Find all the jypyter notebook files in the current directory and count the number of lines each file, add the result and print it\"})\nawait executor.run(ctx)\n\nprint(ctx.results['execute_plan'])\n</pre> from opsmate.tools import FilesFind from opsmate.dino.react import react from opsmate.dino.tools import dtool from opsmate.workflow.workflow import step from typing import Annotated from pydantic import BaseModel, Field import asyncio import structlog  logger = structlog.get_logger()   @dtool async def count_loc(filepath: Annotated[str, \"The path of the exact file\"]) -&gt; str:     \"\"\"     Count the lines of code in the given file.     \"\"\"      output = await asyncio.create_subprocess_shell(         f\"wc -l {filepath}\",         stdout=asyncio.subprocess.PIPE,         stderr=asyncio.subprocess.STDOUT,     )     stdout, _ = await output.communicate()     return stdout.decode().strip()   @dtool async def add(numbers: Annotated[list[int], \"The numbers to add\"]) -&gt; str:     \"\"\"     Add the given numbers.     \"\"\"     return str(sum(numbers))   class Step(BaseModel):     description: str = Field(description=\"The description of the step\")  class Plan(BaseModel):     goal: str = Field(description=\"The goal of the task\")     steps: list[Step] = Field(description=\"Steps to solve the problem the steps must be in sequential order\")  @step @dino(     model=\"gpt-4o\",     response_model=Plan, ) async def planning(ctx: WorkflowContext):     \"\"\"     You are a world-class planning algorithm.     You are given a problem, try to break it down into a series of steps in sequential order.     Think step by step to have a good understanding of the problem     \"\"\"     return ctx.input[\"problem\"]  @react(     model=\"gpt-4o\",     tools=[FilesFind, count_loc, add],     iterable=True, ) async def _execute_plan(plan: Plan):     \"\"\"     You are given a plan, execute the plan by using the tools provided.     Please do it step by step.     \"\"\"     return f\"\"\"  ## Goal {plan.goal}  ## Steps {\"---\\n\".join([step.description for step in plan.steps])}  \"\"\"  @step async def execute_plan(ctx: WorkflowContext):     plan = ctx.step_results     async for result in await _execute_plan(plan):         _result = result         logger.info(\"thought process\", result=result)      return _result  blueprint = planning &gt;&gt; execute_plan  workflow = build_workflow(\"plan-then-execute\", \"Plan then execute\", blueprint, session) executor = WorkflowExecutor(workflow, session)  ctx = WorkflowContext(input={\"problem\": \"Find all the jypyter notebook files in the current directory and count the number of lines each file, add the result and print it\"}) await executor.run(ctx)  print(ctx.results['execute_plan'])  <pre>2025-01-15 18:32:55 [info     ] rshift                         left=Step(planning-9db1b9b7) right=Step(execute_plan-bf3f0b37)\n2025-01-15 18:32:55 [info     ] Running round                  round=1\n2025-01-15 18:32:55 [info     ] Running step                   step_id=18 step_name=planning\n2025-01-15 18:32:55 [info     ] Running step                   step_id=18 step_name=planning step_type=&lt;WorkflowType.NONE: 'none'&gt;\n2025-01-15 18:32:55 [info     ] Running step                   step_fn=planning step_id=18 step_name=planning\n2025-01-15 18:32:58 [info     ] Running round                  round=2\n2025-01-15 18:32:58 [info     ] Running step                   step_id=19 step_name=\n2025-01-15 18:32:58 [info     ] Running step                   step_id=19 step_name= step_type=&lt;WorkflowType.SEQUENTIAL: 'sequential'&gt;\n2025-01-15 18:32:58 [info     ] Running round                  round=3\n2025-01-15 18:32:58 [info     ] Running step                   step_id=20 step_name=execute_plan\n2025-01-15 18:32:58 [info     ] Running step                   step_id=20 step_name=execute_plan step_type=&lt;WorkflowType.NONE: 'none'&gt;\n2025-01-15 18:32:58 [info     ] Running step                   step_fn=execute_plan step_id=20 step_name=execute_plan\n2025-01-15 18:33:00 [info     ] thought process                result=React(thoughts='I need to find all files in the current directory and filter for Jupyter notebook files, then count the lines of code in each and sum them.', action='find files in the current directory')\n2025-01-15 18:33:01 [info     ] Tool called                    tool={\"output\":\"./plugin-system.ipynb\\n./5-levels-of-workflow-orchestration.ipynb\\n./automation-using-python-runtime.ipynb\",\"path\":\".\",\"filename\":\"*.ipynb\"}\n2025-01-15 18:33:03 [info     ] thought process                result=Observation(tool_outputs=[FilesFind(output='./plugin-system.ipynb\\n./5-levels-of-workflow-orchestration.ipynb\\n./automation-using-python-runtime.ipynb', path='.', filename='*.ipynb')], observation='Found the following Jupyter notebook files: plugin-system.ipynb, 5-levels-of-workflow-orchestration.ipynb, and automation-using-python-runtime.ipynb in the current directory.')\n2025-01-15 18:33:04 [info     ] thought process                result=React(thoughts='I have found the Jupyter notebook files. Now, I need to count the lines of code in each of these files.', action='Count the lines of code in all the Jupyter notebook file.')\n2025-01-15 18:33:06 [info     ] Tool called                    tool={\"filepath\":\"plugin-system.ipynb\",\"output\":\"554 plugin-system.ipynb\"}\n2025-01-15 18:33:06 [info     ] Tool called                    tool={\"filepath\":\"5-levels-of-workflow-orchestration.ipynb\",\"output\":\"581 5-levels-of-workflow-orchestration.ipynb\"}\n2025-01-15 18:33:06 [info     ] Tool called                    tool={\"filepath\":\"automation-using-python-runtime.ipynb\",\"output\":\"534 automation-using-python-runtime.ipynb\"}\n2025-01-15 18:33:08 [info     ] thought process                result=Observation(tool_outputs=[count_loc(filepath='plugin-system.ipynb', output='554 plugin-system.ipynb'), count_loc(filepath='5-levels-of-workflow-orchestration.ipynb', output='581 5-levels-of-workflow-orchestration.ipynb'), count_loc(filepath='automation-using-python-runtime.ipynb', output='534 automation-using-python-runtime.ipynb')], observation='The line count for each Jupyter notebook file is as follows: plugin-system.ipynb has 554 lines, 5-levels-of-workflow-orchestration.ipynb has 581 lines, and automation-using-python-runtime.ipynb has 534 lines.')\n2025-01-15 18:33:09 [info     ] thought process                result=React(thoughts='I have the line counts for each file: 554, 581, and 534. Now, I need to sum these numbers to get the total.', action='add 554, 581, and 534.')\n2025-01-15 18:33:10 [info     ] Tool called                    tool={\"numbers\":[554,581,534],\"output\":\"1669\"}\n2025-01-15 18:33:11 [info     ] thought process                result=Observation(tool_outputs=[add(numbers=[554, 581, 534], output='1669')], observation='The total number of lines of code across all the Jupyter notebook files is 1669.')\n2025-01-15 18:33:13 [info     ] thought process                result=ReactAnswer(answer='The total number of lines of code across all the Jupyter notebook files is 1669.')\nanswer='The total number of lines across all the Jupyter notebook files in the current directory is 1674.'\n</pre> <p>In the example above, we simply defined a very high level and simple workflow:</p> <pre><code>planning &gt;&gt; execute_plan\n</code></pre> <p>In the <code>planning</code> step, we are asking the LLM to break down the problem into a series of steps in sequential order.</p> <p>In the <code>execute_plan</code> step, we are asking the LLM to execute the plan by using the tools provided. Despite the plan presented to the LLM is step by step, the series of execution still leave a lot of room for the LLM to interpret. Here we use the ReAct to mimic the conventional thinking-acting-observing problem solving loop.</p> <p>One of the benefit of this approach is that again if we are not satisfied with the plan (either due to the flaws in the plan or question being ill-defined), we can directly refine the plan to influnce LLM's execution.</p>"},{"location":"cookbooks/5-levels-of-workflow-orchestration/#level-1-vanilla-async-calls","title":"Level 1: Vanilla Async Calls\u00b6","text":"<p>This is the simplest level of workflow orchestration. It's very much chaining async calls up manually via code and asyncio primitives.</p> <p>Let us create our first workflow.</p>"},{"location":"cookbooks/5-levels-of-workflow-orchestration/#level-2-dsl-based-dag-with-parallel-execution","title":"Level 2: DSL-based DAG with parallel execution\u00b6","text":"<p>We will use the Workflow DSL from Opsmate to define and execute the workflow.</p>"},{"location":"cookbooks/5-levels-of-workflow-orchestration/#level-3-dag-with-human-in-the-loop","title":"Level 3: DAG with human-in-the-loop\u00b6","text":"<p>The previous stateless workflow is working as intended, however there are a few issues:</p> <ol> <li>There is no way to correct the workflow output if the output is not desirable. This is because:<ul> <li>AI, just like human, sometimes makes mistakes, when it makes mistakes we either have to ask it to re-run, or take direct control to correct the output.</li> <li>Sometimes the question presented to the LLM is simply ill-defined or unclear, thus need constant refinement.</li> </ul> </li> <li>The state of the workflow is not persisted, meaning if the workflow is interrupted there is no way to recover.</li> </ol> <p>Luckily both issues can be solved via persisting the workflow state. In this level we will try to add a human-in-the-loop to the workflow, we will demonstate this via correcting the output of Tony Blair's home town.</p>"},{"location":"cookbooks/5-levels-of-workflow-orchestration/#level-4-dag-with-reasoning","title":"Level 4: DAG with reasoning\u00b6","text":"<p>Imagining in the real world as a platform engineer or SRE, you are presented with a problem, and ask it to solve it following the SOP. Despite the steps are well defined and broken down, chances are certain steps require quite a bit thinking and reasoning to solve.</p> <p>Same is applicable to LLM. There are tasks where it just cannot \"one-shot\" solve it, and requires thinking and trial and error.</p> <p>In this level, we will be looking at how to add reasoning to the workflow. Unlike the workflow examples above where the scope of each step is narrowly defined, in this level the scope of each step is broader, and requires reasoning to solve.</p>"},{"location":"cookbooks/5-levels-of-workflow-orchestration/#level-5-workflow-that-support-loop-back","title":"Level 5: Workflow that support loop back\u00b6","text":"<p>The idea is that upon the completion of a step, or during the execution of a step, the workflow itself has a self-reflexion to determine whether it is on the right track. If it is not, it will backtrack to the previous checkpoint and re-run rest of the steps.</p> <p>Coming soon</p>"},{"location":"cookbooks/automation-using-python-runtime/","title":"Use Opsmate for Automation","text":"<p>Opsmate is designed to be predominantly used via CLI and Web UI. That being said it is also trivial to use it for high-level automation via Python runtime. You can consider Opsmate as the \"AppleScript\" for your production enviornment.</p> <p>In this cookbook we will show you how to use Opsmate for performing automation tasks.</p> In\u00a0[4]: Copied! <pre>import getpass\nimport os\n\n\ndef _set_if_undefined(var: str) -&gt; None:\n    if os.environ.get(var):\n        return\n    os.environ[var] = getpass.getpass(var)\n\n\n_set_if_undefined(\"OPENAI_API_KEY\") # Feel to comment this out and use Anthropic API key instead\n_set_if_undefined(\"ANTHROPIC_API_KEY\")\n</pre> import getpass import os   def _set_if_undefined(var: str) -&gt; None:     if os.environ.get(var):         return     os.environ[var] = getpass.getpass(var)   _set_if_undefined(\"OPENAI_API_KEY\") # Feel to comment this out and use Anthropic API key instead _set_if_undefined(\"ANTHROPIC_API_KEY\")  In\u00a0[3]: Copied! <pre>from opsmate.dino import dino\n\n@dino(model=\"gpt-4o-mini\", response_model=str)\nasync def extract_phone_number(text: str):\n  \"\"\"\n  Extract phone number digits from the text\n  \"\"\"\n  return text\n\n\nphone_number = await extract_phone_number(\"My phone number is 123-456-7890\")\nassert phone_number == \"1234567890\"\nprint(phone_number)\n</pre> from opsmate.dino import dino  @dino(model=\"gpt-4o-mini\", response_model=str) async def extract_phone_number(text: str):   \"\"\"   Extract phone number digits from the text   \"\"\"   return text   phone_number = await extract_phone_number(\"My phone number is 123-456-7890\") assert phone_number == \"1234567890\" print(phone_number) <pre>1234567890\n</pre> <p>In the above script, we have defined a function <code>extract_phone_number</code> that takes a text as input and returns the extracted phone number as output.</p> <p>The <code>@dino</code> decorator is used to define the function, and it takes the following arguments:</p> <ul> <li><code>model</code>: The LLM model to use.</li> <li><code>response_model</code>: The type of the output.</li> </ul> <p>Note that there is a <code>Extract phone number digits from the text</code> docstring in the function definition. This is essentially used as a system prompt for the LLM to follow, and help it to understand the purpose of the function.</p> In\u00a0[4]: Copied! <pre>from pydantic import BaseModel, Field, field_validator\nfrom typing import List\n\n\nclass UserInfo(BaseModel):\n  name: str = Field(description=\"The name of the user\")\n  phone_number: str = Field(description=\"The phone number of the user, must be all digits\")\n\n  @field_validator(\"phone_number\")\n  def validate_phone_number(cls, v: str) -&gt; str:\n    if not v.isdigit():\n      raise ValueError(\"Phone number must be all digits\")\n    return v\n\n@dino(model=\"gpt-4o-mini\", response_model=List[UserInfo])\nasync def extract_user_info(text: str):\n  \"\"\"\n  Extract all the user information from the text\n  \"\"\"\n  return text\n\nuser_infos = await extract_user_info(\"\"\"\nYou can call Matt at 123-456-7890. John's number is the same except for the last digit being 1.\n\"\"\")\n\nassert len(user_infos) == 2\nassert user_infos[0].name == \"Matt\"\nassert user_infos[0].phone_number == \"1234567890\"\nassert user_infos[1].name == \"John\"\nassert user_infos[1].phone_number == \"1234567891\"\n\nprint(user_infos)\n</pre> from pydantic import BaseModel, Field, field_validator from typing import List   class UserInfo(BaseModel):   name: str = Field(description=\"The name of the user\")   phone_number: str = Field(description=\"The phone number of the user, must be all digits\")    @field_validator(\"phone_number\")   def validate_phone_number(cls, v: str) -&gt; str:     if not v.isdigit():       raise ValueError(\"Phone number must be all digits\")     return v  @dino(model=\"gpt-4o-mini\", response_model=List[UserInfo]) async def extract_user_info(text: str):   \"\"\"   Extract all the user information from the text   \"\"\"   return text  user_infos = await extract_user_info(\"\"\" You can call Matt at 123-456-7890. John's number is the same except for the last digit being 1. \"\"\")  assert len(user_infos) == 2 assert user_infos[0].name == \"Matt\" assert user_infos[0].phone_number == \"1234567890\" assert user_infos[1].name == \"John\" assert user_infos[1].phone_number == \"1234567891\"  print(user_infos)  <pre>[UserInfo(name='Matt', phone_number='1234567890'), UserInfo(name='John', phone_number='1234567891')]\n</pre> <p>In the example above we not only defined the output type as a list of <code>UserInfo</code>, but also make sure that the <code>phone_number</code> is all digits. Under the hood if the LLM returns a phone number that is not all digits, <code>dino</code> will automatically retry the function call for better results.</p> <p>Note that <code>dino</code> also comes with nuanced type hinting. If you hover over the <code>extract_user_info</code> function, you will see that it is typed as follows by your IDE/Text editor:</p> <pre><code>(function) def extract_user_info(text: str) -&gt; Awaitable[List[UserInfo]]\nExtract all the user information from the text\n</code></pre> In\u00a0[5]: Copied! <pre>from opsmate.plugins import PluginRegistry as plugin\nimport structlog\nimport logging\n\nstructlog.configure(\n    wrapper_class=structlog.make_filtering_bound_logger(logging.ERROR),\n)\nlogger = structlog.get_logger(__name__)\n\nplugin.discover()\n\n# You can also import it directly via\n# from opsmate.tools.command_line import ShellCommand as shell\nshell = plugin.get_tool(\"ShellCommand\")\n\nclass Info(BaseModel):\n  cpus: int = Field(description=\"The number of vCPUs on the machine\")\n  rams: int = Field(description=\"The number of GB of RAM on the machine\")\n\n@dino(model=\"gpt-4o-mini\", response_model=Info, tools=[shell])\nasync def run_command(instruction: str):\n  \"\"\"\n  As a sys admin accessing to a workstation, given the instruction,\n  run the cli and return the result\n  \"\"\"\n  return instruction\n\nresult = await run_command(\"How many cpus and rams on this machine?\")\nprint(result)\n</pre> from opsmate.plugins import PluginRegistry as plugin import structlog import logging  structlog.configure(     wrapper_class=structlog.make_filtering_bound_logger(logging.ERROR), ) logger = structlog.get_logger(__name__)  plugin.discover()  # You can also import it directly via # from opsmate.tools.command_line import ShellCommand as shell shell = plugin.get_tool(\"ShellCommand\")  class Info(BaseModel):   cpus: int = Field(description=\"The number of vCPUs on the machine\")   rams: int = Field(description=\"The number of GB of RAM on the machine\")  @dino(model=\"gpt-4o-mini\", response_model=Info, tools=[shell]) async def run_command(instruction: str):   \"\"\"   As a sys admin accessing to a workstation, given the instruction,   run the cli and return the result   \"\"\"   return instruction  result = await run_command(\"How many cpus and rams on this machine?\") print(result)  <pre>cpus=8 rams=29\n</pre> <p>In the example above, we have defined a function <code>run_command</code> that takes an instruction as input and returns the result of the command as output.</p> <p>We have also defined the output type as <code>Info</code>, which is a Pydantic model with two fields: <code>cpus</code> and <code>rams</code> with the description of the fields. This agains is one of the benefits of using Pydantic for structured output:</p> <ul> <li>The annotations not only provides a documentation for the output for clarity</li> <li>They are also sent to the LLM as part of the prompt, allowing LLM to understand the output format</li> <li>The validation is also performed by the Python runtime to ensure the legitimacy of the output.</li> </ul> <p>In the example above, we have also added the <code>shell</code> tool to the function, which is a tool call to the <code>ShellCommand</code> tool.</p> <p>Finally, we have called the <code>run_command</code> function with the instruction \"How many cpus and rams on this machine?\" and printed the result.</p> In\u00a0[6]: Copied! <pre>import pandas as pd\n\ndf = pd.DataFrame(plugin.get_tools().items(), columns=[\"Tool Name\", \"Description\"])\ndf[\"Description\"] = df[\"Tool Name\"].apply(lambda x: plugin.get_tool(x).__doc__.strip())\ndf\n</pre> import pandas as pd  df = pd.DataFrame(plugin.get_tools().items(), columns=[\"Tool Name\", \"Description\"]) df[\"Description\"] = df[\"Tool Name\"].apply(lambda x: plugin.get_tool(x).__doc__.strip()) df  Out[6]: Tool Name Description 0 FileAppend FileAppend tool allows you to append to a file 1 FileDelete FileDelete tool allows you to delete a file 2 FileRead FileRead tool allows you to read a file 3 FileWrite FileWrite tool allows you to write to a file 4 FilesFind FilesFind tool allows you to find files in a d... 5 FilesList FilesList tool allows you to list files in a d... 6 HttpCall HttpCall tool allows you to call a URL\\n    Su... 7 HttpGet HttpGet tool allows you to get the content of ... 8 HttpToText HttpToText tool allows you to convert an HTTP ... 9 KnowledgeRetrieval Knowledge retrieval tool allows you to search ... 10 ShellCommand ShellCommand tool allows you to run shell comm... 11 SysEnv SysEnv tool allows you to get the environment ... 12 SysStats SysStats tool allows you to get the stats of a... 13 current_time Get the current time in %Y-%m-%dT%H:%M:%SZ format 14 datetime_extraction You are tasked to extract the datetime range f... In\u00a0[7]: Copied! <pre>from opsmate.dino import dino\nfrom opsmate.dino.tools import dtool\nfrom typing import Annotated\n\n@dtool\n@dino(\"gpt-4o-mini\", response_model=str, tools=[shell])\nasync def k8s_agent(\n    question: Annotated[str, \"The question to solve\"],\n) -&gt; str:\n    \"\"\"\n    k8s_agent is a tool that solves a problem using kubectl.\n    \"\"\"\n    return f\"answer the question: {question}\"\n\n@dino(\"gpt-4o\", response_model=str, tools=[k8s_agent])\nasync def sre_manager(query: str):\n    \"\"\"\n    You are a world class SRE manager who manages a team of SREs.\n    \"\"\"\n    return f\"answer the query: {query}\"\n\nresult = await sre_manager(\"How many pods are running in the cluster?\")\nprint(result)\n</pre> from opsmate.dino import dino from opsmate.dino.tools import dtool from typing import Annotated  @dtool @dino(\"gpt-4o-mini\", response_model=str, tools=[shell]) async def k8s_agent(     question: Annotated[str, \"The question to solve\"], ) -&gt; str:     \"\"\"     k8s_agent is a tool that solves a problem using kubectl.     \"\"\"     return f\"answer the question: {question}\"  @dino(\"gpt-4o\", response_model=str, tools=[k8s_agent]) async def sre_manager(query: str):     \"\"\"     You are a world class SRE manager who manages a team of SREs.     \"\"\"     return f\"answer the query: {query}\"  result = await sre_manager(\"How many pods are running in the cluster?\") print(result) <pre>There are 19 pods currently running in the Kubernetes cluster.\n</pre> In\u00a0[8]: Copied! <pre>from opsmate.dino import dino\nfrom typing import Literal\n\nbrand = Literal[\"openai\", \"anthropic\"]\n\n@dino(model=\"gpt-4o-mini\", response_model=brand)\nasync def query_model():\n  \"\"\"\n  Who creates ya?\n  \"\"\"\n  return \"The language model provider\"\n\nresult = await query_model()\nassert result == \"openai\"\nprint(f\"When the model is gpt-4o-mini, the result is {result}\")\n\nresult = await query_model(model=\"claude-3-5-sonnet-20241022\")\nassert result == \"anthropic\"\nprint(f\"When the model is claude-3-5-sonnet-20241022, the result is {result}\")\n</pre> from opsmate.dino import dino from typing import Literal  brand = Literal[\"openai\", \"anthropic\"]  @dino(model=\"gpt-4o-mini\", response_model=brand) async def query_model():   \"\"\"   Who creates ya?   \"\"\"   return \"The language model provider\"  result = await query_model() assert result == \"openai\" print(f\"When the model is gpt-4o-mini, the result is {result}\")  result = await query_model(model=\"claude-3-5-sonnet-20241022\") assert result == \"anthropic\" print(f\"When the model is claude-3-5-sonnet-20241022, the result is {result}\")  <pre>When the model is gpt-4o-mini, the result is openai\nWhen the model is claude-3-5-sonnet-20241022, the result is anthropic\n</pre>"},{"location":"cookbooks/automation-using-python-runtime/#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>You have a OpenAI API key, otherwise Anthropic API key is also supported, as Opsmate is LLM provider agnostic.</li> <li>You have Opsmate installed - see getting started for more details.</li> </ul>"},{"location":"cookbooks/automation-using-python-runtime/#setup","title":"Setup\u00b6","text":"<p>First, let's install our required packages and set our API keys.</p>"},{"location":"cookbooks/automation-using-python-runtime/#introducing-dino","title":"Introducing <code>dino</code>\u00b6","text":"<p>Under the hood of Opsmate it is powered by <code>dino</code> (short for \"Dino IS NOT Opsmate\") - a lightweight framework that allows you to write LLM powered scripts in a functional manner.</p> <p>Here are some of the core design principles of <code>dino</code>:</p> <ul> <li>Enable end-developers to write code in a high-level and functional manner.</li> <li>Extact the implementation details of an execution procedure away from the code and delegate it to LLM tool calls, so that the end-developers can focus on the business logic.</li> <li>Structured outputs out of box over raw text outputs/schemas, allowing easy validation, chaining and integration with other functions, libraries and tools.</li> <li>The LLM is swappable, allowing you to use different LLM providers without changing the code.</li> </ul>"},{"location":"cookbooks/automation-using-python-runtime/#getting-started","title":"Getting Started\u00b6","text":"<p>Let's start with a simple script that will show you how to use Opsmate for scripting.</p>"},{"location":"cookbooks/automation-using-python-runtime/#structured-outputs","title":"Structured Outputs\u00b6","text":"<p>Structured output is one of the core features of <code>dino</code>. It allows you to define the output type of your function in a structured manner, and <code>dino</code> will automatically parse the output for you.</p> <p>In the example above, we have defined the output type as <code>str</code>, and <code>dino</code> will automatically parse the output for you. It also support more complex and nuanced structures, namely Pydantic models.</p> <p>Here is an example of structured output:</p>"},{"location":"cookbooks/automation-using-python-runtime/#tool-calls","title":"Tool Calls\u00b6","text":"<p>Most of the time you will need opsmate to interact with the production environment, in which case you will need to use \"tool calls\" to the system as the LLM along has no knowledge of your system.</p> <p>Here is an example of how to use tool calls to achieve your goal:</p>"},{"location":"cookbooks/automation-using-python-runtime/#built-in-tools","title":"Built-in Tools\u00b6","text":"<p>To know all the tools available you can run <code>plugin.get_tools()</code>. This will return a list of all the tools available including:</p> <ul> <li>The builtin tools, which are shipped with Opsmate and shown in the table below.</li> <li>The custom tools you have defined - we will cover this in a later section.</li> </ul>"},{"location":"cookbooks/automation-using-python-runtime/#agentic-via-llm-call-as-a-tool","title":"\"Agentic\" via LLM Call as a Tool\u00b6","text":"<p>By now you might wonder can we make the LLM call as a tool call? The answer is yes, and this is a powerful feature of <code>dino</code> and <code>dtool</code>.</p> <p>Here is an example of how to use LLM call as a tool call:</p>"},{"location":"cookbooks/automation-using-python-runtime/#model-swapping","title":"Model Swapping\u00b6","text":"<p>By default you provide a model to the <code>dino</code> decorator. It will be used as the default-sane model for the executing the function. It is also trivial to swap the model at runtime.</p> <p>The example below demonstrates how to do it:</p>"},{"location":"cookbooks/docker-runtime/","title":"Docker Runtime","text":"<p>This cookbook will guide you through how to interact with Docker container using Opsmate's docker runtime.</p>"},{"location":"cookbooks/docker-runtime/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed on your machine</li> <li>Opsmate installed on your machine</li> </ul>"},{"location":"cookbooks/docker-runtime/#example-1-interact-with-a-pre-existing-docker-container","title":"Example 1: Interact with a pre-existing docker container","text":"<p>First thing first let's create a docker container running in the background.</p> <pre><code>docker run -d --name testbox --rm ubuntu:20.04 sleep infinity\n</code></pre> <p>Now with the container running, we can interact with it using Opsmate's docker runtime.</p> <pre><code># -nt only prints out the answer\n$ opsmate run -nt --shell-command-runtime docker --runtime-docker-container-name testbox \"what is the os distro\"\nThe OS distribution is Ubuntu 20.04.6 LTS (Focal Fossa).\n</code></pre> <p>You can also use solve and chat to interact with the container.</p>"},{"location":"cookbooks/docker-runtime/#example-2-interact-with-a-docker-container-from-docker-compose","title":"Example 2: Interact with a docker container from docker-compose","text":"<p>Docker Compose is a tool for defining and running multi-container Docker applications. In conjunction with Opsmate's docker runtime, you can achieve goals such as:</p> <ul> <li>Executing exploratory experiments within a containerised environment.</li> <li>Use the containerised runtime as a workstation powered by AI, such as the three-musketeers approach</li> <li>You need to use a containerised runtime to run complicated evaluation tasks, which otherwise is not feasible to run on your host space.</li> </ul> <p>Let's say we have the following <code>docker-compose.yml</code> file:</p> <pre><code>services:\n  default:\n    image: ubuntu:24.04\n    init: true\n    entrypoint: [\"sleep\", \"infinity\"]\n  redis:\n    image: redis:latest\n</code></pre> <p>To interact with the environment you can run:</p> <pre><code>opsmate chat --shell-command-runtime docker\n</code></pre> <p>By default it will auto detect the <code>docker-compose.yml</code> file in the current directory, and use the <code>default</code> service as the container to interact with.</p> <p>You can also specify the <code>docker-compose.yml</code> file and the service you want to interact with:</p> <pre><code># investigate the redis service\nopsmate solve \\\n  --runtime docker \\\n  --runtime-docker-compose-file ./docker-compose.yml \\\n  --runtime-docker-service-name redis \\\n  \"what are the name of the processes that are running, find it out using the /proc directory\"\n</code></pre> <p>Here are some of the common configuration options for the docker runtime:</p> <pre><code>  --runtime-docker-service-name TEXT\n  --runtime-docker-compose-file TEXT\n                                  Path to the docker compose file (env:\n                                  docker-compose.yml]\n  --runtime-docker-shell TEXT     Set shell_cmd (env: RUNTIME_DOCKER_SHELL)\n  --runtime-docker-container-name TEXT\n</code></pre>"},{"location":"cookbooks/docker-runtime/#see-also","title":"See Also","text":"<ul> <li>Kubernetes Runtime</li> <li>SSH Runtime</li> </ul>"},{"location":"cookbooks/interacting-with-mysql-using-opsmate/","title":"Interacting with MySQL using Opsmate","text":"<p>This cookbook highlights how to create a tool that can be used to interact with a MySQL database.</p> <p>The full code for this cookbook can be found here.</p>"},{"location":"cookbooks/interacting-with-mysql-using-opsmate/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have Opsmate installed. Please refer to the installation guide for more information.</li> <li>You have docker and docker compose installed for the purpose of spinning up a MySQL server.</li> </ul>"},{"location":"cookbooks/interacting-with-mysql-using-opsmate/#should-i-use-this-tool","title":"Should I use this tool?","text":"<p> This is an early prototype and the protocol is yet to be finalized. </p> <p>Here is the guide to help you to make decisions about whether you should use this tool at the moment:</p> Situation Recommendation I am not sure if this tool is mature enough for my use case Don't use it I want this tool to perform all the production db administration tasks for me Absolutely not There is a pressing production issue that needs to be resolved urgently, this mysql plugin might be useful Seriously NO I really want to use this tool but I'm worried about PII and data privacy implications Don't use it I have a non-production database and I want to test this tool Maybe"},{"location":"cookbooks/interacting-with-mysql-using-opsmate/#installation","title":"Installation","text":"<p>Change directory to this folder and run: <pre><code>cd examples/tools/mysql\nopsmate install -e .\n</code></pre></p>"},{"location":"cookbooks/interacting-with-mysql-using-opsmate/#usage","title":"Usage","text":"<p>First, start the MySQL server using docker-compose: Note we have a x-for-pet database schema and sample data in the <code>fixtures/mydb.sql</code> file. <pre><code>docker compose -f fixtures/docker-compose.yml up\n</code></pre></p> <p>Then you can test the tool by running:</p> <pre><code>opsmate chat --mysql-tool-runtime mysql \\\n  --runtime-mysql-password my-secret-pw \\\n  --runtime-mysql-host localhost \\\n  --tools MySQLTool\n</code></pre>"},{"location":"cookbooks/interacting-with-mysql-using-opsmate/#implementation-details","title":"Implementation Details","text":"<p>The tool is implemented in the <code>mysql/tool.py</code> file.</p> <p>The tool uses the <code>MySQLRuntime</code> class to connect to the MySQL server, which is implements the <code>Runtime</code> interface. It is implemented in the <code>mysql/runtime.py</code> file.</p> <p>In the pyproject.toml file you can find the entry points for the tool and the runtime:</p> <pre><code>[project.entry-points.\"opsmate.tools\"]\ntool = \"mysql.tool:MySQLTool\"\n\n[project.entry-points.\"opsmate.runtime.runtimes\"]\nruntime = \"mysql.runtime:MySQLRuntime\"\n</code></pre> <p>This is to make sure that the tools are \"autodiscovered\" by Opsmate on startup. To verify this you can run the following commands:</p> <pre><code># to verify the mysql tool is autodiscovered\nopsmate list-tools | grep -i mysql\n\u2502 MySQLTool           \u2502 MySQL tool\n</code></pre> <pre><code># to verify the mysql runtime is autodiscovered\nopsmate chat --help | grep -i mysql\n  --runtime-mysql-timeout INTEGER\n                                  The timeout of the MySQL server (env:\n                                  RUNTIME_MYSQL_TIMEOUT)  [default: 120]\n  --runtime-mysql-charset TEXT    The charset of the MySQL server (env:\n                                  RUNTIME_MYSQL_CHARSET)  [default: utf8mb4]\n  --runtime-mysql-database TEXT   The database of the MySQL server (env:\n                                  RUNTIME_MYSQL_DATABASE)\n  --runtime-mysql-password TEXT   The password of the MySQL server (env:\n                                  RUNTIME_MYSQL_PASSWORD)  [default: \"\"]\n  --runtime-mysql-user TEXT       The user of the MySQL server (env:\n                                  RUNTIME_MYSQL_USER)  [default: root]\n  --runtime-mysql-port INTEGER    The port of the MySQL server (env:\n                                  RUNTIME_MYSQL_PORT)  [default: 3306]\n  --runtime-mysql-host TEXT       The host of the MySQL server (env:\n                                  RUNTIME_MYSQL_HOST)  [default: localhost]\n</code></pre>"},{"location":"cookbooks/interacting-with-mysql-using-opsmate/#show-cases","title":"Show Cases","text":"<p>Here is an example of \"chatting\" with the <code>x-for-pet</code> database using Opsmate:</p> <p> </p> <p>Here is another example of Claude Sonnet 3.7 conducting database schema analysis (the text size is a bit small, please feel free to zoom in):</p> <p> </p>"},{"location":"cookbooks/k8s-runtime/","title":"Kubernetes Runtime","text":"<p>This cookbook demonstrates how to use the Kubernetes runtime to interact with a Kubernetes pod.</p>"},{"location":"cookbooks/k8s-runtime/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Kubernetes cluster</li> <li>Opsmate installed on your machine</li> </ul>"},{"location":"cookbooks/k8s-runtime/#interact-with-a-pre-existing-kubernetes-pod","title":"Interact with a pre-existing Kubernetes pod","text":"<p>First let's create a pod in the Kubernetes cluster.</p> <pre><code>kubectl run -i --tty --rm debug --image=alpine -- sh\n</code></pre> <p>Now that we have a pod running, we can interact with it using Opsmate's Kubernetes runtime.</p> <pre><code>opsmate run -nt --runtime\nk8s --runtime-k8s-pod debug \"what's the distro of this container?\" --tools ShellCommand\nThe container is running Alpine Linux, version 3.21.3.\n</code></pre> <p>Here are some of the common configuration options for the Kubernetes runtime:</p> <pre><code>  --runtime-k8s-shell TEXT        Set shell_cmd (env: RUNTIME_K8S_SHELL)\n  --runtime-k8s-container TEXT    Name of the container of the pod, if not\n  --runtime-k8s-pod TEXT          Set pod_name (env: RUNTIME_K8S_POD)\n  --runtime-k8s-namespace TEXT    Set namespace (env: RUNTIME_K8S_NAMESPACE)\n</code></pre>"},{"location":"cookbooks/k8s-runtime/#see-also","title":"See Also","text":"<ul> <li>Docker Runtime</li> <li>SSH Runtime</li> </ul>"},{"location":"cookbooks/knowledge-management/","title":"Knowledge Management","text":"<p>In this cookbook, we will learn how to use Opsmate to manage knowledge.</p> <p>Notes the knowledge management feature is currently in the early stage of development, the features and the UX are subject to change. At the moment 2 type of data source can be ingested as knowledge:</p> <ol> <li>Any text based files from your local file system or network-attached storage.</li> <li>Any text based files from Github repositories.</li> </ol> <p>We use lancedb as the underlying vector database to store the knowledge. We use lancedb mainly because of the serverless nature of the database where you can use the cloud storage as the backend, which reduces the cost of ownership.</p> <p>Knowledge retrieval is can be achieved via the <code>KnowledgeRetrival</code> tool - which is a built-in tool in Opsmate.</p> In\u00a0[3]: Copied! <pre>from opsmate.tools import KnowledgeRetrieval\n\nresult = await KnowledgeRetrieval(query=\"how to do env test against a real cluster in kubebuilder using environment variables?\").run()\n\nprint(result.summary)\n</pre> from opsmate.tools import KnowledgeRetrieval  result = await KnowledgeRetrieval(query=\"how to do env test against a real cluster in kubebuilder using environment variables?\").run()  print(result.summary)  <pre>2025-02-21 17:02:04 [info     ] running knowledge retrieval tool query=how to do env test against a real cluster in kubebuilder using environment variables?\nTo run envtest against a real cluster using Kubebuilder, you need to set specific environment variables to point to the existing cluster's control plane and binaries. Here are the key environment variables to use:\n\n1. **`USE_EXISTING_CLUSTER`**: Set this to `true` to connect to an existing cluster instead of creating a local control plane.\n\n2. **`KUBEBUILDER_ASSETS`**: This should point to the directory containing the binaries needed for your tests (like `kubectl`, `etcd`, and `kube-apiserver`).\n\n3. **`TEST_ASSET_KUBE_APISERVER`, `TEST_ASSET_ETCD`, `TEST_ASSET_KUBECTL`**: These variables can be set to the specific paths of the `kube-apiserver`, `etcd`, and `kubectl` binaries, respectively. They provide a more granular way to specify which binaries to use if they differ from the default ones.\n\n### Example of Setting Variables\nYou can export the necessary variables in your terminal session before running your tests:\n```bash\nexport USE_EXISTING_CLUSTER=true\nexport KUBEBUILDER_ASSETS=\"/path/to/binaries/\"\nexport TEST_ASSET_KUBE_APISERVER=\"/path/to/kube-apiserver\"\nexport TEST_ASSET_ETCD=\"/path/to/etcd\"\nexport TEST_ASSET_KUBECTL=\"/path/to/kubectl\"\n```\nAfter setting these environment variables, you can run your tests, and they will utilize the existing cluster rather than initializing a new one.\n</pre> <p>The question is fairly obscure, which in the past took me several hours to figure out with the help from mixture of google search and reading the kubebuilder documentation.</p> <p>With semantic search, the answer is returned in seconds.</p>"},{"location":"cookbooks/knowledge-management/#environment-variablebased-configuration-options","title":"Environment variablebased configuration Options\u00b6","text":""},{"location":"cookbooks/knowledge-management/#fs_embeddings_config","title":"FS_EMBEDDINGS_CONFIG\u00b6","text":"<p>This is a JSON key-value pair where the key is the path to the directory to be ingested and the value is the glob pattern to match the files to be ingested.</p> <p>Example usage:</p> <pre><code>FS_EMBEDDINGS_CONFIG='{\"./docs/cookbooks\": \"*.md\"}'\n</code></pre> <p>This will ingest all the markdown files in the <code>./docs/cookbooks</code> directory.</p>"},{"location":"cookbooks/knowledge-management/#github_embeddings_config","title":"GITHUB_EMBEDDINGS_CONFIG\u00b6","text":"<p>This is a JSON key-value pair where the key is the <code>owner/repo:optional[branch]</code> and the value is the glob pattern to match the files to be ingested.</p> <p>Example usage:</p> <pre><code>GITHUB_EMBEDDINGS_CONFIG='{\"opsmate/opsmate\": \"*.md\", \"kubernetes/kubernetes:test-branch\": \"*.txt\"}'\n</code></pre> <p>In the example above, the first entry will ingest all the markdown files in the <code>opsmate/opsmate</code> repository. The second entry will ingest all the text files in the <code>kubernetes/kubernetes</code> repository on the <code>test-branch</code> branch.</p> <p>If the branch is not specified, it will default to <code>main</code>.</p> <p>:important: The Github token is required to be set in the environment variable <code>GITHUB_TOKEN</code>.</p>"},{"location":"cookbooks/knowledge-management/#embedding_registry_name-and-embedding_model_name","title":"EMBEDDING_REGISTRY_NAME and EMBEDDING_MODEL_NAME\u00b6","text":"<p><code>EMBEDDING_REGISTRY_NAME</code> is the name of the embedding registry to use. It is default to <code>openai</code>.</p> <p><code>EMBEDDING_MODEL_NAME</code> is the name of the embedding model to use. It is default to <code>text-embedding-ada-002</code>.</p> <p>LanceDB supports wide range of embedding models, you can refer to the lancedb embedding documentation for more details.</p>"},{"location":"cookbooks/knowledge-management/#embeddings_db_path","title":"EMBEDDINGS_DB_PATH\u00b6","text":"<p>EMBEDDINGS_DB_PATH is the path to the lancedb database. It is default to <code>~/.data/opsmate-embeddings</code>.</p> <p>Right now it is defaulted to the local file system, but there are wide range of storage options supported by lancedb, you can refer to the lancedb storage documentation for more details. In the documentation it provides a very comprehensive diagram to show case the thought process that goes into choosing the right storage backend.</p> <p>WARNING: Currently the ingestion chunk size is set to 1000, with overlap set to 0, with recursive text splitter as the default chunking strategy. This is hardcoded right now through environment-variable based configuration, but we will support more flexible configuration in the future.</p>"},{"location":"cookbooks/knowledge-management/#splitter_config","title":"SPLITTER_CONFIG\u00b6","text":"<p>Currently there are 2 types of splitter:</p> <ol> <li>RecursiveTextSplitter</li> <li>MarkdownHeaderTextSplitter</li> </ol> <p>Here are the example configurations:</p> <pre>SPLITTER_CONFIG='{\"name\": \"recursive\", \"chunk_size\": 1000, \"chunk_overlap\": 0}' # this is the default configuration\n\n# OR\n\nSPLITTER_CONFIG='{\"name\": \"markdown_header\", \"headers_to_split_on\": [[\"#\", \"h1\"], [\"##\", \"h2\"], [\"###\", \"h3\"]]}'\n</pre>"},{"location":"cookbooks/knowledge-management/#sdk-based-data-ingestion","title":"SDK-based data ingestion\u00b6","text":"<p>You can also choose to ingest the knowledge via the SDK which provides greater flexibility in terms of configuration.</p> <p>In the example below, we ingest all the markdown files in the <code>docs/book/src</code> directory of the <code>kubernetes-sigs/kubebuilder</code> repository to learn about the kubebuilder.</p> <p>Note this is going to take a while to complete and emit a lot of logs so we are not going to run it here.</p> <pre>from opsmate.config import config\nimport asyncio\nfrom sqlmodel import create_engine, text\nimport structlog\nfrom opsmate.app.base import on_startup as base_app_on_startup\nfrom opsmate.ingestions import ingest_from_config\nfrom opsmate.config import Config\n\nlogger = structlog.get_logger()\n\n\nasync def main():\n    engine = create_engine(\n        config.db_url,\n        connect_args={\"check_same_thread\": False},\n        # echo=True,\n    )\n    with engine.connect() as conn:\n        conn.execute(text(\"PRAGMA journal_mode=WAL\"))\n        conn.close()\n\n    await base_app_on_startup(engine)\n\n    await ingest_from_config(\n        Config(\n            github_embeddings_config={\n                \"kubernetes-sigs/kubebuilder:master\": \"./docs/book/src/**/*.md\"\n            },\n            categorise=False,  # By default we categorise the knowledge into categories for better segmentation, but we disable it here for the sake of speed.\n        ),\n        engine=engine,\n    )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</pre> <p>You can initiate the ingestion via <code>OPSMATE_DB_URL=sqlite:////tmp/sqlite.db python main.py</code></p> <p>For the actual ingestion, start the background worker via <code>OPSMATE_DB_URL=sqlite:////tmp/sqlite.db python -m opsmate.dbqapp.app</code></p> <p>Once the knowledge of the kubebuilder is ingested, we can use the <code>KnowledgeRetrieval</code> tool to provide retrieval augmented generation (RAG):</p>"},{"location":"cookbooks/knowledge-management/#future-capabilities","title":"Future capabilities\u00b6","text":"<ul> <li>Right now the async based knowledge ingestion is fairly naive and is not designed to be run in a distributed and fault-tolerant manner. We need to design a more robust system to support this - Potentially brining in the big gun such as Celery but ideally anything easy to maintain and scale.</li> <li>We need to support more data source types, such as databases or other API-based data sources.</li> <li>Currently only text-based files are supported, we need to support more file types, such as images, videos, and other binary data.</li> </ul>"},{"location":"cookbooks/manage-vms/","title":"Manage VMs via SSH","text":"<p>In this cookbook we will demonstrate how to manage VMs using Opsmate.</p> <p>By default Opsmate runs shell commands in the same namespace as the opsmate process, but it also provides a <code>ssh</code> runtime that allows you to manage VMs using SSH. This is particularly useful when the virtual machine (VM) is:</p> <ul> <li>not accessible via the internet or running in an air-gapped network.</li> <li>cannot directly access the large language model (LLM) provider.</li> <li>a legacy system that cannot accommodate the runtime requirements of Opsmate (e.g. python 3.10+).</li> </ul>"},{"location":"cookbooks/manage-vms/#prerequisites","title":"Prerequisites","text":"<ul> <li>A VM instance</li> <li>Opsmate CLI</li> </ul>"},{"location":"cookbooks/manage-vms/#how-to-use-the-ssh-runtime","title":"How to use the SSH runtime","text":"<p>The remote runtime is available to <code>run</code>, <code>solve</code> and <code>chat</code> commands.</p> <p>Here is an example of how you can <code>chat</code> with a remote VM.</p> <pre><code>opsmate chat --shell-command-runtime ssh \\\n    --runtime-ssh-host &lt;vm-host&gt; \\\n    --runtime-ssh-username &lt;vm-username&gt;\n</code></pre> <p>The following asciinema demo shows how to use the SSH runtime to \"chat\" with a remote VM.</p> <p>Here are some of the common configuration options for the SSH runtime:</p> <pre><code>  --runtime-ssh-connect-retries INTEGER\n                                  Set connect_retries (env:\n                                  RUNTIME_SSH_CONNECT_RETRIES)  [default: 3]\n  --runtime-ssh-timeout INTEGER   Set timeout (env: RUNTIME_SSH_TIMEOUT)\n                                  [default: 10]\n  --runtime-ssh-shell TEXT        Set shell_cmd (env: RUNTIME_SSH_SHELL)\n                                  [default: /bin/bash]\n  --runtime-ssh-key-file TEXT     Set key_file (env: RUNTIME_SSH_KEY_FILE)\n  --runtime-ssh-password TEXT     Set password (env: RUNTIME_SSH_PASSWORD)\n  --runtime-ssh-username TEXT     Set username (env: RUNTIME_SSH_USERNAME)\n                                  [default: \"\"]\n  --runtime-ssh-port INTEGER      Set port (env: RUNTIME_SSH_PORT)  [default:\n                                  22]\n  --runtime-ssh-host TEXT         Set host (env: RUNTIME_SSH_HOST)  [default:\n                                  \"\"]\n</code></pre>"},{"location":"cookbooks/plugin-system/","title":"Plugin System","text":"<p>Opsmate has a handful of built-in tools you can use out of the box, however there are always customisations and extensions you want to build for your own use cases.</p> <p>Opsmate comes with a plugin system to allow you to build your own tools and use them in your automation. In fact it is used by the Opsmate CLI tool and the API/Web service.</p> <p>In this cookbook we will show you how to write and author your own plugins and use it in your automation via writing a Prometheus query plugin.</p> In\u00a0[1]: Copied! <pre>import getpass\nimport os\n\n\ndef _set_if_undefined(var: str) -&gt; None:\n    if os.environ.get(var):\n        return\n    os.environ[var] = getpass.getpass(var)\n\n\n_set_if_undefined(\"OPENAI_API_KEY\") # Feel to comment this out and use Anthropic API key instead\n_set_if_undefined(\"ANTHROPIC_API_KEY\")\n</pre> import getpass import os   def _set_if_undefined(var: str) -&gt; None:     if os.environ.get(var):         return     os.environ[var] = getpass.getpass(var)   _set_if_undefined(\"OPENAI_API_KEY\") # Feel to comment this out and use Anthropic API key instead _set_if_undefined(\"ANTHROPIC_API_KEY\")  <p>Then we will spin up a local k8s cluster and install the Prometheus Operator onto the local cluster.</p> In\u00a0[2]: Copied! <pre>! kind create cluster --name opsmate-plugin-test\n</pre> ! kind create cluster --name opsmate-plugin-test <pre>Creating cluster \"opsmate-plugin-test\" ...\n \u2713 Ensuring node image (kindest/node:v1.31.2) \ud83d\uddbc7l\n \u2713 Preparing nodes \ud83d\udce6 7l\n \u2713 Writing configuration \ud83d\udcdc7l\n \u2713 Starting control-plane \ud83d\udd79\ufe0f7l\n \u2713 Installing CNI \ud83d\udd0c7l\n \u2713 Installing StorageClass \ud83d\udcbe7l\nSet kubectl context to \"kind-opsmate-plugin-test\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-opsmate-plugin-test\n\nNot sure what to do next? \ud83d\ude05  Check out https://kind.sigs.k8s.io/docs/user/quick-start/\n</pre> In\u00a0[3]: Copied! <pre># make sure that you are running on the correct cluster\n! kubectl config current-context\n</pre> # make sure that you are running on the correct cluster ! kubectl config current-context <pre>kind-opsmate-plugin-test\n</pre> <p>Now we will install the LGTM stack on the cluster.</p> In\u00a0[4]: Copied! <pre>%%capture\n%%bash\n(\n  rm -rf /tmp/kube-prometheus\n  git clone https://github.com/prometheus-operator/kube-prometheus  --depth 1 /tmp/kube-prometheus\n  cd /tmp/kube-prometheus\n  kubectl apply --server-side -f manifests/setup\n  kubectl wait \\\n    --for condition=Established \\\n    --all CustomResourceDefinition \\\n    --namespace=monitoring\n  kubectl apply -f manifests/\n)\n</pre> %%capture %%bash (   rm -rf /tmp/kube-prometheus   git clone https://github.com/prometheus-operator/kube-prometheus  --depth 1 /tmp/kube-prometheus   cd /tmp/kube-prometheus   kubectl apply --server-side -f manifests/setup   kubectl wait \\     --for condition=Established \\     --all CustomResourceDefinition \\     --namespace=monitoring   kubectl apply -f manifests/ ) <p>The prometheus server is running on :9090 against the cluster local IP address thus likely unavailable in the host space. To make it available you can run</p> <pre>kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090\n</pre> <p>The <code>./plugins/prom.py</code> file contains the plugin we wrote for this cookbook. The job it tries to accomplish is reasonably complex:</p> <ul> <li>It extracts the datetime range from the user's query</li> <li>It queries the Prometheus API to get the metrics within the time range</li> <li>It returns the metrics to a string and represent it as a markdown table</li> </ul> In\u00a0[2]: Copied! <pre>! cat ./plugins/prom.py\n</pre> ! cat ./plugins/prom.py <pre>from opsmate.dino.types import ToolCall, PresentationMixin\nfrom pydantic import Field, PrivateAttr\nfrom typing import Optional, Any\nfrom httpx import AsyncClient\nfrom opsmate.dino import dino\nfrom opsmate.dino.types import Message\nfrom opsmate.tools.datetime import DatetimeRange, datetime_extraction\nfrom opsmate.plugins import auto_discover\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\nDEFAULT_ENDPOINT = \"http://localhost:9090\"\nDEFAULT_PATH = \"/api/v1/query_range\"\n\n\nclass PromQuery(ToolCall[dict[str, Any]], DatetimeRange, PresentationMixin):\n    \"\"\"\n    A tool to query metrics from Prometheus\n    \"\"\"\n\n    query: str = Field(description=\"The prometheus query\")\n    step: str = Field(\n        description=\"Query resolution step width in duration format or float number of seconds\",\n        default=\"15s\",\n    )\n    y_label: str = Field(\n        description=\"The y-axis label of the time series based on the query\",\n        default=\"Value\",\n    )\n    x_label: str = Field(\n        description=\"The x-axis label of the time series based on the query\",\n        default=\"Timestamp\",\n    )\n    title: str = Field(\n        description=\"The title of the time series based on the query\",\n        default=\"Time Series Data\",\n    )\n\n    _client: AsyncClient = PrivateAttr(default_factory=AsyncClient)\n\n    @property\n    def headers(self):\n        return {\n            \"Content-Type\": \"application/x-www-form-urlencoded\",\n            \"User-Agent\": \"opsmate prometheus tool\",\n        }\n\n    async def __call__(self, context: dict[str, Any] = {}):\n        endpoint = context.get(\"endpoint\", DEFAULT_ENDPOINT)\n        path = context.get(\"path\", DEFAULT_PATH)\n\n        response = await self._client.post(\n            endpoint + path,\n            data={\n                \"query\": self.query,\n                \"start\": self.start,\n                \"end\": self.end,\n                \"step\": self.step,\n            },\n            headers=self.headers,\n        )\n        return response.json()\n\n    class Config:\n        underscore_attrs_are_private = True\n\n    def markdown(self): ...\n\n    def time_series(self):\n        values = self.output[\"data\"][\"result\"][0][\"values\"]\n        timestamps = [datetime.fromtimestamp(ts) for ts, _ in values]\n        measurements = [float(val) for _, val in values]\n\n        df = pd.DataFrame({\"timestamp\": timestamps, \"measurement\": measurements})\n        plt.figure(figsize=(12, 6))\n        plt.plot(df[\"timestamp\"], df[\"measurement\"], marker=\"o\")\n        plt.grid(True)\n        plt.title(f\"{self.title} - {self.query}\")\n        plt.xlabel(self.x_label)\n        plt.ylabel(self.y_label)\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.show()\n\n\n@auto_discover(\n    name=\"prometheus_query\",\n    description=\"A tool to query metrics from Prometheus\",\n    version=\"0.0.1\",\n    author=\"Micky\",\n)\n@dino(\n    model=\"gpt-4o\",\n    response_model=PromQuery,\n    tools=[datetime_extraction],\n)\nasync def prometheus_query(query: str, extra_context: str = \"\"):\n    \"\"\"\n    You are a world class SRE who excels at querying metrics from Prometheus\n    You are given a query in natural language and you need to convert it into a valid Prometheus query\n    \"\"\"\n    return [\n        Message.user(content=extra_context),\n        Message.user(content=query),\n    ]\n</pre> In\u00a0[3]: Copied! <pre>from opsmate.plugins import PluginRegistry as plugins\n\nplugins.clear()\nplugins.discover(\"./plugins\", ignore_conflicts=True)\n</pre> from opsmate.plugins import PluginRegistry as plugins  plugins.clear() plugins.discover(\"./plugins\", ignore_conflicts=True)  <pre>2025-02-21 17:13:07 [debug    ] loading builtin tools         \n2025-02-21 17:13:07 [debug    ] loading builtin tools from     builtin_module=opsmate.tools\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=ACITool\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=FileAppend\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=FileDelete\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=FileRead\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=FileWrite\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=FilesFind\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=FilesList\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=GithubCloneAndCD\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=GithubRaisePR\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=HttpCall\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=HttpGet\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=HttpToText\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=KnowledgeRetrieval\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=ShellCommand\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=SysEnv\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=SysStats\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=current_time\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=datetime_extraction\n2025-02-21 17:13:07 [info     ] adding the plugin directory to the sys path plugin_dir=/home/jingkaihe/workspace/opsmate/docs/cookbooks/plugins\n2025-02-21 17:13:07 [info     ] loading plugin file            plugin_path=./plugins/prom.py\n2025-02-21 17:13:07 [info     ] Discovered plugin prometheus_query\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=PromQuery\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=ToolCall\n2025-02-21 17:13:07 [debug    ] loading dtool                  dtool=datetime_extraction\n2025-02-21 17:13:07 [warning  ] tool already exists            conflict_source=/home/jingkaihe/workspace/opsmate/opsmate/tools/__init__.py current_source=/home/jingkaihe/workspace/opsmate/docs/cookbooks/plugins/prom.py tool=datetime_extraction\n2025-02-21 17:13:07 [info     ] loaded plugin file             plugin_path=./plugins/prom.py\n</pre> <p>With the prometheus_query plugin discovered, we can now use it to query the Prometheus API.</p> In\u00a0[4]: Copied! <pre>prom_query_builder = plugins.get_plugin(\"prometheus_query\")\n\n# make sure the plugin is discovered\nassert prom_query_builder is not None\n\n# execute the plugin to build a query\nquery = await prom_query_builder.execute(\"number of pods within the cluster over the last 10 minutes\")\n\nprint(query)\n</pre> prom_query_builder = plugins.get_plugin(\"prometheus_query\")  # make sure the plugin is discovered assert prom_query_builder is not None  # execute the plugin to build a query query = await prom_query_builder.execute(\"number of pods within the cluster over the last 10 minutes\")  print(query)  <pre>2025-02-21 17:13:24 [info     ] Tool called                    tool={\"output\":\"2025-02-21T17:13:24Z\"}\n2025-02-21 17:13:25 [info     ] Tool called                    tool={\"text\":\"last 10 minutes\",\"output\":{\"start\":\"2025-02-21T17:03:24Z\",\"end\":\"2025-02-21T17:13:24Z\"}}\nstart='2025-02-21T17:03:24Z' end='2025-02-21T17:13:24Z' query='count(kube_pod_info)' step='15s' y_label='Number of Pods' x_label='Timestamp' title='Number of Pods in the Cluster over the Last 10 Minutes' output=None\n</pre> <p>As you have already seen The query builder is capable of figuring out the time range from the query, which is very much the Achilles heel of the LLM. In the specific example the query builder is able to figure out the time range from <code>over the last 10 minutes</code> via the <code>datetime_extraction</code> tool, which is part of the built-in tools.</p> <p>Now with the query built from the query builder, we can now execute the query to get the result.</p> <p>The custom method <code>time_series()</code> is a method that is defined in the plugin class to plot the time series data.</p> In\u00a0[5]: Copied! <pre>await query.run()\n\nquery.time_series()\n</pre> await query.run()  query.time_series()  <p>Here we will create an async function that wraps up the above procedure.</p> In\u00a0[11]: Copied! <pre>async def text_to_time_series(query: str):\n    query = await prom_query_builder.execute(query)\n    await query.run()\n    query.time_series()\n\nawait text_to_time_series(\"CPU utilisation of the nodes between 10 mins ago to 5 mins ago\")\n</pre> async def text_to_time_series(query: str):     query = await prom_query_builder.execute(query)     await query.run()     query.time_series()  await text_to_time_series(\"CPU utilisation of the nodes between 10 mins ago to 5 mins ago\")  <pre>2025-02-21 17:15:03 [info     ] Tool called                    tool={\"output\":\"2025-02-21T17:15:03Z\"}\n2025-02-21 17:15:03 [info     ] Tool called                    tool={\"output\":\"2025-02-21T17:15:03Z\"}\n2025-02-21 17:15:03 [info     ] Tool called                    tool={\"output\":\"2025-02-21T17:15:03Z\"}\n2025-02-21 17:15:03 [info     ] Tool called                    tool={\"output\":\"2025-02-21T17:15:03Z\"}\n2025-02-21 17:15:03 [info     ] Tool called                    tool={\"output\":\"2025-02-21T17:15:03Z\"}\n2025-02-21 17:15:05 [info     ] Tool called                    tool={\"text\":\"CPU utilisation of the nodes between 10 mins ago to 5 mins ago\",\"output\":{\"start\":\"2025-02-21T17:05:03Z\",\"end\":\"2025-02-21T17:15:03Z\"}}\n</pre> In\u00a0[12]: Copied! <pre>! kind delete cluster --name opsmate-plugin-test\n</pre> ! kind delete cluster --name opsmate-plugin-test <pre>Deleting cluster \"opsmate-plugin-test\" ...\nDeleted nodes: [\"opsmate-plugin-test-control-plane\"]\n</pre>"},{"location":"cookbooks/plugin-system/#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>You have a OpenAI API key, otherwise Anthropic API key is also supported, as Opsmate is LLM provider agnostic.</li> <li>You have Opsmate installed - see getting started for more details.</li> <li>You have kind installed, as Opsmate uses kind to run local Kubernetes cluster.</li> <li>You have kubectl installed for interacting with the local Kubernetes cluster.</li> </ul>"},{"location":"cookbooks/plugin-system/#setup","title":"Setup\u00b6","text":"<p>First, let's install our required packages and set our API keys.</p>"},{"location":"cookbooks/plugin-system/#our-first-plugin","title":"Our First Plugin\u00b6","text":""},{"location":"cookbooks/plugin-system/#discovering-plugins","title":"Discovering plugins\u00b6","text":"<p>In the plugin code if you have a keen eye you probably have already noticed the <code>@auto_discover</code> decorator. This is the key to discover the plugin.</p> <p>By default all the tools are discovered automatically, but for LLM functions you will need to explicitly mark it as discoverable via the <code>@auto_discover</code> decorator.</p> <p>To make the plugin discovered from where-ever your current python path is, you can execute the discovery via the following snippet:</p>"},{"location":"cookbooks/plugin-system/#cleanup","title":"Cleanup\u00b6","text":"<p>You can delete the cluster via running the following command:</p>"},{"location":"cookbooks/plugin-system/#known-limitations-of-the-existing-plugin-system","title":"Known Limitations of The Existing Plugin System\u00b6","text":"<p>There are a few known limitations of the existing plugin system, namely:</p> <p>1 The plugin system does not support (well no documented support) for installing extra Python dependencies. There are a few approaches to it, e.g. PythonVirtualenvOperator from the Airflow project, or embedding the the entire package system into a enterprise-ish bundle like Chef. 2. Lack other language runtime (e.g. Golang, NodeJS) support.</p> <p>All things considered a coherent solution is needed to make the above possible.</p>"},{"location":"cookbooks/plugin-system/#conclusion","title":"Conclusion\u00b6","text":"<p>In this cookbook we have shown you how to write and author your own Opsmate plugin and use it in your automation via writing a text-to-time-series-graph plugin.</p> <p>Obviously in the example the plugin is very naive with a few shortcomings, notably:</p> <ul> <li>Retrieval of the available metrics from the existing metrics system.</li> <li>Optimised prompt that increases the PromQL accuracy and precision.</li> <li>Only works with a single time series.</li> </ul> <p>That being said the whole purpose is to show you the ropes ;)</p>"},{"location":"cookbooks/plugins/prom/","title":"Prom","text":"In\u00a0[\u00a0]: Copied! <pre>from opsmate.dino.types import ToolCall, PresentationMixin\nfrom pydantic import Field, PrivateAttr\nfrom typing import Optional, Any\nfrom httpx import AsyncClient\nfrom opsmate.dino import dino\nfrom opsmate.dino.types import Message\nfrom opsmate.tools.datetime import DatetimeRange, datetime_extraction\nfrom opsmate.plugins import auto_discover\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n</pre> from opsmate.dino.types import ToolCall, PresentationMixin from pydantic import Field, PrivateAttr from typing import Optional, Any from httpx import AsyncClient from opsmate.dino import dino from opsmate.dino.types import Message from opsmate.tools.datetime import DatetimeRange, datetime_extraction from opsmate.plugins import auto_discover import pandas as pd import matplotlib.pyplot as plt from datetime import datetime In\u00a0[\u00a0]: Copied! <pre>DEFAULT_ENDPOINT = \"http://localhost:9090\"\nDEFAULT_PATH = \"/api/v1/query_range\"\n</pre> DEFAULT_ENDPOINT = \"http://localhost:9090\" DEFAULT_PATH = \"/api/v1/query_range\" In\u00a0[\u00a0]: Copied! <pre>class PromQuery(ToolCall[dict[str, Any]], DatetimeRange, PresentationMixin):\n    \"\"\"\n    A tool to query metrics from Prometheus\n    \"\"\"\n\n    query: str = Field(description=\"The prometheus query\")\n    step: str = Field(\n        description=\"Query resolution step width in duration format or float number of seconds\",\n        default=\"15s\",\n    )\n    y_label: str = Field(\n        description=\"The y-axis label of the time series based on the query\",\n        default=\"Value\",\n    )\n    x_label: str = Field(\n        description=\"The x-axis label of the time series based on the query\",\n        default=\"Timestamp\",\n    )\n    title: str = Field(\n        description=\"The title of the time series based on the query\",\n        default=\"Time Series Data\",\n    )\n\n    _client: AsyncClient = PrivateAttr(default_factory=AsyncClient)\n\n    @property\n    def headers(self):\n        return {\n            \"Content-Type\": \"application/x-www-form-urlencoded\",\n            \"User-Agent\": \"opsmate prometheus tool\",\n        }\n\n    async def __call__(self, context: dict[str, Any] = {}):\n        endpoint = context.get(\"endpoint\", DEFAULT_ENDPOINT)\n        path = context.get(\"path\", DEFAULT_PATH)\n\n        response = await self._client.post(\n            endpoint + path,\n            data={\n                \"query\": self.query,\n                \"start\": self.start,\n                \"end\": self.end,\n                \"step\": self.step,\n            },\n            headers=self.headers,\n        )\n        return response.json()\n\n    class Config:\n        underscore_attrs_are_private = True\n\n    def markdown(self): ...\n\n    def time_series(self):\n        values = self.output[\"data\"][\"result\"][0][\"values\"]\n        timestamps = [datetime.fromtimestamp(ts) for ts, _ in values]\n        measurements = [float(val) for _, val in values]\n\n        df = pd.DataFrame({\"timestamp\": timestamps, \"measurement\": measurements})\n        plt.figure(figsize=(12, 6))\n        plt.plot(df[\"timestamp\"], df[\"measurement\"], marker=\"o\")\n        plt.grid(True)\n        plt.title(f\"{self.title} - {self.query}\")\n        plt.xlabel(self.x_label)\n        plt.ylabel(self.y_label)\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.show()\n</pre> class PromQuery(ToolCall[dict[str, Any]], DatetimeRange, PresentationMixin):     \"\"\"     A tool to query metrics from Prometheus     \"\"\"      query: str = Field(description=\"The prometheus query\")     step: str = Field(         description=\"Query resolution step width in duration format or float number of seconds\",         default=\"15s\",     )     y_label: str = Field(         description=\"The y-axis label of the time series based on the query\",         default=\"Value\",     )     x_label: str = Field(         description=\"The x-axis label of the time series based on the query\",         default=\"Timestamp\",     )     title: str = Field(         description=\"The title of the time series based on the query\",         default=\"Time Series Data\",     )      _client: AsyncClient = PrivateAttr(default_factory=AsyncClient)      @property     def headers(self):         return {             \"Content-Type\": \"application/x-www-form-urlencoded\",             \"User-Agent\": \"opsmate prometheus tool\",         }      async def __call__(self, context: dict[str, Any] = {}):         endpoint = context.get(\"endpoint\", DEFAULT_ENDPOINT)         path = context.get(\"path\", DEFAULT_PATH)          response = await self._client.post(             endpoint + path,             data={                 \"query\": self.query,                 \"start\": self.start,                 \"end\": self.end,                 \"step\": self.step,             },             headers=self.headers,         )         return response.json()      class Config:         underscore_attrs_are_private = True      def markdown(self): ...      def time_series(self):         values = self.output[\"data\"][\"result\"][0][\"values\"]         timestamps = [datetime.fromtimestamp(ts) for ts, _ in values]         measurements = [float(val) for _, val in values]          df = pd.DataFrame({\"timestamp\": timestamps, \"measurement\": measurements})         plt.figure(figsize=(12, 6))         plt.plot(df[\"timestamp\"], df[\"measurement\"], marker=\"o\")         plt.grid(True)         plt.title(f\"{self.title} - {self.query}\")         plt.xlabel(self.x_label)         plt.ylabel(self.y_label)         plt.xticks(rotation=45)         plt.tight_layout()         plt.show() In\u00a0[\u00a0]: Copied! <pre>@auto_discover(\n    name=\"prometheus_query\",\n    description=\"A tool to query metrics from Prometheus\",\n    version=\"0.0.1\",\n    author=\"Micky\",\n)\n@dino(\n    model=\"gpt-4o\",\n    response_model=PromQuery,\n    tools=[datetime_extraction],\n)\nasync def prometheus_query(query: str, extra_context: str = \"\"):\n    \"\"\"\n    You are a world class SRE who excels at querying metrics from Prometheus\n    You are given a query in natural language and you need to convert it into a valid Prometheus query\n    \"\"\"\n    return [\n        Message.user(content=extra_context),\n        Message.user(content=query),\n    ]\n</pre> @auto_discover(     name=\"prometheus_query\",     description=\"A tool to query metrics from Prometheus\",     version=\"0.0.1\",     author=\"Micky\", ) @dino(     model=\"gpt-4o\",     response_model=PromQuery,     tools=[datetime_extraction], ) async def prometheus_query(query: str, extra_context: str = \"\"):     \"\"\"     You are a world class SRE who excels at querying metrics from Prometheus     You are given a query in natural language and you need to convert it into a valid Prometheus query     \"\"\"     return [         Message.user(content=extra_context),         Message.user(content=query),     ]"},{"location":"experiments/prometheus/","title":"Prometheus","text":"In\u00a0[1]: Copied! <pre>from opsmate.tools.knowledge_retrieval import KnowledgeRetrieval\nfrom opsmate.knowledgestore.models import aconn, Category, init_table\nfrom opsmate.tools.prom import PromQL, prometheus_query\n\nimport os\n\n# await init_table()\n\ndbconn = await aconn()\n\ntable = await dbconn.open_table(\"knowledge_store\")\n\nasync def prom_graph(query: str):\n    query = await prometheus_query(query, context={\n        \"llm_summary\": False,\n        \"top_n\": 20,\n    })\n\n    print(query)\n\n    await query.run(context={\n        \"prometheus_endpoint\": \"https://prometheus-prod-01-eu-west-0.grafana.net/api/prom\",\n        \"prometheus_user_id\": os.getenv(\"GRAFANA_USER_ID\"),\n        \"prometheus_api_key\": os.getenv(\"GRAFANA_API_KEY\"),\n    })\n\n    query.time_series()\n\n\nawait prom_graph(\"\"\"\n    cpu usage of the cluster over the past 2 hour, use 2m as the rate interval\n\"\"\")\n</pre> from opsmate.tools.knowledge_retrieval import KnowledgeRetrieval from opsmate.knowledgestore.models import aconn, Category, init_table from opsmate.tools.prom import PromQL, prometheus_query  import os  # await init_table()  dbconn = await aconn()  table = await dbconn.open_table(\"knowledge_store\")  async def prom_graph(query: str):     query = await prometheus_query(query, context={         \"llm_summary\": False,         \"top_n\": 20,     })      print(query)      await query.run(context={         \"prometheus_endpoint\": \"https://prometheus-prod-01-eu-west-0.grafana.net/api/prom\",         \"prometheus_user_id\": os.getenv(\"GRAFANA_USER_ID\"),         \"prometheus_api_key\": os.getenv(\"GRAFANA_API_KEY\"),     })      query.time_series()   await prom_graph(\"\"\"     cpu usage of the cluster over the past 2 hour, use 2m as the rate interval \"\"\") <pre>2025-03-07 21:40:10 [info     ] datetime_extraction            text=past 2 hour\n2025-03-07 21:40:10 [info     ] running knowledge retrieval tool categories=[] llm_summary=False query=prometheus metrics for cpu usage in cluster top_n=20\nLinear Dim set to: 96 for downcasting\n2025-03-07 21:40:20 [info     ] reranked results               length=20\n2025-03-07 21:40:20 [info     ] call completed                 function=opsmate.tools.knowledge_retrieval.__call__ time=9.382406234741211\nstart='2025-03-07T19:40:21Z' end='2025-03-07T21:40:21Z' query='sum(rate(node_cpu_seconds_total{mode!=\"idle\"}[2m])) by (cluster) / sum(rate(node_cpu_seconds_total[2m])) by (cluster)' y_label='CPU Usage (%)' x_label='Timestamp' title='Cluster CPU Usage (Percentage)' explanation='This query calculates the CPU usage percentage for each cluster by dividing the sum of all non-idle CPU time by the total CPU time. It uses the node_cpu_seconds_total metric with a 2m rate interval as specified, and aggregates the results by cluster.' sample_points=50 start_dt=datetime.datetime(2025, 3, 7, 19, 40, 21) end_dt=datetime.datetime(2025, 3, 7, 21, 40, 21) output=None step='15s'\n2025-03-07 21:40:30 [info     ] plotting time series           query=sum(rate(node_cpu_seconds_total{mode!=\"idle\"}[2m])) by (cluster) / sum(rate(node_cpu_seconds_total[2m])) by (cluster)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"providers/","title":"LLM Providers","text":""},{"location":"providers/#ai-providers","title":"AI Providers","text":"<p>Opsmate supports a variety of LLM providers. You can use the <code>opsmate list-models</code> command to see the models supported by your provider.</p>"},{"location":"providers/#links","title":"Links","text":"<ul> <li>OpenAI</li> <li>Anthropic</li> <li>XAI</li> <li>Groq</li> <li>Fireworks AI</li> </ul>"},{"location":"providers/anthropic/","title":"Anthropic","text":"<p>Anthropic is a large language model provider that, based on the vibe and evaluation metrics by far provides the best results.</p>"},{"location":"providers/anthropic/#configuration","title":"Configuration","text":"<p>Anthropic API key is required to use Anthropic models. You can set the API key using the <code>ANTHROPIC_API_KEY</code> environment variable.</p> <pre><code>export ANTHROPIC_API_KEY=&lt;your-anthropic-api-key&gt;\n</code></pre> <p>Like OpenAI we only support select models from Anthropic which produces reasonably good results.</p> <p>To find all the models supported by Anthropic, you can run:</p> <pre><code>opsmate list-models --provider anthropic\n</code></pre>"},{"location":"providers/anthropic/#usage","title":"Usage","text":"<p>You can specify the <code>-m</code> or <code>--model</code> option for the <code>run</code>, <code>solve</code>, and <code>chat</code> commands.</p> <pre><code>opsmate run -m claude-3-5-sonnet-20241022 \"What is the OS?\"\n\n# use claude-3-opus-20240229\nopsmate run -m claude-3-7-sonnet-20250219 \"What is the OS?\"\n</code></pre>"},{"location":"providers/anthropic/#see-also","title":"See also","text":"<ul> <li>run</li> <li>solve</li> <li>chat</li> <li>serve</li> <li>list-models</li> </ul>"},{"location":"providers/fireworks-ai/","title":"Fireworks AI","text":"<p>Fireworks AI is another LLM inference provider that supports a wide range of models. Notably it supports models such as deepseek and llama that comes with 400B+ parameters with affordable prices.</p>"},{"location":"providers/fireworks-ai/#installation","title":"Installation","text":"<p>Fireworks AI is not installed by default in Opsmate. You can install it using the following command:</p> <pre><code>opsmate install opsmate-provider-fireworks\n</code></pre>"},{"location":"providers/fireworks-ai/#configuration","title":"Configuration","text":"<p>Fireworks AI API key is required to use Fireworks AI models. You can set the API key using the <code>FIREWORKS_API_KEY</code> environment variable.</p> <pre><code>export FIREWORKS_API_KEY=&lt;your-fireworks-api-key&gt;\n\n# You can also proxy the API calls to an alternative endpoint\nexport FIREWORKS_BASE_URL=&lt;your-fireworks-base-url&gt;\n</code></pre> <p>To find all the models supported by Fireworks AI, you can run:</p> <pre><code>opsmate list-models --provider fireworks\n</code></pre>"},{"location":"providers/fireworks-ai/#usage","title":"Usage","text":"<p>You can specify the <code>-m</code> or <code>--model</code> option for the <code>run</code>, <code>solve</code>, and <code>chat</code> commands.</p> <pre><code># deepseek-v3-0324 comes with 671B parameters\nopsmate run -m accounts/fireworks/models/deepseek-v3-0324 \"What is the OS?\"\n</code></pre>"},{"location":"providers/fireworks-ai/#see-also","title":"See also","text":"<ul> <li>run</li> <li>solve</li> <li>chat</li> <li>serve</li> </ul>"},{"location":"providers/google-genai/","title":"Google Vertex AI","text":"<p>Google GenAI supports a wide range of state-of-the-art generative AI models hosted on Google's advanced, global infrastructure.</p>"},{"location":"providers/google-genai/#installation","title":"Installation","text":"<pre><code>opsmate install opsmate-provider-google-genai\n</code></pre> <p>After installation you can list all the models via</p> <pre><code>$ opsmate list-models --provider google-genai\n                    Models\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Provider     \u2503 Model                        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 google-genai \u2502 gemini-2.5-pro-preview-03-25 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 google-genai \u2502 gemini-2.5-pro-exp-03-25     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 google-genai \u2502 gemini-2.0-flash-001         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 google-genai \u2502 gemini-2.0-flash-lite        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"providers/google-genai/#limitations","title":"Limitations","text":""},{"location":"providers/google-genai/#only-vertex-ai-models-are-supported","title":"Only Vertex AI models are supported","text":"<p> This provider currently does not support Gemini API, because the Gemini API does not support <code>default</code> value in the response schema.</p> <p>As the result, currently only vertex AI models are supported, meaning you need to have a Google Cloud account in order to use this provider.</p>"},{"location":"providers/google-genai/#limited-region-support-for-gemini-25-pro","title":"Limited region support for gemini-2.5-pro","text":"<p>By the time the provider is published, the <code>gemini-2.5-pro-preview-03-25</code> and <code>gemini-2.5-pro-exp-03-25</code> models are only available in the <code>us-central1</code> region. To use it you will need to set <code>GOOGLE_CLOUD_LOCATION</code> as below:</p> <pre><code>export GOOGLE_CLOUD_LOCATION=us-central1\n\n# or\nexport GOOGLE_CLOUD_LOCATION=global\n</code></pre>"},{"location":"providers/google-genai/#usage","title":"Usage","text":"<pre><code>export GOOGLE_CLOUD_PROJECT=&lt;your-project-id&gt;\nexport GOOGLE_CLOUD_LOCATION=&lt;your-location&gt;\n\nopsmate chat -m gemini-2.0-flash-001\n</code></pre>"},{"location":"providers/google-genai/#uninstall","title":"Uninstall","text":"<pre><code>opsmate uninstall -y opsmate-provider-google-genai\n</code></pre>"},{"location":"providers/groq/","title":"Groq","text":"<p>Groq provides fast inference with affordable prices. It supports a wide range of open-weight models.</p>"},{"location":"providers/groq/#installation","title":"Installation","text":"<p>Groq is not installed by default in Opsmate. You can install it using the following command:</p> <pre><code>opsmate install opsmate-provider-groq\n</code></pre>"},{"location":"providers/groq/#configuration","title":"Configuration","text":"<p>Groq API key is required to use Groq models. You can set the API key using the <code>GROQ_API_KEY</code> environment variable.</p> <pre><code>export GROQ_API_KEY=&lt;your-groq-api-key&gt;\n</code></pre> <p>To find all the models supported by Groq, you can run:</p> <pre><code>opsmate list-models --provider groq\n</code></pre>"},{"location":"providers/groq/#usage","title":"Usage","text":"<p>You can specify the <code>-m</code> or <code>--model</code> option for the <code>run</code>, <code>solve</code>, and <code>chat</code> commands.</p> <pre><code>opsmate run -m llama-3.3-70b-versatile \"What is the OS?\"\n</code></pre>"},{"location":"providers/groq/#see-also","title":"See also","text":"<ul> <li>run</li> <li>solve</li> <li>chat</li> <li>serve</li> <li>list-models</li> </ul>"},{"location":"providers/ollama/","title":"Ollama","text":"<p>Ollama is popular choice for running LLMs locally.</p>"},{"location":"providers/ollama/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have already Ollama installed on your machine.</li> <li>Ollama is up and running on your machine.</li> </ul>"},{"location":"providers/ollama/#caution","title":"Caution","text":"<p>Currently we have only experimented with Ollama on Apple Silicon with bunch of 7b - 12b parameter models. <code>gemma3:12b</code> so far is the only model that can produce barely acceptable results, and it's way behind in terms of quality and latency compared to frontier models.</p> <p>That being said we encourage you to give it a try and report any issues, especially for those who has 128GB+ vRAM to run bigger models.</p>"},{"location":"providers/ollama/#usage","title":"Usage","text":"<p>Assuming you have <code>gemma3:12b</code> model pulled via <code>ollama pull gemma3:12b</code>, you can run it with: <pre><code>opsmate run --context cli-lite -m gemma3:12b \"how many cores on the machine\"\n</code></pre></p> <p>We strongly recommend you to use <code>cli-lite</code> context for running 7b - 12b parameter small models. You can find the prompt of <code>cli-lite</code> context in cli_lite.py.</p> <p>To find all the ollama models you can run:</p> <pre><code>opsmate list-models --provider ollama\n</code></pre> <p>Behind the scene it fetches the list of models from <code>http://localhost:11434/v1/models</code>.</p> <p>If you have a remote ollama server, you can point to the remote server with:</p> <pre><code># by default it's http://localhost:11434/v1\nexport OLLAMA_BASE_URL=http://$YOUR_REMOTE_SERVER:11434/v1\n</code></pre>"},{"location":"providers/ollama/#further-exploration","title":"Further Exploration","text":"<p>The <code>cli-lite</code> context is far from optimal. To test your own prompt, you can create your own context in side <code>~/.opsmate/contexts</code> directory. The contexts in the directory will be loaded automatically by opsmate on startup.</p>"},{"location":"providers/openai/","title":"OpenAI","text":"<p>By default Opsmate uses OpenAI as the default LLM provider, and <code>gpt-4o</code> as the default model.</p> <p>To find all the models supported by OpenAI, you can run:</p> <pre><code>opsmate list-models --provider openai\n</code></pre> <p>At the moment we only support select models from OpenAI which produces reasonably good results.</p>"},{"location":"providers/openai/#configuration","title":"Configuration","text":"<p>OpenAI API key is required to use OpenAI models. You can set the API key using the <code>OPENAI_API_KEY</code> environment variable.</p> <pre><code>export OPENAI_API_KEY=&lt;your-openai-api-key&gt;\n</code></pre> <p>If your request goes through a proxy, you can set the <code>OPENAI_API_BASE</code> environment variable to the proxy URL.</p> <pre><code>export OPENAI_BASE_URL=&lt;your-proxy-url&gt;\n</code></pre> <p>Other configuration options such as <code>OPENAI_PROJECT_ID</code> and <code>OPENAI_ORG_ID</code> can be set using the environment variables. They will be picked up by the OpenAI SDK used by Opsmate.</p> <pre><code>export OPENAI_PROJECT_ID=&lt;your-openai-project-id&gt;\nexport OPENAI_ORG_ID=&lt;your-openai-org-id&gt;\n</code></pre>"},{"location":"providers/openai/#usage","title":"Usage","text":"<p>You can specify the <code>-m</code> or <code>--model</code> option for the <code>run</code>, <code>solve</code>, and <code>chat</code> commands.</p> <pre><code># gpt-4o is the default model\nopsmate run -m gpt-4o \"What is the OS?\"\n\n# use gpt-4o-mini\nopsmate run -m gpt-4o-mini \"What is the OS?\"\n</code></pre>"},{"location":"providers/openai/#see-also","title":"See also","text":"<ul> <li>run</li> <li>solve</li> <li>chat</li> <li>serve</li> <li>list-models</li> </ul>"},{"location":"providers/xai/","title":"XAI","text":"<p>We also support xAI as a provider.</p>"},{"location":"providers/xai/#configuration","title":"Configuration","text":"<p>xAI API key is required to use xAI models. You can set the API key using the <code>XAI_API_KEY</code> environment variable.</p> <pre><code>export XAI_API_KEY=&lt;your-xai-api-key&gt;\n</code></pre> <p>To find all the models supported by xAI, you can run:</p> <pre><code>opsmate list-models --provider xai\n</code></pre>"},{"location":"providers/xai/#usage","title":"Usage","text":"<p>You can specify the <code>-m</code> or <code>--model</code> option for the <code>run</code>, <code>solve</code>, and <code>chat</code> commands.</p> <pre><code>opsmate run -m grok-2-1212 \"What is the OS?\"\n</code></pre>"},{"location":"providers/xai/#see-also","title":"See also","text":"<ul> <li>run</li> <li>solve</li> <li>chat</li> <li>serve</li> <li>list-models</li> </ul>"},{"location":"tools/","title":"Tools","text":"<p>Out of the box, Opsmate provides diverse tools for performing different tasks.</p> <p>You can find the tools available to you via running list-tools command.</p> <pre><code>opsmate list-tools\n</code></pre>"},{"location":"tools/loki/","title":"Loki","text":"<p>Grafana Loki is a horizontally scalable, highly available, multi-tenant log aggregation system inspired by Prometheus. It is designed to be very cost effective and easy to operate. It does not index the contents of the logs, but rather a set of labels for each log stream.</p> <p>Opsmate offers <code>LokiQueryTool</code> to query logs in loki via natural language.</p> <p> This is a highly experimental tool and the API is subject to change.</p>"},{"location":"tools/loki/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have your system logs pushed to loki</li> <li>You have access to the loki api</li> </ul>"},{"location":"tools/loki/#setup","title":"Setup","text":"<p><code>LokiQueryTool</code> is out of box supported by Opsmate without plugin installation.</p> <p>Here are the default configuration for the tool:</p> <pre><code>LOKI_ENDPOINT=http://localhost:3100\nLOKI_PATH=/api/v1/query_range\n# Optional: LOKI_USER_ID\n# Optional: LOKI_API_KEY\n</code></pre> <p>You can also override the default configuration by setting the environment variables. In the example below we point Opsmate to a loki instance deployed within the Grafana Cloud:</p> <pre><code>LOKI_ENDPOINT=https://logs-prod-eu-west-0.grafana.net/loki\nLOKI_USER_ID=xxxx\nLOKI_API_KEY=glc_xxx\n</code></pre> <p>To use the tool you can specify <code>LokiQueryTool</code> as part of the <code>--tools</code> option when running <code>opsmate run</code>, <code>opsmate solve</code>, <code>opsmate chat</code> or <code>opsmate serve</code>:</p> <pre><code>opsmate run --tools LokiQueryTool,OtherTool ...\n</code></pre> <p>Alternatively you can add the tool in <code>~/.opsmate/config.yaml</code> via:</p> <pre><code>OPSMATE_TOOLS:\n- LokiQueryTool\n- OtherTool\n</code></pre> <p>Once the tool is added to the config, Opsmate will prioritise using Loki for query logs over other tools.</p>"},{"location":"tools/loki/#current-limitations","title":"Current Limitations","text":"<ul> <li>The Loki Tool at the moment is Kubernetes centric meaning it can only query based on the <code>namespace</code>, <code>pod</code> and <code>container</code> labels.</li> <li>The tool at the moment only support <code>logfmt</code> and <code>json</code> for effective log parsing.</li> </ul>"},{"location":"tools/mysql/","title":"MySQL","text":"<p>MySQLTool is a tool that allows you to interact with MySQL databases.</p>"},{"location":"tools/mysql/#installation","title":"Installation","text":"<p>The MySQLTool is not pre-installed with Opsmate. You need to install it explicitly:</p> <pre><code>opsmate install opsmate-tools-mysql\n</code></pre> <p>Once installed, the tool will be autodiscovered by Opsmate on startup. To verify this you can run the following commands:</p> <pre><code>opsmate list-tools | grep -i mysql\n\u2502 MySQLTool           \u2502 MySQL tool\n</code></pre> <p>The command line options will be added to the <code>opsmate [run|solve|chat|serve]</code> commands:</p> <pre><code># to verify the mysql runtime is autodiscovered\nopsmate chat --help | grep -i mysql\n  --runtime-mysql-timeout INTEGER\n                                  The timeout of the MySQL server (env:\n                                  RUNTIME_MYSQL_TIMEOUT)  [default: 120]\n  --runtime-mysql-charset TEXT    The charset of the MySQL server (env:\n                                  RUNTIME_MYSQL_CHARSET)  [default: utf8mb4]\n  --runtime-mysql-database TEXT   The database of the MySQL server (env:\n                                  RUNTIME_MYSQL_DATABASE)\n  --runtime-mysql-password TEXT   The password of the MySQL server (env:\n                                  RUNTIME_MYSQL_PASSWORD)  [default: \"\"]\n  --runtime-mysql-user TEXT       The user of the MySQL server (env:\n                                  RUNTIME_MYSQL_USER)  [default: root]\n  --runtime-mysql-port INTEGER    The port of the MySQL server (env:\n                                  RUNTIME_MYSQL_PORT)  [default: 3306]\n  --runtime-mysql-host TEXT       The host of the MySQL server (env:\n                                  RUNTIME_MYSQL_HOST)  [default: localhost]\n</code></pre>"},{"location":"tools/mysql/#show-cases","title":"Show Cases","text":"<p>Here is an example of \"chatting\" with the <code>x-for-pet</code> database using Opsmate:</p> <p> </p> <p>Here is another example of Claude Sonnet 3.7 conducting database schema analysis (the text size is a bit small, please feel free to zoom in):</p> <p> </p>"},{"location":"tools/mysql/#uninstallation","title":"Uninstallation","text":"<pre><code>opsmate uninstall -y opsmate-tools-mysql\n</code></pre>"},{"location":"tools/postgres/","title":"Postgres","text":"<p>PostgresTool is a tool that allows you to interact with PostgreSQL databases.</p>"},{"location":"tools/postgres/#installation","title":"Installation","text":"<p>The PostgresTool is not pre-installed with Opsmate. You need to install it explicitly:</p> <pre><code>opsmate install opsmate-tool-postgres\n</code></pre> <p>To verify the installation, you can run:</p> <pre><code>$ opsmate list-tools | grep -i postgres\n\u2502 PostgresTool        \u2502 PostgreSQL tool\n</code></pre> <p>The command line options will be added to the <code>opsmate [run|solve|chat|serve]</code> commands:</p> <pre><code># to verify the postgres runtime is autodiscovered\nopsmate chat --help | grep -i postgres\n  --postgres-tool-runtime TEXT    The runtime to use for the tool call (env:\n                                  POSTGRES_TOOL_RUNTIME)  [default: postgres]\n  --runtime-postgres-timeout INTEGER\n                                  The timeout of the PostgreSQL server in\n                                  seconds (env: RUNTIME_POSTGRES_TIMEOUT)\n  --runtime-postgres-schema TEXT  The schema of the PostgreSQL server (env:\n                                  RUNTIME_POSTGRES_SCHEMA)  [default: public]\n  --runtime-postgres-database TEXT\n                                  The database of the PostgreSQL server (env:\n                                  RUNTIME_POSTGRES_DATABASE)\n  --runtime-postgres-password TEXT\n                                  The password of the PostgreSQL server (env:\n                                  RUNTIME_POSTGRES_PASSWORD)  [default: \"\"]\n  --runtime-postgres-user TEXT    The user of the PostgreSQL server (env:\n                                  RUNTIME_POSTGRES_USER)  [default: postgres]\n  --runtime-postgres-port INTEGER\n                                  The port of the PostgreSQL server (env:\n                                  RUNTIME_POSTGRES_PORT)  [default: 5432]\n  --runtime-postgres-host TEXT    The host of the PostgreSQL server (env:\n                                  RUNTIME_POSTGRES_HOST)  [default: localhost]\n</code></pre>"},{"location":"tools/postgres/#usage","title":"Usage","text":"<p>Similar to the MySQLTool, you can interact with the Postgres database by running:</p> <pre><code>opsmate chat \\\n  --runtime-postgres-password postgres \\\n  --runtime-postgres-host localhost \\\n  --runtime-postgres-database &lt;your-database&gt; \\\n  --runtime-postgres-schema &lt;your-schema&gt; \\\n  --tools PostgresTool\n</code></pre>"},{"location":"tools/postgres/#uninstall","title":"Uninstall","text":"<pre><code>opsmate uninstall -y opsmate-tool-postgres\n</code></pre>"},{"location":"tools/prometheus/","title":"Prometheus","text":"<p>PrometheusTool is a tool to query metrics from prometheus tsdb via natural language. The tool itself is out of box supported by opsmate and added to all the prebuilt contexts.</p> <p>You can also explicitly add the tool to your session via</p> <pre><code>opsmate [run|solve|chat|serve] --models PrometheusTool ...\n</code></pre> <p>You can configure the prometheus endpoint and other parameters via environment variables:</p> <pre><code>PROMETHEUS_ENDPOINT=http://localhost:9090 # default endpoint\nPROMETHEUS_PATH=/api/v1/query # default path\n# Optional: PROMETHEUS_USER_ID\n# Optional: PROMETHEUS_API_KEY\n</code></pre> <p>Example usage:</p> <p>Here is a simple example of how to use the tool (you probably need to zoom in to see the text):</p> <p> </p> <p>Note that for LLM to come up with the correct promql query, you need to provide enough information about:</p> <ul> <li>the metrics name</li> <li>the labels</li> </ul> <p>In Opsmate you can store the metrics metadata in the vector db and ask LLM to retrieve the metrics semantically on the fly.</p> <p>See ingest-prometheus-metrics-metadata for more details.</p>"}]}